{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff125a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from obspy.clients.fdsn import Client\n",
    "import numpy as np\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "from obspy import Stream, Trace\n",
    "from obspy.signal.trigger import trigger_onset\n",
    "\n",
    "from pnwstore.mseed import WaveformClient\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import pandas as pd\n",
    "import gc\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "\n",
    "\"\"\" \n",
    "module containing functions needed in parallel_pick_201?.py\n",
    "\"\"\" \n",
    "# Initialize module logger \n",
    "Logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define clients\n",
    "client_inventory = Client('IRIS')\n",
    "client_waveform = WaveformClient()\n",
    "client_ncedc = Client('NCEDC')\n",
    "\n",
    "twin = 6000     # length of time window\n",
    "step = 3000     # step length\n",
    "l_blnd, r_blnd = 500, 500\n",
    "\n",
    "def pred_trigger_pick(pred, source_trace, label, thrd=0.1, **kwargs):\n",
    "    \"\"\"Use a simple, single-threshold trigger to detect local maxima\n",
    "    in a positive valued prediction time-series output by EQTransformer\n",
    "\n",
    "    :param pred: model prediction output time-series \n",
    "    :type pred: numpy.ndarray\n",
    "    :param source_trace: source obspy Trace-like object from which **pred**\n",
    "    :type source_trace: obspy.core.trace.Trace\n",
    "    :param label: prediction label\n",
    "    :type label: str, optional\n",
    "    :param thrd: Detection threshold for this label, defaults to 0.1\n",
    "    :type thrd: float, optional\n",
    "    :param kwargs: key-word argument collector passed to :meth:`~obspy.signal.trigger.trigger_onset`\n",
    "\n",
    "    :raises ValueError: _description_\n",
    "    :raises TypeError: _description_\n",
    "    :return: picks\n",
    "    :rtype: pandas.core.dataframe.DataFrame\n",
    "    \"\"\"    \n",
    "    if len(pred) < 300:\n",
    "        raise ValueError('insufficient samples in pred')\n",
    "    if not isinstance(source_trace, Trace):\n",
    "        raise TypeError('source_trace must be type obspy.Trace')\n",
    "    pred[:300] = 0.\n",
    "    triggers = trigger_onset(pred, thrd, thrd, **kwargs)\n",
    "    t0 = source_trace.stats.starttime\n",
    "    sr = source_trace.stats.sampling_rate\n",
    "    id = source_trace.id\n",
    "    picks = []\n",
    "    \n",
    "    for s0, s1 in triggers:\n",
    "        to = t0 + s0/sr\n",
    "        tp = t0 + (s0 + np.argmax(pred[s0:s1+1]))/sr\n",
    "        tf = t0 + s1/sr\n",
    "        pv = np.max(pred[s0:s1+1])\n",
    "        line = id[:-1].split('.') + [label, t0, to, tp, tf, pv, thrd]\n",
    "        # pick = Pick(trace_id = id, start_time=to, end_time=tf, peak_time=tp, peak_value=pv, phase=phz_name)\n",
    "        picks.append(line)\n",
    "    \n",
    "    picks = pd.DataFrame(data=picks,columns=['network','station','location','band_inst','label','trace_starttime',\n",
    "                                             'trigger_onset','pick_time','trigger_offset','max_prob','thresh_prob'])\n",
    "    return picks\n",
    "\n",
    "def stacking(data, npts, l_blnd, r_blnd, nseg):\n",
    "    _data = data.copy()\n",
    "    stack = np.full(npts, np.nan, dtype = np.float32)\n",
    "    _data[:, :l_blnd] = np.nan; _data[:, -r_blnd:] = np.nan\n",
    "    stack[:twin] = _data[0, :]\n",
    "    for iseg in range(nseg-1):\n",
    "        idx = step*(iseg+1)\n",
    "        stack[idx:idx + twin] = \\\n",
    "                np.nanmax([stack[idx:idx + twin], _data[iseg+1, :]], axis = 0)\n",
    "    return stack\n",
    "\n",
    "def run_detection(network,station,t1,t2,filepath,twin,step,l_blnd,r_blnd):\n",
    "    \"\"\"Run an ensemble machine learning model semblance detection workflow on a\n",
    "    specified job\n",
    "\n",
    "    :param network: network code for the station being analyzed\n",
    "    :type network: str\n",
    "    :param station: station code for the station being analyzed\n",
    "    :type station: str\n",
    "    :param t1: start time for waveform records to analyze\n",
    "    :type t1: pandas.core.timestamp.Timestamp\n",
    "    :param t2: end time for waveform records to analyze\n",
    "    :type t2: pandas.core.timestamp.Timestamp\n",
    "    :param filepath: file name and path to save results to (do not include *.csv extension)\n",
    "    :type filepath: str\n",
    "    :param twin: scale of the input layer for the specified ML model architecture\n",
    "    :type twin: int\n",
    "    :param step: samples to advance subsequent windows\n",
    "    :type step: int\n",
    "    :param l_blnd: number of samples to ignore (\"blind\") on the left side of each\n",
    "        model input\n",
    "    :type l_blnd: int\n",
    "    :param r_blnd: number of samples to blind on the right side of each model input\n",
    "    :type r_blnd: int\n",
    "    :param lat: station location latitude (NTS: Obsolite this...)\n",
    "    :type lat: float\n",
    "    :param lon: station location longitude (NTS: Obsolite this...)\n",
    "    :type lon: float\n",
    "    :param elev: station elevation (NTS: Obsolite this...)\n",
    "    :type elev: float\n",
    "    \"\"\"    \n",
    "    # TODO: Should install compatability checks for inputs\n",
    "\n",
    "\n",
    "    # Columns for the end-product\n",
    "    columns = ['network','station','band','instrument','label','t0','sr','p_trig','i_on','i_max','i_off','pmax']\n",
    "    \n",
    "    # Define tstring\n",
    "    tstring = t1.strftime('%Y%m%d')\n",
    "    tstring2 = t2.strftime('%Y%m%d')\n",
    "    save_file_name = filepath+network+'_'+station+'_'+tstring+'_'+tstring2+'.csv'\n",
    "    \n",
    "    check_filepath = filepath.replace(\"_122-123_46-50/\", \"_122-129/\")\n",
    "    #################################\n",
    "    check_file_name = check_filepath+network+'_'+station+'_'+tstring+'_'+tstring2+'.csv'\n",
    "    if os.path.exists(check_file_name):\n",
    "        print(f'File {check_file_name} already exists')\n",
    "        return\n",
    "    #################################\n",
    "    # Safety catch against overwriting previous analyses\n",
    "    if os.path.exists(save_file_name):\n",
    "        print(f'File {save_file_name} already exists')\n",
    "        return\n",
    "    # print the file path \n",
    "    print('test1')\n",
    "    print(save_file_name)\n",
    "    \n",
    "\t# Load data\n",
    "\t# Reshape data\n",
    "\t# Predict on base models\n",
    "\t# Stack\n",
    "\t# Create and write csv file. Define file name using the station code and the input filepath\n",
    "    network = network\n",
    "    channels = '*'\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Get waveforms and filter\n",
    "    # NTS: This sampling scheme leans heavily on prior data QC going into PNW store\n",
    "    try:\n",
    "        if network in ['NC', 'BK']:\n",
    "            # Query waveforms\n",
    "            _sdata = client_ncedc.get_waveforms(network=network, station=station, location=\"*\", channel=channels,\n",
    "                                               starttime=UTCDateTime(t1), endtime=UTCDateTime(t2))\n",
    "        else:\n",
    "            # Shouldn't this have an explicit starttime + endtime inputs?\n",
    "            _sdata = client_waveform.get_waveforms(network=network, station=station, channel=channels, \n",
    "                                              year=t1.strftime('%Y'), month=t1.strftime('%m'), \n",
    "                                              day=t1.strftime('%d'))\n",
    "    except obspy.clients.fdsn.header.FDSNNoDataException:\n",
    "        Logger.warning(f\"WARNING: No data for {network}.{station}.{channels} on {t1} - {t2}.\")\n",
    "        return\n",
    "    \n",
    "    # Create a new stream\n",
    "    sdata = Stream()\n",
    "    # Check if loaded data have a vertical component (minimum requirement)\n",
    "    has_Z = bool(_sdata.select(channel='??Z'))\n",
    "    has_N = bool(_sdata.select(channel='??[2N]'))\n",
    "    has_E = bool(_sdata.select(channel='??[1E]'))\n",
    "\n",
    "    # Check for HH and BH channels presence\n",
    "    has_HH = bool(_sdata.select(channel=\"HH?\"))\n",
    "    has_BH = bool(_sdata.select(channel=\"BH?\"))\n",
    "    has_EH = bool(_sdata.select(channel=\"EH?\"))\n",
    "#     has_EN = bool(_sdata.select(channel=\"EN?\"))\n",
    "\n",
    "\n",
    "#     # Apply selection logic based on channel presence\n",
    "#     if has_HH and has_BH and has_EH and has_EN:\n",
    "#         # If all HH, BH, EH, and EN channels are present, select only HH\n",
    "#         sdata += _sdata.select(channel=\"HH?\")\n",
    "#     elif has_BH and has_EH and has_EN:\n",
    "#         # If BH, EH, and EN channels are present, select only BH\n",
    "#         sdata += _sdata.select(channel=\"BH?\")\n",
    "#     elif has_EH and has_EN:\n",
    "#         # If only EH and EN channels are present, select only EH\n",
    "#         sdata += _sdata.select(channel=\"EH?\")\n",
    "#     elif has_EN:\n",
    "#         # If only EN channels are present\n",
    "#         sdata += _sdata.select(channel=\"EN?\")\n",
    "    if not has_Z:\n",
    "        Logger.warning('No Vertical Component Data Present. Skipping')\n",
    "        return\n",
    "   # Apply selection logic based on channel presence\n",
    "    if has_HH:\n",
    "        # If all HH, BH, EH, and EN channels are present, select only HH\n",
    "        sdata += _sdata.select(channel=\"HH?\")\n",
    "    elif has_BH:\n",
    "        # If BH, EH, and EN channels are present, select only BH\n",
    "        sdata += _sdata.select(channel=\"BH?\")\n",
    "    elif has_EH:\n",
    "        # If only EH and EN channels are present, select only EH\n",
    "        # NTS: This may result in getting only vertical component data - EH? is used for PNSN analog stations\n",
    "        # NTS: This may also be tricky for pulling full day-volumes because the sampling rate shifts for\n",
    "        #      analog stations due to the remote digitization scheme used with analog stations\n",
    "        sdata += _sdata.select(channel=\"EH?\")\n",
    "    else:\n",
    "        return\n",
    "    # If only Z component is present, triplicate the Z component trace\n",
    "    if has_Z and not (has_N and has_E):\n",
    "        z_trace = sdata.select(channel=\"??Z\")[0]\n",
    "        sdata += z_trace.copy()\n",
    "        sdata += z_trace.copy()\n",
    "    \n",
    "    ###############################\n",
    "    # If no data returned, skipping\n",
    "    if len(sdata) == 0:\n",
    "        Logger.warning(\"No stream returned. Skipping.\")\n",
    "        return\n",
    "    if np.abs(np.mean(sdata[0].data[1:] - sdata[0].data[0:-1])) <= 1e-8:\n",
    "        Logger.warning(\"constant/no data in the stream. Skipping.\")\n",
    "        return\n",
    "    ###############################\n",
    "    # NTS: Filter and then resample\n",
    "    # Filter\n",
    "    sdata.filter(type='bandpass',freqmin=4,freqmax=15)\n",
    "    # Resample\n",
    "    sdata.resample(100)\n",
    "    \n",
    "    ###############################\n",
    "    # NTS: This may produce unintended swathes of filled gaps - advise revising this\n",
    "    sdata.merge(fill_value='interpolate') # fill gaps if there are any.\n",
    "    ###############################\n",
    "\n",
    "    # Get the necassary information about the station\n",
    "    delta = sdata[0].stats.delta\n",
    "    starttime = sdata[0].stats.starttime\n",
    "    fs = sdata[0].stats.sampling_rate\n",
    "    dt = 1/fs\n",
    "    \n",
    "\n",
    "    # Make all the traces in the stream have the same lengths\n",
    "    # This is risky as it may result in gappy data\n",
    "    max_starttime = max([tr.stats.starttime for tr in sdata])\n",
    "    min_endtime = min([tr.stats.endtime for tr in sdata])\n",
    "    \n",
    "    for tr in sdata:\n",
    "        tr.trim(starttime=max_starttime,endtime=min_endtime, nearest_sample=True)    \n",
    "\n",
    "    # NTS: Make sure traces are in Z[1E][2N] order\n",
    "    _s2d = Stream()\n",
    "    _s2x = Stream()\n",
    "    for _c in ['3Z','[1E]','[2N]']:\n",
    "        _s = sdata.select(channel=f'??{_c}')\n",
    "        # Prioritize only the first if more than one is present\n",
    "        for _e, _tr in enumerate(_s):\n",
    "            if _e == 0:\n",
    "                _s2d += _tr\n",
    "            else:\n",
    "                _s2x += _tr\n",
    "    # Tack the extra traces back onto the leading 3 ordered traces\n",
    "    _s2d += _s2x\n",
    "    # Overwrite sdata with re-ordered traces\n",
    "    sdata = _s2d\n",
    "\n",
    "    # Reshaping data\n",
    "    arr_sdata = np.array(sdata)\n",
    "    npts = arr_sdata.shape[1]\n",
    "    ############################### avoiding errors at the end of a stream\n",
    "   #nseg = int(np.ceil((npts - twin) / step)) + 1\n",
    "    nseg = int(np.floor((npts - twin) / step)) + 1\n",
    "    ###############################\n",
    "    windows = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    tap = 0.5 * (1 + np.cos(np.linspace(np.pi, 2 * np.pi, 6)))\n",
    "    \n",
    "    # Define the parameters for semblance\n",
    "    paras_semblance = {'dt':dt, 'semblance_order':2, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "    p_thrd, s_thrd = 0.05, 0.05\n",
    "\n",
    "    windows_std = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    windows_max = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    windows = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    windows_idx = np.zeros(nseg, dtype=np.int32)\n",
    "\n",
    "    for iseg in range(nseg):\n",
    "        idx = iseg * step\n",
    "        windows[iseg, :] = arr_sdata[:, idx:idx + twin]\n",
    "        windows[iseg, :] -= np.mean(windows[iseg, :], axis=-1, keepdims=True)\n",
    "        # original use std norm\n",
    "        windows_std[iseg, :] = windows[iseg, :] / np.std(windows[iseg, :]) + 1e-10\n",
    "        # others use max norm\n",
    "        windows_max[iseg, :] = windows[iseg, :] / (np.max(np.abs(windows[iseg, :]), axis=-1, keepdims=True))\n",
    "        windows_idx[iseg] = idx\n",
    "\n",
    "    # taper\n",
    "    windows_std[:, :, :6] *= tap; windows_std[:, :, -6:] *= tap[::-1]; \n",
    "    windows_max[:, :, :6] *= tap; windows_max[:, :, -6:] *= tap[::-1];\n",
    "    del windows\n",
    "\n",
    "    print(f\"Window data shape: {windows_std.shape}\")\n",
    "    \n",
    "    # Predict on base models\n",
    "    pretrain_list = ['original', 'ethz', 'instance', 'scedc', 'stead']\n",
    "\n",
    "    # dim 0: 0 = P, 1 = S\n",
    "    # NTS: Carefully note that batch_pred and _torch_pred are offset by 1 in their\n",
    "    #      0-axis. The native 0-axis of EQTransformer predictions are the \"detection\"\n",
    "    #      predictor.\n",
    "    batch_pred = np.zeros([2, len(pretrain_list), nseg, twin], dtype = np.float32) \n",
    "    for ipre, pretrain in enumerate(pretrain_list):\n",
    "        print('test10')\n",
    "        t0 = time.time()\n",
    "        eqt = sbm.EQTransformer.from_pretrained(pretrain)\n",
    "        eqt.to(device);\n",
    "        eqt._annotate_args['overlap'] = ('Overlap between prediction windows in samples \\\n",
    "                                        (only for window prediction models)', step)\n",
    "        eqt._annotate_args['blinding'] = ('Number of prediction samples to discard on \\\n",
    "                                         each side of each window prediction', (l_blnd, r_blnd))\n",
    "        eqt.eval();\n",
    "        if pretrain == 'original':\n",
    "            # batch prediction through torch model\n",
    "            windows_std_tt = torch.Tensor(windows_std)\n",
    "            _torch_pred = eqt(windows_std_tt.to(device))\n",
    "        else:\n",
    "            windows_max_tt = torch.Tensor(windows_max)\n",
    "            _torch_pred = eqt(windows_max_tt.to(device))\n",
    "        batch_pred[0, ipre, :] = _torch_pred[1].detach().cpu().numpy()\n",
    "        batch_pred[1, ipre, :] = _torch_pred[2].detach().cpu().numpy()\n",
    "\n",
    "    # clean up memory\n",
    "    # python parameters\n",
    "    del _torch_pred, windows_max_tt, windows_std_tt\n",
    "    del windows_std, windows_max\n",
    "    # gARBAGE cOLLECTOR cleaup\n",
    "    gc.collect()\n",
    "    # Torch GPU cleanup (if used?)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"All prediction shape: {batch_pred.shape}\")\n",
    "    \n",
    "    smb_pred = np.zeros([2, nseg, twin], dtype = np.float32)\n",
    "    # calculate the semblance\n",
    "    ## the semblance may takes a while bit to calculate\n",
    "    \n",
    "    for iseg in range(nseg):\n",
    "    #############################\n",
    "        # 0 for P-wave\n",
    "        smb_pred[0, iseg, :] = ensemble_semblance(batch_pred[0, :, iseg, :], paras_semblance)\n",
    "\n",
    "        # 1 for P-wave\n",
    "        smb_pred[1, iseg, :] = ensemble_semblance(batch_pred[1, :, iseg, :], paras_semblance)\n",
    "\n",
    "    ## ... and stack\n",
    "    # 0 for P-wave\n",
    "    smb_p = stacking(smb_pred[0, :], npts, l_blnd, r_blnd, nseg)\n",
    "\n",
    "    # 1 for P-wave\n",
    "    smb_s = stacking(smb_pred[1, :], npts, l_blnd, r_blnd, nseg)\n",
    "  \n",
    "    del smb_pred, batch_pred\n",
    "\n",
    "    # NTS: Went scorched-earth past this point - largely empty format\n",
    "    idf_p = pred_trigger_pick(smb_p, sdata[0],'P', thrd=p_thrd)\n",
    "    idf_s = pred_trigger_pick(smb_s, sdata[0], 'S', thrd=s_thrd)\n",
    "\n",
    "    df = pd.concat([idf_p, idf_s], axis=0, ignore_index=True)\n",
    "    df.to_csv(save_file_name)\n",
    "\n",
    "    # p_index = picks_summary_simple(smb_p, p_thrd)\n",
    "    # s_index = picks_summary_simple(smb_s, s_thrd)\n",
    "    # print(f\"{len(p_index)} P picks\\n{len(s_index)} S picks\")\n",
    "    \n",
    "    # print('test2')\n",
    "    # # Create lists and a data frame\n",
    "    # event_id = []\n",
    "    # source_type = []\n",
    "    # station_network_code = []\n",
    "    # station_channel_code = []\n",
    "    # station_code = []\n",
    "    # station_location_code = []\n",
    "    # station_latitude_deg= []\n",
    "    # station_longitude_deg = []\n",
    "    # station_elevation_m = []\n",
    "    # trace_name = []\n",
    "    # trace_sampling_rate_hz = []\n",
    "    # trace_start_time = []\n",
    "    # trace_S_arrival_sample = []\n",
    "    # trace_P_arrival_sample = []\n",
    "    # trace_S_onset = []\n",
    "    # trace_P_onset = []\n",
    "    # trace_snr_db = []\n",
    "    # trace_p_arrival = []\n",
    "    # trace_s_arrival = []\n",
    "    \n",
    "    # print(\"This is the cwd:\"+str(os.getcwd()))\n",
    "    # print('test3',len(p_index),len(s_index))\n",
    "    # for i, idx in enumerate(p_index):\n",
    "    #     event_id.append(' ')\n",
    "    #     source_type.append(' ')\n",
    "    #     station_network_code.append(network)   # Change to otehr networks\n",
    "    #     station_channel_code.append(' ')\n",
    "    #     station_code.append(station)\n",
    "    #     station_location_code.append(sdata[0].stats.location) \n",
    "    #     print('test3-1')\n",
    "    #     station_latitude_deg.append(lat)\n",
    "    #     print('test3-2')\n",
    "    #     station_longitude_deg.append(lon) \n",
    "    #     print('test3-3')\n",
    "    #     station_elevation_m.append(elev)\n",
    "    #     print('test3-4')\n",
    "    #     trace_name.append(' ')\n",
    "    #     trace_sampling_rate_hz.append(sdata[0].stats.sampling_rate)\n",
    "    #     print('test3-5')\n",
    "    #     trace_start_time.append(sdata[0].stats.starttime)\n",
    "    #     trace_S_arrival_sample.append(' ')\n",
    "    #     trace_P_arrival_sample.append(' ')\n",
    "    #     trace_S_onset.append(' ')\n",
    "    #     trace_P_onset.append(' ')\n",
    "    #     trace_snr_db.append(' ')\n",
    "    #     trace_s_arrival.append(np.nan)\n",
    "    #     print('test3-6')\n",
    "    #     trace_p_arrival.append(str(starttime  + idx * delta))\n",
    "\n",
    "    # print('test4')\n",
    "    # for i, idx in enumerate(s_index):\n",
    "    #     event_id.append(' ')\n",
    "    #     source_type.append(' ')\n",
    "    #     station_network_code.append(network) # Change to otehr networks\n",
    "    #     station_channel_code.append(' ')\n",
    "    #     station_code.append(station)\n",
    "    #     station_location_code.append(sdata[0].stats.location)   \n",
    "    #     print('test3-7')\n",
    "    #     station_latitude_deg.append(lat)\n",
    "    #     print('test3-8')\n",
    "    #     station_longitude_deg.append(lon)\n",
    "    #     print('test3-9')\n",
    "    #     station_elevation_m.append(elev)\n",
    "    #     print('test3-10')\n",
    "    #     trace_name.append(' ')\n",
    "    #     trace_sampling_rate_hz.append(sdata[0].stats.sampling_rate)\n",
    "    #     trace_start_time.append(sdata[0].stats.starttime)\n",
    "    #     trace_S_arrival_sample.append(' ')\n",
    "    #     trace_P_arrival_sample.append(' ')\n",
    "    #     trace_S_onset.append(' ')\n",
    "    #     trace_P_onset.append(' ')\n",
    "    #     trace_snr_db.append(' ')\n",
    "    #     trace_s_arrival.append(str(starttime  + idx * delta))\n",
    "    #     print('test3-11')\n",
    "    #     trace_p_arrival.append(np.nan)\n",
    "    # print('test5')\n",
    "    # # dictionary of lists\n",
    "    # dict = {'event_id':event_id,'source_type':source_type,'station_network_code':station_network_code,\\\n",
    "    #         'station_channel_code':station_channel_code,'station_code':station_code,'station_location_code':station_location_code,\\\n",
    "    #         'station_latitude_deg':station_latitude_deg,'station_longitude_deg':station_longitude_deg, \\\n",
    "    #         'station_elevation_m':station_elevation_m,'trace_name':trace_name,'trace_sampling_rate_hz':trace_sampling_rate_hz,\\\n",
    "    #         'trace_start_time':trace_start_time,'trace_S_arrival_sample':trace_S_arrival_sample,\\\n",
    "    #         'trace_P_arrival_sample':trace_P_arrival_sample, 'trace_S_onset':trace_S_onset,'trace_P_onset':trace_P_onset,\\\n",
    "    #         'trace_snr_db':trace_snr_db, 'trace_s_arrival':trace_s_arrival, 'trace_p_arrival':trace_p_arrival}\n",
    "\n",
    "    # df = pd.DataFrame(dict)\n",
    "    \n",
    "    # print('test6')\n",
    "    # # Make the specific day into a string:\n",
    "    # tstring = t1.strftime('%Y%m%d')\n",
    "    # # Build the full file name:\n",
    "    # print(\"test7\")\n",
    "    # print(\"This is the cwd:\"+str(os.getcwd()))\n",
    "    # print('This is the filepath:'+str(filepath))\n",
    "    # file_name = filepath+station+'_'+tstring+'.csv'\n",
    "    # ##################################################\n",
    "    # # Write to file using that name\n",
    "    # print(file_name,'this is before test9')\n",
    "    # print(df)\n",
    "    # print(f\"P and S summary:\\n{len(p_index)} P picks\\n{len(s_index)} S picks\")\n",
    "    # df.to_csv(file_name)\n",
    "    # print('test9')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531aa885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "[                                        ] | 0% Completed | 861.31 ms\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'sqlite3.Connection' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Use ProgressBar to track the progress\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Using the processes scheduler with num_workers specified\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlazy_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocesses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.9/site-packages/dask/base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.9/site-packages/cloudpickle/cloudpickle.py:1479\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m   1478\u001b[0m     cp \u001b[38;5;241m=\u001b[39m Pickler(file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback)\n\u001b[0;32m-> 1479\u001b[0m     \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.9/site-packages/cloudpickle/cloudpickle.py:1245\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'sqlite3.Connection' object"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from obspy.clients.fdsn import Client\n",
    "import numpy as np\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "from obspy import Stream\n",
    "\n",
    "from pnwstore.mseed import WaveformClient\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import pandas as pd\n",
    "import gc\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "\n",
    "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# if parent_dir not in sys.path:\n",
    "#     sys.path.append(parent_dir)\n",
    "    \n",
    "# from picking_utils_prio_tri_z import *\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print('test')\n",
    "\n",
    "# Define clients\n",
    "client_inventory = Client('IRIS')\n",
    "client_waveform = WaveformClient()\n",
    "client_ncedc = Client('NCEDC')\n",
    "\n",
    "# Parameters\n",
    "year1 = 2010\n",
    "filepath = f\"/home/hbito/cascadia_obs_ensemble_backup/data/picks_{year1}_123-127_EH/\"\n",
    "os.makedirs(filepath,exist_ok=True)\n",
    "\n",
    "twin = 6000     # length of time window\n",
    "step = 3000     # step length\n",
    "l_blnd, r_blnd = 500, 500\n",
    "\n",
    "# Now create your list of days to loop over!\n",
    "time1 = datetime.datetime(year=year1,month=1,day=1)\n",
    "time2 = datetime.datetime(year=year1+1,month=1,day=1)\n",
    "time_bins = pd.to_datetime(np.arange(time1,time2,pd.Timedelta(1,'days')))\n",
    "\n",
    "inventory = client_inventory.get_stations(network=\"C8,7D,7A,CN,NV,UW,UO,NC,BK,TA,OO,PB,X6,Z5,X9\", station=\"*\", minlatitude=40,minlongitude=-127,maxlatitude=50,maxlongitude=-123, starttime=time1.strftime('%Y%m%d'),endtime=time2.strftime('%Y%m%d'))\n",
    "\n",
    "\n",
    "\n",
    "# Make a list of networks and stations\n",
    "networks_stas = []\n",
    "lat =[]\n",
    "lon =[]\n",
    "elev =[]\n",
    "\n",
    "for i in range(len(inventory)):\n",
    "    network = inventory[i].code\n",
    "    \n",
    "    for j in range(len(inventory[i])):\n",
    "        networks_stas.append([network,inventory[i].stations[j].code,\n",
    "                              inventory[i].stations[j].latitude,\n",
    "                              inventory[i].stations[j].longitude,inventory[i].stations[j].elevation])\n",
    "    \n",
    "\n",
    "networks_stas =np.array(networks_stas)\n",
    "    \n",
    "# download models\n",
    "pretrain_list = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "\n",
    "# Combine that list of days with the list of stations and networks\n",
    "# We are essentially creating a list of the number of tasks we have to do with the information that is unique to each task; we will do them in parallel\n",
    "task_list = []\n",
    "for i in range(len(networks_stas)):\n",
    "\tfor t in time_bins:\n",
    "\t\ttask_list.append([networks_stas[i][0], networks_stas[i][1],networks_stas[i][2],networks_stas[i][3],networks_stas[i][4],t])\n",
    "# Now we start setting up a parallel operation using a package called Dask.\n",
    "\n",
    "@dask.delayed\n",
    "def loop_days(task, filepath, twin, step, l_blnd, r_blnd):\n",
    "    # Define the parameters that are specific to each task\n",
    "    t1 = obspy.UTCDateTime(task[5])\n",
    "    t2 = obspy.UTCDateTime(t1 + pd.Timedelta(1,'days'))\n",
    "    network = task[0]\n",
    "    station = task[1]\n",
    "    lat = task[2]\n",
    "    lon = task[3]\n",
    "    elev = task[4]\n",
    "\n",
    "    # Print network and station\n",
    "    print([network, station, t1])\n",
    "    # Call to the function that will perform the operation and write the results to file\n",
    "    try:\n",
    "        run_detection(network, station, t1, t2, filepath, twin, step, l_blnd, r_blnd)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your task_list, filepath, twin, step, l_blnd, r_blnd here or import them from another module\n",
    "    task_list = task_list # Replace with your actual task list\n",
    "    filepath = filepath  # Replace with your actual file path\n",
    "    twin = twin\n",
    "    step = step\n",
    "    l_blnd = l_blnd\n",
    "    r_blnd = r_blnd\n",
    "\n",
    "    # Wrap loop_days with dask.delayed\n",
    "    lazy_results = [dask.delayed(loop_days)(task, filepath, twin, step, l_blnd, r_blnd) for task in task_list]\n",
    "\n",
    "    # Use ProgressBar to track the progress\n",
    "    with ProgressBar():\n",
    "        # Using the processes scheduler with num_workers specified\n",
    "        compute(lazy_results, scheduler='processes', num_workers=4)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8020c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
