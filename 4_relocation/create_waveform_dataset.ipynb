{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef4c1fe",
   "metadata": {},
   "source": [
    "This notebook will collect waveform data and format them into a seisbench compatible hdf5 format in order to disseminate the data sets.\n",
    "\n",
    "\n",
    "by Marine Denolle (mdenolle@uw.edu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c980ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "import numpy as np\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# from pnwstore.mseed import WaveformClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95149d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define clients\n",
    "client_iris = Client('IRIS')\n",
    "# client_waveform = WaveformClient()\n",
    "client_ncedc = Client('NCEDC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8b14ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if file exists at path: /Users/marinedenolle/arrival_2010_2015_reloc_cog_ver3.csv\n",
      "File exists. Size: 45602750 bytes\n",
      "Successfully read the file!\n",
      "Successfully read the file!\n"
     ]
    }
   ],
   "source": [
    "# Read the association table\n",
    "import os\n",
    "\n",
    "# Define the file path\n",
    "file_path = '~/arrival_2010_2015_reloc_cog_ver3.csv'\n",
    "\n",
    "# Expand the home directory\n",
    "expanded_path = os.path.expanduser(file_path)\n",
    "\n",
    "# Check if the file exists\n",
    "print(f\"Checking if file exists at path: {expanded_path}\")\n",
    "if os.path.exists(expanded_path):\n",
    "    print(f\"File exists. Size: {os.path.getsize(expanded_path)} bytes\")\n",
    "    # Try to read the file\n",
    "    try:\n",
    "        assoc_df = pd.read_csv(expanded_path)\n",
    "        print(\"Successfully read the file!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "else:\n",
    "    print(\"File does not exist at the specified path!\")\n",
    "    # Look for similar files in the directory\n",
    "    dir_path = os.path.dirname(expanded_path)\n",
    "    print(f\"Files in {dir_path}:\")\n",
    "    similar_files = [f for f in os.listdir(dir_path) if 'arrival' in f.lower()]\n",
    "    for file in similar_files:\n",
    "        print(f\" - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33fe9981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved cleaned data to: /Users/marinedenolle/arrival_2010_2015_reloc_cog_ver3_cleaned.csv\n",
      "File size: 65.62 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sta</th>\n",
       "      <th>time</th>\n",
       "      <th>arid</th>\n",
       "      <th>iphase</th>\n",
       "      <th>prob</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UW.PCMD</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2010-01-01 00:15:27.180000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UW.RVW</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>P</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2010-01-01 00:15:37.840399872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UW.PCMD</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2010-01-01 00:15:33.280000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UW.GNW</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2010-01-01 00:15:42.002000128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PB.B013</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>S</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2010-01-01 00:15:43.618400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sta          time  arid iphase  prob                      datetime\n",
       "0  UW.PCMD  1.262305e+09     0      P  0.68 2010-01-01 00:15:27.180000000\n",
       "1   UW.RVW  1.262305e+09     1      P  0.68 2010-01-01 00:15:37.840399872\n",
       "2  UW.PCMD  1.262305e+09     2      S  0.68 2010-01-01 00:15:33.280000000\n",
       "3   UW.GNW  1.262305e+09     3      S  0.68 2010-01-01 00:15:42.002000128\n",
       "4  PB.B013  1.262305e+09     4      S  0.68 2010-01-01 00:15:43.618400000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the \"Unnamed: 0\" column which contains only indexes\n",
    "cleaned_df = assoc_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Define the new file path\n",
    "new_file_path = os.path.join(os.path.dirname(expanded_path), 'arrival_2010_2015_reloc_cog_ver3_cleaned.csv')\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file without the index\n",
    "cleaned_df.to_csv(new_file_path, index=False)\n",
    "\n",
    "# Verify the new file was created and print its size\n",
    "if os.path.exists(new_file_path):\n",
    "    print(f\"Successfully saved cleaned data to: {new_file_path}\")\n",
    "    print(f\"File size: {os.path.getsize(new_file_path) / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "# Display the first few rows of the cleaned dataframe\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78e0b599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 1004335\n",
      "First few rows:\n",
      "   Unnamed: 0      sta          time  arid iphase  prob  \\\n",
      "0           0  UW.PCMD  1.262305e+09     0      P  0.68   \n",
      "1           1   UW.RVW  1.262305e+09     1      P  0.68   \n",
      "2           2  UW.PCMD  1.262305e+09     2      S  0.68   \n",
      "3           3   UW.GNW  1.262305e+09     3      S  0.68   \n",
      "4           4  PB.B013  1.262305e+09     4      S  0.68   \n",
      "\n",
      "                       datetime  \n",
      "0 2010-01-01 00:15:27.180000000  \n",
      "1 2010-01-01 00:15:37.840399872  \n",
      "2 2010-01-01 00:15:33.280000000  \n",
      "3 2010-01-01 00:15:42.002000128  \n",
      "4 2010-01-01 00:15:43.618400000  \n",
      "Columns: ['Unnamed: 0', 'sta', 'time', 'arid', 'iphase', 'prob', 'datetime']\n",
      "\n",
      "Example time conversion:\n",
      "Epoch: 1262304927.18\n",
      "Datetime: 2010-01-01 00:15:27.180000\n",
      "UTCDateTime: 2010-01-01T00:15:27.180000Z\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Number of records: {len(assoc_df)}\")\n",
    "print(\"First few rows:\")\n",
    "print(assoc_df.head())\n",
    "print(\"Columns:\", assoc_df.columns.tolist())\n",
    "\n",
    "# Assuming the epoch time column is named 'time' - adjust if needed\n",
    "time_column = 'time'  # Change this if your column has a different name\n",
    "if time_column in assoc_df.columns:\n",
    "    # Convert epoch time to datetime\n",
    "    assoc_df['datetime'] = pd.to_datetime(assoc_df[time_column], unit='s')\n",
    "    \n",
    "    # Create a function to convert to UTCDateTime when needed\n",
    "    def to_utc_datetime(dt):\n",
    "        return obspy.UTCDateTime(dt)\n",
    "    \n",
    "    # Example conversion\n",
    "    print(\"\\nExample time conversion:\")\n",
    "    example = assoc_df.iloc[0]\n",
    "    print(f\"Epoch: {example[time_column]}\")\n",
    "    print(f\"Datetime: {example['datetime']}\")\n",
    "    print(f\"UTCDateTime: {to_utc_datetime(example['datetime'])}\")\n",
    "\n",
    "# Extract unique station information\n",
    "station_col = 'station'  # Change if needed\n",
    "network_col = 'network'  # Change if needed\n",
    "\n",
    "if station_col in assoc_df.columns and network_col in assoc_df.columns:\n",
    "    station_list = assoc_df[[network_col, station_col]].drop_duplicates().reset_index(drop=True)\n",
    "    print(f\"\\nFound {len(station_list)} unique stations:\")\n",
    "    print(station_list.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/lzmy975n0l5bjbmr9db291m00000gn/T/ipykernel_7668/2566019531.py:12: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['event_id'] = 'ev' + df['time'].astype(str).str.replace('.', '_')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No S arrival for event ev1262304927_18 at station PCMD\n",
      "Trace start: 1262304924.18, Trace end: 1262304987.18\n",
      "UW PCMD 1262304924.18\n",
      "Inventory created at 2025-05-23T14:13:47.966400Z\n",
      "\tCreated by: IRIS WEB SERVICE: fdsnws-station | version: 1.1.52\n",
      "\t\t    http://service.iris.edu/fdsnws/station/1/query?starttime=2010-01-...\n",
      "\tSending institution: IRIS-DMC (IRIS-DMC)\n",
      "\tContains:\n",
      "\t\tNetworks (1):\n",
      "\t\t\tUW\n",
      "\t\tStations (1):\n",
      "\t\t\tUW.PCMD (PC Mountain Detachment ANSS-SMO)\n",
      "\t\tChannels (0):\n",
      "\n",
      "Inventory created at 2025-05-23T14:13:47.966400Z\n",
      "\tCreated by: IRIS WEB SERVICE: fdsnws-station | version: 1.1.52\n",
      "\t\t    http://service.iris.edu/fdsnws/station/1/query?starttime=2010-01-...\n",
      "\tSending institution: IRIS-DMC (IRIS-DMC)\n",
      "\tContains:\n",
      "\t\tNetworks (1):\n",
      "\t\t\tUW\n",
      "\t\tStations (1):\n",
      "\t\t\tUW.PCMD (PC Mountain Detachment ANSS-SMO)\n",
      "\t\tChannels (0):\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "starttime and endtime must be UTCDateTime objects or None for this call to Stream.trim()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/noisepy/lib/python3.10/site-packages/obspy/core/stream.py:1526\u001b[0m, in \u001b[0;36mStream.trim\u001b[0;34m(self, starttime, endtime, pad, keep_empty_traces, nearest_sample, fill_value)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m starttime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1525\u001b[0m     delta \u001b[38;5;241m=\u001b[39m compatibility\u001b[38;5;241m.\u001b[39mround_away(\n\u001b[0;32m-> 1526\u001b[0m         (\u001b[43mstarttime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarttime\u001b[49m) \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m   1527\u001b[0m         tr\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39msampling_rate)\n\u001b[1;32m   1528\u001b[0m     starttime \u001b[38;5;241m=\u001b[39m tr\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mstarttime \u001b[38;5;241m+\u001b[39m delta \u001b[38;5;241m*\u001b[39m tr\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mdelta\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'UTCDateTime'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m sta \u001b[38;5;241m=\u001b[39m client_iris\u001b[38;5;241m.\u001b[39mget_stations(network\u001b[38;5;241m=\u001b[39mnetwork, station\u001b[38;5;241m=\u001b[39mstation,location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m,channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, \\\n\u001b[1;32m     36\u001b[0m                                starttime\u001b[38;5;241m=\u001b[39mtrace_start1 , endtime\u001b[38;5;241m=\u001b[39mtrace_end1)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(sta)\n\u001b[0;32m---> 38\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43mclient_iris\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_waveforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mstarttime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace_start1\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrace_end1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(waveform\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#save the waveform data to a seisbench format\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# h5_file = f\"{network}.{station}..HHZ.{trace_start.strftime('%Y%m%dT%H%M%S')}.h5\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# waveform.write(h5_file, format='HDF5')\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate P and S samples\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/noisepy/lib/python3.10/site-packages/obspy/clients/fdsn/client.py:883\u001b[0m, in \u001b[0;36mClient.get_waveforms\u001b[0;34m(self, network, station, location, channel, starttime, endtime, quality, minimumlength, longestonly, filename, attach_response, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_responses(st)\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_dataselect_url_to_stream(st)\n\u001b[0;32m--> 883\u001b[0m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarttime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m st\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/noisepy/lib/python3.10/site-packages/obspy/core/stream.py:1537\u001b[0m, in \u001b[0;36mStream.trim\u001b[0;34m(self, starttime, endtime, pad, keep_empty_traces, nearest_sample, fill_value)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1535\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarttime and endtime must be UTCDateTime objects \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1536\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor None for this call to Stream.trim()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraces:\n\u001b[1;32m   1539\u001b[0m     trace\u001b[38;5;241m.\u001b[39mtrim(starttime, endtime, pad\u001b[38;5;241m=\u001b[39mpad,\n\u001b[1;32m   1540\u001b[0m                nearest_sample\u001b[38;5;241m=\u001b[39mnearest_sample, fill_value\u001b[38;5;241m=\u001b[39mfill_value)\n",
      "\u001b[0;31mTypeError\u001b[0m: starttime and endtime must be UTCDateTime objects or None for this call to Stream.trim()"
     ]
    }
   ],
   "source": [
    "# Extract network and station from sta column\n",
    "df = cleaned_df.copy()\n",
    "df[['network', 'station']] = df['sta'].str.split('.', expand=True)\n",
    "\n",
    "# Set constants\n",
    "sampling_rate = 100  # Hz\n",
    "pre_arrival_time = 3  # seconds before first arrival\n",
    "post_arrival_time = 60  # seconds before first arrival\n",
    "\n",
    "# Create a unique event ID for each unique time\n",
    "# This assumes different events have different timestamps\n",
    "df['event_id'] = 'ev' + df['time'].astype(str).str.replace('.', '_')\n",
    "\n",
    "# Group by event_id, network and station to combine P and S arrivals\n",
    "rows = []\n",
    "for (event_id, network, station), group in df.groupby(['event_id', 'network', 'station']):\n",
    "    # Find P and S arrivals\n",
    "    p_arrival = group[group['iphase'] == 'P']\n",
    "    s_arrival = group[group['iphase'] == 'S']\n",
    "    if s_arrival.empty:\n",
    "        print(f\"No S arrival for event {event_id} at station {station}\")\n",
    "        \n",
    "    \n",
    "    # Calculate trace start time (30 seconds before the first arrival)\n",
    "    first_arrival = group['time'].min()\n",
    "    trace_start = first_arrival - pre_arrival_time\n",
    "    trace_end = first_arrival + post_arrival_time  # 30 seconds after the first arrival\n",
    "\n",
    "    # Convert to UTCDateTime\n",
    "    trace_start1 = obspy.UTCDateTime(trace_start).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    trace_end1 = obspy.UTCDateTime(trace_end).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    print(f\"Trace start: {trace_start}, Trace end: {trace_end}\")\n",
    "    # download the waveform data\n",
    "    print(network, station, trace_start)\n",
    "    sta = client_iris.get_stations(network=network, station=station,location=\"*\",channel=\"*\", \\\n",
    "                                   starttime=trace_start1 , endtime=trace_end1)\n",
    "    print(sta)\n",
    "    waveform = client_iris.get_waveforms(network=network, station=station, location='*',channel= '*',\\\n",
    "                                          starttime=trace_start1 , endtime= trace_end1)\n",
    "    print(waveform.data.shape)\n",
    "    #save the waveform data to a seisbench format\n",
    "    # h5_file = f\"{network}.{station}..HHZ.{trace_start.strftime('%Y%m%dT%H%M%S')}.h5\"\n",
    "    # waveform.write(h5_file, format='HDF5')\n",
    "\n",
    "    \n",
    "    # Calculate P and S samples\n",
    "    p_sample = None\n",
    "    if not p_arrival.empty:\n",
    "        p_time = p_arrival['time'].iloc[0]\n",
    "        p_sample = int((p_time - trace_start) * sampling_rate)\n",
    "    \n",
    "    s_sample = None\n",
    "    if not s_arrival.empty:\n",
    "        s_time = s_arrival['time'].iloc[0]\n",
    "        s_sample = int((s_time - trace_start) * sampling_rate)\n",
    "    \n",
    "    # Create row\n",
    "    row = {\n",
    "        \"event_id\": event_id,\n",
    "        \"source_type\": \"earthquake\",\n",
    "        \"station_network_code\": network,\n",
    "        \"station_channel_code\": \"?H\",  # Default channel code\n",
    "        \"station_code\": station,\n",
    "        \"station_location_code\": \"\",    # Default location code\n",
    "        \"station_latitude_deg\": None,   # Station metadata not available\n",
    "        \"station_longitude_deg\": None,\n",
    "        \"station_elevation_m\": None,\n",
    "        \"trace_name\": ,  # Network.Station.Location.Channel\n",
    "        \"trace_sampling_rate_hz\": sampling_rate,\n",
    "        \"trace_start_time\": trace_start,\n",
    "        \"trace_S_arrival_sample\": s_sample,\n",
    "        \"trace_S_onset\": \"impulsive\" if s_sample is not None else None,\n",
    "        \"trace_P_arrival_sample\": p_sample,\n",
    "        \"trace_P_onset\": \"impulsive\" if p_sample is not None else None,\n",
    "        \"trace_snr_db\": None  # No SNR information available\n",
    "    }\n",
    "    \n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# Create the seisbench metadata dataframe\n",
    "seisbench_df = pd.DataFrame(rows)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Created {len(seisbench_df)} metadata entries\")\n",
    "seisbench_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7155fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved H5 file\n",
    "import h5py\n",
    "import pandas as pd\n",
    "output_dir = os.path.expanduser('~/seisbench_data')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# Path to the generated H5 file\n",
    "h5_path = os.path.join(output_dir, 'cascadia_waveforms.h5')\n",
    "\n",
    "if os.path.exists(h5_path):\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        # Print basic file information\n",
    "        print(f\"SeisBench version: {f.attrs.get('seisbench_version')}\")\n",
    "        \n",
    "        # List groups\n",
    "        print(\"\\nGroups in file:\")\n",
    "        for group_name in f.keys():\n",
    "            print(f\" - {group_name}\")\n",
    "        \n",
    "        # Print metadata statistics\n",
    "        print(\"\\nMetadata statistics:\")\n",
    "        if 'metadata' in f:\n",
    "            metadata_group = f['metadata']\n",
    "            for key in metadata_group.keys():\n",
    "                data = metadata_group[key][:]\n",
    "                print(f\" - {key}: {len(data)} entries\")\n",
    "                \n",
    "            # Convert metadata to DataFrame for viewing\n",
    "            metadata_dict = {key: metadata_group[key][:] for key in metadata_group.keys()}\n",
    "            metadata_df = pd.DataFrame(metadata_dict)\n",
    "            print(\"\\nSample of metadata:\")\n",
    "            display(metadata_df.head())\n",
    "        \n",
    "        # Print waveform statistics\n",
    "        print(\"\\nWaveform statistics:\")\n",
    "        if 'waveforms' in f:\n",
    "            waveforms_group = f['waveforms']\n",
    "            print(f\" - Number of waveforms: {len(waveforms_group.keys())}\")\n",
    "            \n",
    "            # Show information for first few waveforms\n",
    "            print(\"\\nSample of waveforms:\")\n",
    "            for i, key in enumerate(list(waveforms_group.keys())[:5]):\n",
    "                waveform = waveforms_group[key][:]\n",
    "                print(f\" - {key}: shape={waveform.shape}, min={waveform.min():.2f}, max={waveform.max():.2f}\")\n",
    "                \n",
    "            # Plot a sample waveform if matplotlib is available\n",
    "            if list(waveforms_group.keys()):\n",
    "                sample_key = list(waveforms_group.keys())[0]\n",
    "                sample_waveform = waveforms_group[sample_key][:]\n",
    "                \n",
    "                plt.figure(figsize=(12, 4))\n",
    "                plt.plot(sample_waveform)\n",
    "                plt.title(f\"Sample Waveform: {sample_key}\")\n",
    "                plt.xlabel(\"Samples\")\n",
    "                plt.ylabel(\"Amplitude\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "else:\n",
    "    print(f\"H5 file not found at {h5_path}\")\n",
    "</VSCode.Cell>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noisepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
