{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f77395",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98014543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import obspy\n",
    "from obspy import Stream\n",
    "from obspy.clients.fdsn import Client\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from obspy import Stream\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "from itertools import islice\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11662011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Initiate clients and constants----------------#\n",
    "# Define clients\n",
    "# client_iris = Client(\"IRIS\")\n",
    "client_ncedc = Client(\"NCEDC\")\n",
    "client_waveform = WaveformClient()\n",
    "\n",
    "# Define constants\n",
    "sampling_rate = 100  # Hz\n",
    "pre_arrival_time = 50\n",
    "window_length = 150\n",
    "\n",
    "# Load the arrival table and define the output file names\n",
    "assoc_df = pd.read_csv('/home/hbito/cascadia_obs_ensemble_backup/data/datasets_all_regions/arrival_assoc_origin_2010_2015_reloc_cog_ver3.csv', index_col=0)\n",
    "output_waveform_file = \"/home/hbito/cascadia_obs_ensemble_backup/data/datasets_all_regions/waveforms_HH_BH_on_the_fly_bulk_backup2.h5\"\n",
    "output_metadata_file = \"/home/hbito/cascadia_obs_ensemble_backup/data/datasets_all_regions/metadata_HH_BH_on_the_fly_bulk_backup2.csv\"\n",
    "error_log_file = \"/home/hbito/cascadia_obs_ensemble_backup/data/datasets_all_regions/save_errors_on_the_fly_bulk_backup2.csv\"\n",
    "\n",
    "# Preprocess dataframe\n",
    "assoc_df[['network', 'station']] = assoc_df['sta'].str.split('.', expand=True)\n",
    "assoc_df['event_id'] = 'ev' + assoc_df['otime'].astype(str).str.replace('.', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5300b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to reorder the traces in a stream\n",
    "def order_traces(stream: Stream, expected_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts an ObsPy stream into a (3, expected_len) numpy array, \n",
    "    consistently ordered as [Z, E, N].\n",
    "\n",
    "    Parameters:\n",
    "    - stream: ObsPy Stream containing cleaned traces (padded to expected_len)\n",
    "    - expected_len: Target length of each waveform trace\n",
    "\n",
    "    Returns:\n",
    "    - data_array: np.ndarray of shape (3, expected_len)\n",
    "    \"\"\"\n",
    "    # Fixed component order: Z → 0, E → 1, N → 2\n",
    "    comp_to_index = {\"Z\": 0, \"E\": 1, \"N\": 2}\n",
    "    data_list = [np.zeros(expected_len) for _ in range(3)]  # Default to zeros\n",
    "\n",
    "    for tr in stream:\n",
    "        chan_suffix = tr.stats.channel[-1]\n",
    "        if chan_suffix in comp_to_index:\n",
    "            idx = comp_to_index[chan_suffix]\n",
    "            data_list[idx] = tr.data  \n",
    "\n",
    "    return np.vstack(data_list)  # Shape: (3, expected_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--------------Gather Station Information----------------#\n",
    "# # Obtain unique network-station combinations\n",
    "# unique_ns = assoc_df.sta.unique()\n",
    "\n",
    "# # Define the start and end times for requesting station information\n",
    "# starttime_bulk = obspy.UTCDateTime(\"2010-01-01T00:00:00\")\n",
    "# endtime_bulk = obspy.UTCDateTime(\"2015-12-31T23:59:59\")\n",
    "\n",
    "# # Make a list of stations for bulk request \n",
    "# bulk =[]\n",
    "# for u_ns in unique_ns:\n",
    "#     n,s = u_ns.split('.')\n",
    "\n",
    "#     for bi in ['EH?', 'BH?', 'HH?']:\n",
    "#         line = (n, s, '*', bi, starttime_bulk, endtime_bulk)\n",
    "#         bulk.append(line)\n",
    "\n",
    "# # Make a bulk request \n",
    "# inv = client_iris.get_stations_bulk(bulk, level='channel')\n",
    "# time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c04f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sta</th>\n",
       "      <th>pick_time</th>\n",
       "      <th>arid</th>\n",
       "      <th>iphase</th>\n",
       "      <th>prob</th>\n",
       "      <th>orid</th>\n",
       "      <th>phase</th>\n",
       "      <th>timeres</th>\n",
       "      <th>slatitude</th>\n",
       "      <th>slongitude</th>\n",
       "      <th>...</th>\n",
       "      <th>nass</th>\n",
       "      <th>p_picks</th>\n",
       "      <th>s_picks</th>\n",
       "      <th>rms</th>\n",
       "      <th>nsphz</th>\n",
       "      <th>gap</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>network</th>\n",
       "      <th>station</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UW.PCMD</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "      <td>0.049</td>\n",
       "      <td>46.888962</td>\n",
       "      <td>-122.301483</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.081</td>\n",
       "      <td>5.0</td>\n",
       "      <td>235.831208</td>\n",
       "      <td>genie</td>\n",
       "      <td>UW</td>\n",
       "      <td>PCMD</td>\n",
       "      <td>ev1262304917_262282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UW.RVW</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>P</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "      <td>1.264</td>\n",
       "      <td>46.149750</td>\n",
       "      <td>-122.742996</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.081</td>\n",
       "      <td>5.0</td>\n",
       "      <td>235.831208</td>\n",
       "      <td>genie</td>\n",
       "      <td>UW</td>\n",
       "      <td>RVW</td>\n",
       "      <td>ev1262304917_262282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UW.GNW</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>2.402</td>\n",
       "      <td>47.564130</td>\n",
       "      <td>-122.824980</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.081</td>\n",
       "      <td>5.0</td>\n",
       "      <td>235.831208</td>\n",
       "      <td>genie</td>\n",
       "      <td>UW</td>\n",
       "      <td>GNW</td>\n",
       "      <td>ev1262304917_262282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PB.B013</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>S</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>-0.651</td>\n",
       "      <td>47.813000</td>\n",
       "      <td>-122.910797</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.081</td>\n",
       "      <td>5.0</td>\n",
       "      <td>235.831208</td>\n",
       "      <td>genie</td>\n",
       "      <td>PB</td>\n",
       "      <td>B013</td>\n",
       "      <td>ev1262304917_262282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PB.B943</td>\n",
       "      <td>1.262305e+09</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>-0.511</td>\n",
       "      <td>47.813202</td>\n",
       "      <td>-122.911301</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.081</td>\n",
       "      <td>5.0</td>\n",
       "      <td>235.831208</td>\n",
       "      <td>genie</td>\n",
       "      <td>PB</td>\n",
       "      <td>B943</td>\n",
       "      <td>ev1262304917_262282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690483</th>\n",
       "      <td>7D.J11D</td>\n",
       "      <td>1.435102e+09</td>\n",
       "      <td>1004326</td>\n",
       "      <td>P</td>\n",
       "      <td>0.694</td>\n",
       "      <td>63886</td>\n",
       "      <td>P</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>43.541599</td>\n",
       "      <td>-126.368599</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.447</td>\n",
       "      <td>5.0</td>\n",
       "      <td>247.683119</td>\n",
       "      <td>genie</td>\n",
       "      <td>7D</td>\n",
       "      <td>J11D</td>\n",
       "      <td>ev1435101498_841147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690484</th>\n",
       "      <td>7D.J19D</td>\n",
       "      <td>1.435102e+09</td>\n",
       "      <td>1004327</td>\n",
       "      <td>P</td>\n",
       "      <td>0.694</td>\n",
       "      <td>63886</td>\n",
       "      <td>P</td>\n",
       "      <td>-0.419</td>\n",
       "      <td>44.179001</td>\n",
       "      <td>-126.271202</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.447</td>\n",
       "      <td>5.0</td>\n",
       "      <td>247.683119</td>\n",
       "      <td>genie</td>\n",
       "      <td>7D</td>\n",
       "      <td>J19D</td>\n",
       "      <td>ev1435101498_841147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690485</th>\n",
       "      <td>7D.J10D</td>\n",
       "      <td>1.435102e+09</td>\n",
       "      <td>1004328</td>\n",
       "      <td>P</td>\n",
       "      <td>0.694</td>\n",
       "      <td>63886</td>\n",
       "      <td>P</td>\n",
       "      <td>-0.505</td>\n",
       "      <td>43.348499</td>\n",
       "      <td>-125.545097</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.447</td>\n",
       "      <td>5.0</td>\n",
       "      <td>247.683119</td>\n",
       "      <td>genie</td>\n",
       "      <td>7D</td>\n",
       "      <td>J10D</td>\n",
       "      <td>ev1435101498_841147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690486</th>\n",
       "      <td>7D.J27D</td>\n",
       "      <td>1.435102e+09</td>\n",
       "      <td>1004329</td>\n",
       "      <td>P</td>\n",
       "      <td>0.694</td>\n",
       "      <td>63886</td>\n",
       "      <td>P</td>\n",
       "      <td>0.915</td>\n",
       "      <td>44.848900</td>\n",
       "      <td>-126.308296</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.447</td>\n",
       "      <td>5.0</td>\n",
       "      <td>247.683119</td>\n",
       "      <td>genie</td>\n",
       "      <td>7D</td>\n",
       "      <td>J27D</td>\n",
       "      <td>ev1435101498_841147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690487</th>\n",
       "      <td>7D.G35D</td>\n",
       "      <td>1.435102e+09</td>\n",
       "      <td>1004331</td>\n",
       "      <td>S</td>\n",
       "      <td>0.694</td>\n",
       "      <td>63886</td>\n",
       "      <td>S</td>\n",
       "      <td>0.358</td>\n",
       "      <td>42.555698</td>\n",
       "      <td>-126.399002</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.447</td>\n",
       "      <td>5.0</td>\n",
       "      <td>247.683119</td>\n",
       "      <td>genie</td>\n",
       "      <td>7D</td>\n",
       "      <td>G35D</td>\n",
       "      <td>ev1435101498_841147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690488 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sta     pick_time     arid iphase   prob   orid phase  timeres  \\\n",
       "0       UW.PCMD  1.262305e+09        0      P  0.680      0     P    0.049   \n",
       "1        UW.RVW  1.262305e+09        1      P  0.680      0     P    1.264   \n",
       "2        UW.GNW  1.262305e+09        3      S  0.680      0     S    2.402   \n",
       "3       PB.B013  1.262305e+09        4      S  0.680      0     S   -0.651   \n",
       "4       PB.B943  1.262305e+09        5      S  0.680      0     S   -0.511   \n",
       "...         ...           ...      ...    ...    ...    ...   ...      ...   \n",
       "690483  7D.J11D  1.435102e+09  1004326      P  0.694  63886     P   -0.336   \n",
       "690484  7D.J19D  1.435102e+09  1004327      P  0.694  63886     P   -0.419   \n",
       "690485  7D.J10D  1.435102e+09  1004328      P  0.694  63886     P   -0.505   \n",
       "690486  7D.J27D  1.435102e+09  1004329      P  0.694  63886     P    0.915   \n",
       "690487  7D.G35D  1.435102e+09  1004331      S  0.694  63886     S    0.358   \n",
       "\n",
       "        slatitude  slongitude  ...  nass  p_picks  s_picks    rms  nsphz  \\\n",
       "0       46.888962 -122.301483  ...     7        2        5  1.081    5.0   \n",
       "1       46.149750 -122.742996  ...     7        2        5  1.081    5.0   \n",
       "2       47.564130 -122.824980  ...     7        2        5  1.081    5.0   \n",
       "3       47.813000 -122.910797  ...     7        2        5  1.081    5.0   \n",
       "4       47.813202 -122.911301  ...     7        2        5  1.081    5.0   \n",
       "...           ...         ...  ...   ...      ...      ...    ...    ...   \n",
       "690483  43.541599 -126.368599  ...     9        4        5  0.447    5.0   \n",
       "690484  44.179001 -126.271202  ...     9        4        5  0.447    5.0   \n",
       "690485  43.348499 -125.545097  ...     9        4        5  0.447    5.0   \n",
       "690486  44.848900 -126.308296  ...     9        4        5  0.447    5.0   \n",
       "690487  42.555698 -126.399002  ...     9        4        5  0.447    5.0   \n",
       "\n",
       "               gap  algorithm  network  station             event_id  \n",
       "0       235.831208      genie       UW     PCMD  ev1262304917_262282  \n",
       "1       235.831208      genie       UW      RVW  ev1262304917_262282  \n",
       "2       235.831208      genie       UW      GNW  ev1262304917_262282  \n",
       "3       235.831208      genie       PB     B013  ev1262304917_262282  \n",
       "4       235.831208      genie       PB     B943  ev1262304917_262282  \n",
       "...            ...        ...      ...      ...                  ...  \n",
       "690483  247.683119      genie       7D     J11D  ev1435101498_841147  \n",
       "690484  247.683119      genie       7D     J19D  ev1435101498_841147  \n",
       "690485  247.683119      genie       7D     J10D  ev1435101498_841147  \n",
       "690486  247.683119      genie       7D     J27D  ev1435101498_841147  \n",
       "690487  247.683119      genie       7D     G35D  ev1435101498_841147  \n",
       "\n",
       "[690488 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------Gather Waveform Information----------------#\n",
    "# Obtain uniquee otime-network-station combinations\n",
    "unique_n_s_otime = assoc_df.drop_duplicates(['event_id', 'network', 'station'],keep='first').reset_index(drop=True)\n",
    "unique_n_s_otime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5d9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to append entries for the bulk request\n",
    "def append_bulk_lists_chunks(bulk_waveforms, n, s, bi, trace_start, trace_end, day_end, next_day_start):\n",
    "    \"\"\"\n",
    "    Append waveform requests to the bulk list based on the availability of HH? and BH? channels. If the stream runs over the midnight, split the request into two.\n",
    "    \"\"\"\n",
    "    if day_end > trace_end:\n",
    "        # If the trace end is within the same day, we can use HH?\n",
    "        bulk_waveforms.append((n, s, '*', bi, trace_start, trace_end))\n",
    "    else:\n",
    "        # If the trace end goes beyond the day, we need to adjust\n",
    "        bulk_waveforms.append((n, s, '*', bi, trace_start, day_end))\n",
    "        bulk_waveforms.append((n, s, '*', bi, next_day_start, trace_end))\n",
    "    return bulk_waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63d7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_bulk_lists(bulk_waveforms, n, s, bi, trace_start, trace_end):\n",
    "    \"\"\"\n",
    "    Append waveform requests to the bulk list based on the availability of HH? and BH? channels.\n",
    "    \"\"\"\n",
    "    bulk_waveforms.append((n, s, '*', bi, trace_start, trace_end))\n",
    "\n",
    "    return bulk_waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72e2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [06:44<00:00, 36.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# compose \n",
    "batches_bulk_waveforms_chunks =[]\n",
    "batches_bulk_waveforms_chunks_ncedc =[]\n",
    "\n",
    "batches_bulk_waveforms = []\n",
    "num_batches = 10\n",
    "len_batches = len(unique_n_s_otime) // num_batches\n",
    "\n",
    "count_EH_pairs = 0\n",
    "\n",
    "# Constants\n",
    "sampling_rate = 100  # Hz\n",
    "pre_arrival_time = 50\n",
    "window_length = 150\n",
    "\n",
    "for i in tqdm(range(0, num_batches+1)):\n",
    "    bulk_waveforms_chunks = []\n",
    "    bulk_waveforms_chunks_ncedc = []\n",
    "    bulk_waveforms = []\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    for index, u_ns in islice(unique_n_s_otime.iterrows(), i*len_batches, (i + 1) * len_batches):\n",
    "        n,s = u_ns['network'], u_ns['station']\n",
    "\n",
    "        first_arrival = u_ns['otime']\n",
    "        pick_time = u_ns['pick_time']\n",
    "        trace_start = obspy.UTCDateTime(first_arrival - pre_arrival_time)\n",
    "        trace_end = trace_start + window_length\n",
    "\n",
    "        day_end = obspy.UTCDateTime(trace_start.date + timedelta(days=1))-1e-6\n",
    "        next_day_start = obspy.UTCDateTime(trace_start.date + timedelta(days=1))\n",
    "\n",
    "        # print(trace_start, trace_end)\n",
    "\n",
    "        sta = inv.select(network=n, station=s, time=pick_time)\n",
    "\n",
    "        has_Z = bool(sta.select(channel='??Z'))\n",
    "        has_HH = bool(sta.select(channel='HH?'))\n",
    "        has_BH = bool(sta.select(channel='BH?'))\n",
    "\n",
    "        if not has_Z or not (has_HH or has_BH):\n",
    "            count_EH_pairs += 1\n",
    "            # print(\"count_EH_pairs\", count_EH_pairs)\n",
    "            continue\n",
    "        \n",
    "        if has_HH:\n",
    "            if n in ['NC', 'BK']:\n",
    "                bulk_waveforms_chunks_ncedc = append_bulk_lists_chunks(bulk_waveforms_chunks_ncedc, n, s, 'HH?', trace_start, trace_end, day_end, next_day_start)\n",
    "            else:\n",
    "                bulk_waveforms_chunks = append_bulk_lists_chunks(bulk_waveforms_chunks, n, s, 'HH?', trace_start, trace_end, day_end, next_day_start)\n",
    "            \n",
    "            bulk_waveforms = append_bulk_lists(bulk_waveforms, n, s, 'HH?', trace_start, trace_end)\n",
    "\n",
    "        else:\n",
    "            if n in ['NC', 'BK']:\n",
    "                bulk_waveforms_chunks_ncedc = append_bulk_lists_chunks(bulk_waveforms_chunks_ncedc, n, s, 'BH?', trace_start, trace_end, day_end, next_day_start)\n",
    "            else:\n",
    "                bulk_waveforms_chunks = append_bulk_lists_chunks(bulk_waveforms_chunks, n, s, 'BH?', trace_start, trace_end, day_end, next_day_start)\n",
    "            \n",
    "            bulk_waveforms = append_bulk_lists(bulk_waveforms, n, s, 'BH?', trace_start, trace_end)\n",
    "\n",
    "    batches_bulk_waveforms_chunks.append(bulk_waveforms_chunks)\n",
    "    batches_bulk_waveforms_chunks_ncedc.append(bulk_waveforms_chunks_ncedc)\n",
    "    batches_bulk_waveforms.append(bulk_waveforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3458c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Create Waveform Datasets in batches----------------#\n",
    "# Find entries that have already been processed\n",
    "processed_keys = set()\n",
    "if os.path.exists(output_metadata_file):\n",
    "    processed_df = pd.read_csv(output_metadata_file)\n",
    "    processed_keys = set(zip(processed_df['trace_start_time'], processed_df['station_network_code'], processed_df['station_code']))\n",
    "    print(f\"Loaded {len(processed_keys)} processed entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50de6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open output files\n",
    "h5f = h5py.File(output_waveform_file, \"a\")\n",
    "meta_out = open(output_metadata_file, \"a\")\n",
    "write_header = os.stat(output_metadata_file).st_size == 0 if os.path.exists(output_metadata_file) else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da171d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = ['event_id', 'source_origin_time', 'source_latitude_deg', 'source_longitude_deg',\n",
    "              'source_type', 'source_depth_km', 'preferred_source_magnitude', 'preferred_source_magnitude_type',\n",
    "              'preferred_source_magnitude_uncertainty', 'source_depth_uncertainty_km', 'source_horizontal_uncertainty_km',\n",
    "              'station_network_code', 'station_channel_code', 'station_code', 'station_location_code',\n",
    "              'station_latitude_deg', 'station_longitude_deg', 'station_elevation_m', 'trace_name',\n",
    "              'trace_sampling_rate_hz', 'trace_start_time', 'trace_S_arrival_sample', 'trace_P_arrival_sample',\n",
    "              'trace_S_arrival_uncertainty_s', 'trace_P_arrival_uncertainty_s', 'trace_P_polarity',\n",
    "              'trace_S_onset', 'trace_P_onset', 'trace_snr_db', 'source_type_pnsn_label',\n",
    "              'source_local_magnitude', 'source_local_magnitude_uncertainty', 'source_duration_magnitude',\n",
    "              'source_duration_magnitude_uncertainty', 'source_hand_magnitude', 'trace_missing_channel', 'trace_has_offset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1ab315",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_writer = csv.DictWriter(meta_out, fieldnames=fieldnames)\n",
    "\n",
    "if write_header:\n",
    "    meta_writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48879f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 100  # Hz\n",
    "window_length = 150  # seconds\n",
    "expected_len = int(sampling_rate * window_length)\n",
    "\n",
    "i_iter = 0\n",
    "\n",
    "# for i in range(len(batches_bulk_waveforms)):\n",
    "for i in range(1):\n",
    "    print(\"Batch\",i)\n",
    "    batch_chunk = batches_bulk_waveforms_chunks[1]\n",
    "    batch_chunk_ncedc = batches_bulk_waveforms_chunks_ncedc[1]\n",
    "    batch = batches_bulk_waveforms[1]\n",
    "\n",
    "    save_errors = []\n",
    "\n",
    "\n",
    "    st = Stream()\n",
    "\n",
    "    for j in range(len(batch_chunk)):\n",
    "        n, s, loc, bi, trace_start_time, trace_end_time = batch_chunk[j]\n",
    "        try: \n",
    "            st1 = client_waveform.get_waveforms(network=n, station=s, location=loc, channel=bi,\n",
    "                                                starttime=trace_start_time, endtime=trace_end_time)\n",
    "            st.extend(st1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching waveforms for {n}.{s} {bi} from {trace_start_time} to {trace_end_time}: {e}\")\n",
    "            # Write error immediately\n",
    "            continue\n",
    "    print('Finished downloading from WaveformClient')\n",
    "\n",
    "    for j in range(len(batch_chunk_ncedc)):\n",
    "        n, s, loc, bi, trace_start_time, trace_end_time = batch_chunk_ncedc[j]\n",
    "        try: \n",
    "            st2 = client_ncedc.get_waveforms(network=n, station=s, location=loc, channel=bi,\n",
    "                                                 starttime=trace_start_time, endtime=trace_end_time)\n",
    "            time.sleep(0.2)\n",
    "            st.extend(st2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching waveforms for {n}.{s} {bi} from {trace_start_time} to {trace_end_time}: {e}\")\n",
    "            # Write error immediately\n",
    "            continue\n",
    "    print('Finished downloading NCEDC')    \n",
    "\n",
    "    \n",
    "\n",
    "    # print(\"Requesting waveforms.\")\n",
    "    # if len(batch_chunk) != 0:\n",
    "    #     st1 = client_waveform.get_waveforms_bulk(batch_chunk)\n",
    "    #     time.sleep(0.2) # Stop the execution to avoid making too many requests to the server\n",
    "    # if len(batch_chunk_ncedc) != 0:\n",
    "    #     st2 = client_ncedc.get_waveforms_bulk(batch_chunk_ncedc)\n",
    "    #     time.sleep(0.2) # Stop the execution to avoid making too many requests to the server\n",
    "    # if len(st1) == 0 and len(st2) == 0:\n",
    "    #     print(f\"Batch {i+1} has no waveform requests.\")\n",
    "    #     continue\n",
    "\n",
    "    time.sleep(0.2) # Stop the execution to avoid making too many requests to the server\n",
    "\n",
    "    # st = st1.extend(st2) if len(st2) != 0 else st1\n",
    "\n",
    "    for n_s_time in tqdm(batch[775:875]):\n",
    "        i_iter += 1\n",
    "        network, station, location, channel, trace_start_time, trace_end_time = n_s_time\n",
    "\n",
    "        rows_sta  = assoc_df.loc[(assoc_df['sta'] == f\"{network}.{station}\") & (abs(assoc_df['otime'] - float(trace_start_time + timedelta(seconds=pre_arrival_time))) < 1)]\n",
    "        \n",
    "\n",
    "        p_arrival = rows_sta[rows_sta['iphase'] == 'P']\n",
    "        s_arrival = rows_sta[rows_sta['iphase'] == 'S']\n",
    "\n",
    "        key = (str(trace_start_time), network, station)\n",
    "        if key in processed_keys:\n",
    "            print(f\"Skipping already processed entry: {key}\")\n",
    "            # time.sleep(0.2)\n",
    "            continue\n",
    "\n",
    "        # inv_n_s_time = inv.select(network=network, station=station, location=location, channel='*',\n",
    "        #                            starttime=trace_start_time, endtime=trace_end_time)\n",
    "\n",
    "        # inv_n_s_time = inv.select(network=network, station=station, location=location, channel='*')\n",
    "        # print('inv_n_s_time', inv_n_s_time)\n",
    "        st_n_s = st.select(id=f\"{network}.{station}.*.{channel}\",)\n",
    "        print('st_n_s', st_n_s)\n",
    "\n",
    "        st_n_s_time = Stream([tr for tr in st_n_s if tr.stats.starttime > (trace_start_time-1) and tr.stats.endtime < (trace_end_time+1)]) # Tolerate the error of 1 second when selecting the traces in the stream for the specific time window\n",
    "        st_n_s_time.merge(method=0, fill_value='interpolate')\n",
    "        st_n_s_time.detrend()\n",
    "        st_n_s_time.resample(sampling_rate)\n",
    "\n",
    "        cleaned_stream = Stream()\n",
    "        print('st_n_s_time', st_n_s_time)\n",
    "        for tr in st_n_s_time:\n",
    "            trace_data = tr.data[:expected_len]\n",
    "            if len(trace_data) < expected_len:\n",
    "                trace_data = np.pad(trace_data, (0, expected_len - len(trace_data)), mode=\"constant\") # Pads zeros at the end\n",
    "            tr.data = trace_data\n",
    "            cleaned_stream.append(tr)\n",
    "\n",
    "        print('cleaned_stream', cleaned_stream)\n",
    "\n",
    "        _cleaned_stream = order_traces(cleaned_stream, expected_len)\n",
    "\n",
    "        try:\n",
    "            data = np.stack(_cleaned_stream, axis=0)\n",
    "    #         data = np.stack([tr.data[:window_length * sampling_rate - 2] for tr in waveform], axis=0)\n",
    "        except Exception as e:\n",
    "            # Write error immediately\n",
    "            file_exists = os.path.exists(error_log_file)\n",
    "            with open(error_log_file, \"a\", newline=\"\") as errfile:\n",
    "                writer = csv.DictWriter(errfile, fieldnames=['i_iter', 'network', 'station', 'starttime', 'endtime', 'stage', 'error'])\n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "                writer.writerow({'i_iter': i_iter, 'network': network, 'station': station, 'starttime': trace_start_time, 'endtime': trace_end_time, 'stage': 'metadata_write', 'error': str(e)})\n",
    "            continue\n",
    "\n",
    "        bucket = str(random.randint(0, 10))\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            dset_path = f\"/data/{bucket}\"\n",
    "            if dset_path not in h5f:\n",
    "                h5f.create_dataset(dset_path, data=np.expand_dims(data, axis=0), maxshape=(None, *data.shape), chunks=True, dtype='float32')\n",
    "                dataset_index = 0\n",
    "            else:\n",
    "                dset = h5f[dset_path]\n",
    "                dataset_index = dset.shape[0]\n",
    "                dset.resize((dataset_index + 1), axis=0)\n",
    "                dset[dataset_index] = data\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing to HDF5 for bucket {bucket}: {e}\")\n",
    "            # Write error immediately\n",
    "            file_exists = os.path.exists(error_log_file)\n",
    "            with open(error_log_file, \"a\", newline=\"\") as errfile:\n",
    "                writer = csv.DictWriter(errfile, fieldnames=['i_iter', 'network', 'station', 'starttime', 'endtime', 'stage', 'error'])\n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "                writer.writerow({'i_iter': i_iter, 'network': network, 'station': station, 'starttime': trace_start_time, 'endtime': trace_end_time, 'stage': 'metadata_write', 'error': str(e)})\n",
    "            continue\n",
    "\n",
    "        trace_name = f\"{bucket}${dataset_index},:{data.shape[0]},:{data.shape[1]}\"\n",
    "\n",
    "        print(network, station, location, channel, trace_start_time, trace_end_time)\n",
    "        print(rows_sta)\n",
    "        print(rows_sta['lat'].iloc[0])\n",
    "        print(rows_sta['lat'].iloc[0])\n",
    "        print(rows_sta['lon'].iloc[0])\n",
    "        print(rows_sta['depth'].iloc[0])\n",
    "        print(s_arrival['pick_time'].iloc[0] if not s_arrival.empty else None)\n",
    "        # print(inv_n_s_time[0][0].latitude)\n",
    "        # print(cleaned_stream[0].stats.channel[:-1])\n",
    "\n",
    "\n",
    "        try:\n",
    "            row = {\n",
    "                'event_id': rows_sta['event_id'].iloc[0],\n",
    "                'source_origin_time': rows_sta['otime'].iloc[0],\n",
    "                'source_latitude_deg': rows_sta['lat'].iloc[0],\n",
    "                'source_longitude_deg': rows_sta['lon'].iloc[0],\n",
    "                'source_type': \"earthquake\",\n",
    "                'source_depth_km': rows_sta['depth'].iloc[0],\n",
    "                'preferred_source_magnitude': None,\n",
    "                'preferred_source_magnitude_type': None,\n",
    "                'preferred_source_magnitude_uncertainty': None,\n",
    "                'source_depth_uncertainty_km': None,\n",
    "                'source_horizontal_uncertainty_km': None,\n",
    "                'station_network_code': network,\n",
    "                'station_channel_code': cleaned_stream[0].stats.channel[:-1],\n",
    "                'station_code': station,\n",
    "                'station_location_code': \"\",\n",
    "                'station_latitude_deg': None,\n",
    "                'station_longitude_deg': None,\n",
    "                'station_elevation_m': None,\n",
    "                'trace_name': trace_name,\n",
    "                'trace_sampling_rate_hz': sampling_rate,\n",
    "                'trace_start_time': trace_start_time,\n",
    "                'trace_S_arrival_sample': int((s_arrival['pick_time'].iloc[0] - (rows_sta['otime'].iloc[0] - pre_arrival_time)) * sampling_rate)if not s_arrival.empty else None,\n",
    "                'trace_P_arrival_sample': int((p_arrival['pick_time'].iloc[0] - (rows_sta['otime'].iloc[0] - pre_arrival_time)) * sampling_rate) if not p_arrival.empty else None,\n",
    "                'trace_S_arrival_uncertainty_s': None,\n",
    "                'trace_P_arrival_uncertainty_s': None,\n",
    "                'trace_P_polarity': None,\n",
    "                'trace_S_onset': \"impulsive\"if not s_arrival.empty else None,\n",
    "                'trace_P_onset': \"impulsive\" if not p_arrival.empty else None,\n",
    "                'trace_snr_db': None,\n",
    "                'source_type_pnsn_label': None,\n",
    "                'source_local_magnitude': None,\n",
    "                'source_local_magnitude_uncertainty': None,\n",
    "                'source_duration_magnitude': None,\n",
    "                'source_duration_magnitude_uncertainty': None,\n",
    "                'source_hand_magnitude': None,\n",
    "                'trace_missing_channel': \"\",\n",
    "                'trace_has_offset': None\n",
    "            }\n",
    "            meta_writer.writerow(row)\n",
    "            meta_out.flush()\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing metadata for {station}/{trace_start_time}: {e}\")\n",
    "            # Write error immediately\n",
    "            file_exists = os.path.exists(error_log_file)\n",
    "            with open(error_log_file, \"a\", newline=\"\") as errfile:\n",
    "                writer = csv.DictWriter(errfile, fieldnames=['i_iter', 'network', 'station', 'starttime', 'endtime', 'stage', 'error'])\n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "                writer.writerow({'i_iter': i_iter, 'network': network, 'station': station, 'starttime': trace_start_time, 'endtime': trace_end_time, 'stage': 'metadata_write', 'error': str(e)})\n",
    "            continue\n",
    "            \n",
    "\n",
    "h5f.close()\n",
    "meta_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1905ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3447"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f5f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
