{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59ec9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "import numpy as np\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy.clients.fdsn import Client\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from pnwstore.mseed import WaveformClient\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import pandas as pd\n",
    "import gc\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "from ELEP.elep.trigger_func import picks_summary_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66e7e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec5a0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clients\n",
    "client_inventory = Client('IRIS')\n",
    "client_waveform = WaveformClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b976476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in whatever you need to start - likely a list of station codes\n",
    "station_list = ['J57A', 'J41A', 'M09B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff0cf559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify some parameters - you can change what you specify here vs. within the large function, this is just an example.\n",
    "# Depending on whether the pertained models take a long time to load every time, you may want to load those outside the function and just feed them to the function rather than loading them every time in parallel.\n",
    "twin = 6000     # length of time window\n",
    "step = 3000     # step length\n",
    "l_blnd, r_blnd = 500, 500\n",
    "filepath = 'https://cascadia.ess.washington.edu/jhub/user/hbito/notebooks/elep-test/surface_events/src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5db51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download models\n",
    "pretrain_list = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd7d1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for stacking the segmented time windows after prediction\n",
    "def stacking(data, npts, l_blnd, r_blnd):\n",
    "    _data = data.copy()\n",
    "    stack = np.full(npts, np.nan, dtype = np.float32)\n",
    "    _data[:, :l_blnd] = np.nan; _data[:, -r_blnd:] = np.nan\n",
    "    stack[:twin] = _data[0, :]\n",
    "    for iseg in range(nseg-1):\n",
    "        idx = step*(iseg+1)\n",
    "        stack[idx:idx + twin] = \\\n",
    "                np.nanmax([stack[idx:idx + twin], _data[iseg+1, :]], axis = 0)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cdf915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function that you want to run in parallel: I recommend you design this to essentially perform your entire workflow on one station for one day, and write a csv file for that station, much the way you already have it.\n",
    "# This is what will run in parallel!\n",
    "# So, the only inputs are the station name, the start and end times you want to detect for, the path of the folder you want to write the results to, and the parameters you already specified. Here is where you could also feed in the preloaded models if that becomes important.\n",
    "def run_detection(station,t1,t2,filepath,twin,step,l_blnd,r_blnd):\n",
    "\t# Load data\n",
    "\t# Reshape data\n",
    "\t# Predict on base models\n",
    "\t# Stack\n",
    "\t# Create and write csv file. Define file name using the station code and the input filepath\n",
    "    \n",
    "    # Get the inventory for the stations\n",
    "    stations = station\n",
    "    network = '7D'\n",
    "    channels = '?H?'\n",
    "    client = client_inventory\n",
    "    inventory = client.get_stations(network=network, station=stations)\n",
    "    \n",
    "    # Get waveforms and filter\n",
    "    sdata = client_waveform.get_waveforms(network=\"7D\", station=station, channel=\"BH?\", starttime=t1, year=t1.strftime('%Y'), month=t1.strftime('%m'), day=t1.strftime('%d'))\n",
    "    sdata.filter(type='bandpass',freqmin=4,freqmax=15)\n",
    "\n",
    "    # Get the necassary information about the station\n",
    "    delta = sdata[0].stats.delta\n",
    "    starttime = sdata[0].stats.starttime\n",
    "    fs = sdata[0].stats.sampling_rate\n",
    "    dt = 1/fs\n",
    "    \n",
    "    # Reshaping data\n",
    "    sdata = np.array(sdata)\n",
    "    npts = sdata.shape[1]\n",
    "    nseg = int(np.ceil((npts - twin) / step)) + 1\n",
    "    windows = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    tap = 0.5 * (1 + np.cos(np.linspace(np.pi, 2 * np.pi, 6)))\n",
    "    \n",
    "    # Define the parameters for semblance\n",
    "    paras_semblance = {'dt':dt, 'semblance_order':2, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "    p_thrd, s_thrd = 0.05, 0.05\n",
    "\n",
    "    windows_std = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    windows_max = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    windows = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    windows_idx = np.zeros(nseg, dtype=np.int32)\n",
    "\n",
    "    for iseg in range(nseg):\n",
    "        idx = iseg * step\n",
    "        windows[iseg, :] = sdata[:, idx:idx + twin]\n",
    "        windows[iseg, :] -= np.mean(windows[iseg, :], axis=-1, keepdims=True)\n",
    "        # original use std norm\n",
    "        windows_std[iseg, :] = windows[iseg, :] / np.std(windows[iseg, :]) + 1e-10\n",
    "        # others use max norm\n",
    "        windows_max[iseg, :] = windows[iseg, :] / (np.max(np.abs(windows[iseg, :]), axis=-1, keepdims=True))\n",
    "        windows_idx[iseg] = idx\n",
    "\n",
    "    # taper\n",
    "    windows_std[:, :, :6] *= tap; windows_std[:, :, -6:] *= tap[::-1]; \n",
    "    windows_max[:, :, :6] *= tap; windows_max[:, :, -6:] *= tap[::-1];\n",
    "    del windows\n",
    "\n",
    "#     print(f\"Window data shape: {windows_std.shape}\")\n",
    "    \n",
    "    # Predict on base models\n",
    "    \n",
    "    pretrain_list = ['original', 'ethz', 'instance', 'scedc', 'stead']\n",
    "\n",
    "    # dim 0: 0 = P, 1 = S\n",
    "    batch_pred = np.zeros([2, len(pretrain_list), nseg, twin], dtype = np.float32) \n",
    "\n",
    "    for ipre, pretrain in enumerate(pretrain_list):\n",
    "        t0 = time.time()\n",
    "        eqt = sbm.EQTransformer.from_pretrained(pretrain)\n",
    "        eqt.to(device);\n",
    "        eqt._annotate_args['overlap'] = ('Overlap between prediction windows in samples \\\n",
    "                                        (only for window prediction models)', step)\n",
    "        eqt._annotate_args['blinding'] = ('Number of prediction samples to discard on \\\n",
    "                                         each side of each window prediction', (l_blnd, r_blnd))\n",
    "        eqt.eval();\n",
    "        if pretrain == 'original':\n",
    "            # batch prediction through torch model\n",
    "            windows_std_tt = torch.Tensor(windows_std)\n",
    "            _torch_pred = eqt(windows_std_tt.to(device))\n",
    "        else:\n",
    "            windows_max_tt = torch.Tensor(windows_max)\n",
    "            _torch_pred = eqt(windows_max_tt.to(device))\n",
    "        batch_pred[0, ipre, :] = _torch_pred[1].detach().cpu().numpy()\n",
    "        batch_pred[1, ipre, :] = _torch_pred[2].detach().cpu().numpy()\n",
    "\n",
    "#         t1 = time.time()\n",
    "#         print(f\"picking using [{pretrain}] model: %.3f second\" % (t1 - t0))\n",
    "\n",
    "    # clean up memory\n",
    "    del _torch_pred, windows_max_tt, windows_std_tt\n",
    "    del windows_std, windows_max\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"All prediction shape: {batch_pred.shape}\")\n",
    "    \n",
    "    pretrain_pred = np.zeros([2, len(pretrain_list), npts], dtype = np.float32)\n",
    "    for ipre, pretrain in enumerate(pretrain_list):\n",
    "        # 0 for P-wave\n",
    "        pretrain_pred[0, ipre, :] = stacking(batch_pred[0, ipre, :], npts, l_blnd, r_blnd)\n",
    "\n",
    "        # 1 for S-wave\n",
    "        pretrain_pred[1, ipre, :] = stacking(batch_pred[1, ipre, :], npts, l_blnd, r_blnd)\n",
    "    \n",
    "    smb_pred = np.zeros([2, nseg, twin], dtype = np.float32)\n",
    "    # calculate the semblance\n",
    "    ## the semblance may takes a while bit to calculate\n",
    "    for iseg in tqdm(range(nseg)):\n",
    "        # 0 for P-wave\n",
    "        smb_pred[0, iseg, :] = ensemble_semblance(batch_pred[0, :, iseg, :], paras_semblance)\n",
    "\n",
    "        # 1 for P-wave\n",
    "        smb_pred[1, iseg, :] = ensemble_semblance(batch_pred[1, :, iseg, :], paras_semblance)\n",
    "\n",
    "    ## ... and stack\n",
    "    # 0 for P-wave\n",
    "    smb_p = stacking(smb_pred[0, :], npts, l_blnd, r_blnd)\n",
    "\n",
    "    # 1 for P-wave\n",
    "    smb_s = stacking(smb_pred[1, :], npts, l_blnd, r_blnd)\n",
    "\n",
    "    # clean-up RAM\n",
    "    del smb_pred, batch_pred\n",
    "\n",
    "    p_index = picks_summary_simple(smb_p, p_thrd)\n",
    "    s_index = picks_summary_simple(smb_s, s_thrd)\n",
    "    print(f\"{len(p_index)} P picks\\n{len(s_index)} S picks\")\n",
    "    \n",
    "    # Create lists and a data frame\n",
    "    event_id = []\n",
    "    source_type = []\n",
    "    station_network_code = []\n",
    "    station_channel_code = []\n",
    "    station_code = []\n",
    "    station_location_code = []\n",
    "    station_latitude_deg= []\n",
    "    station_longitude_deg = []\n",
    "    station_elevation_m = []\n",
    "    trace_name = []\n",
    "    trace_sampling_rate_hz = []\n",
    "    trace_start_time = []\n",
    "    trace_S_arrival_sample = []\n",
    "    trace_P_arrival_sample = []\n",
    "    trace_S_onset = []\n",
    "    trace_P_onset = []\n",
    "    trace_snr_db = []\n",
    "    trace_p_arrival = []\n",
    "    trace_s_arrival = []\n",
    "\n",
    "    for i, idx in enumerate(p_index):\n",
    "        event_id.append(' ')\n",
    "        source_type.append(' ')\n",
    "        station_network_code.append('7D')\n",
    "        station_channel_code.append(' ')\n",
    "        station_code.append(station)\n",
    "        station_location_code.append(station.stats.location)   \n",
    "        station_latitude_deg.append(inventory[0][0].latitude)\n",
    "        station_longitude_deg.append(inventory[0][0].longitude)   \n",
    "        station_elevation_m.append(inventory[0][0].elevation)\n",
    "        trace_name.append(' ')\n",
    "        trace_sampling_rate_hz.append(station.stats.sampling_rate)\n",
    "        trace_start_time.append(station.stats.starttime)\n",
    "        trace_S_arrival_sample.append(' ')\n",
    "        trace_P_arrival_sample.append(' ')\n",
    "        trace_S_onset.append(' ')\n",
    "        trace_P_onset.append(' ')\n",
    "        trace_snr_db.append(' ')\n",
    "        trace_s_arrival.append(np.nan)\n",
    "        trace_p_arrival.append(str(starttime  + idx * delta))\n",
    "\n",
    "    for i, idx in enumerate(s_index):\n",
    "        event_id.append(' ')\n",
    "        source_type.append(' ')\n",
    "        station_network_code.append('7D')\n",
    "        station_channel_code.append(' ')\n",
    "        station_code.append(station)\n",
    "        station_location_code.append(station.stats.location)   \n",
    "        station_latitude_deg.append(inventory[0][0].latitude)\n",
    "        station_longitude_deg.append(inventory[0][0].longitude)   \n",
    "        station_elevation_m.append(inventory[0][0].elevation)\n",
    "        trace_name.append(' ')\n",
    "        trace_sampling_rate_hz.append(station.stats.sampling_rate)\n",
    "        trace_start_time.append(station.stats.starttime)\n",
    "        trace_S_arrival_sample.append(' ')\n",
    "        trace_P_arrival_sample.append(' ')\n",
    "        trace_S_onset.append(' ')\n",
    "        trace_P_onset.append(' ')\n",
    "        trace_snr_db.append(' ')\n",
    "        trace_s_arrival.append(str(starttime  + idx * delta))\n",
    "        trace_p_arrival.append(np.nan)\n",
    "\n",
    "    # dictionary of lists\n",
    "    dict = {'event_id':event_id,'source_type':source_type,'station_network_code':station_network_code,\\\n",
    "            'station_channel_code':station_channel_code,'station_code':station_code,'station_location_code':station_location_code,\\\n",
    "            'station_latitude_deg':station_latitude_deg,'station_longitude_deg':station_longitude_deg, \\\n",
    "            'station_elevation_m':station_elevation_m,'trace_name':trace_name,'trace_sampling_rate_hz':trace_sampling_rate_hz,\\\n",
    "            'trace_start_time':trace_start_time,'trace_S_arrival_sample':trace_S_arrival_sample,\\\n",
    "            'trace_P_arrival_sample':trace_P_arrival_sample, 'trace_S_onset':trace_S_onset,'trace_P_onset':trace_P_onset,\\\n",
    "            'trace_snr_db':trace_snr_db, 'trace_s_arrival':trace_s_arrival, 'trace_p_arrival':trace_p_arrival}\n",
    "\n",
    "    df = pd.DataFrame(dict)\n",
    "\n",
    "    # Make the specific day into a string:\n",
    "    tstring = t1.strftime('%Y%m%d')\n",
    "    # Build the full file name:\n",
    "    file_name = file_path+station+'_'+tstring+'.csv'\n",
    "    # Write to file using that name\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c157967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create your list of days to loop over!\n",
    "t1 = datetime(2012,10,1)\n",
    "t2 = datetime(2012,10,5)\n",
    "time_bins = pd.to_datetime(np.arange(t1,t2,pd.Timedelta(1,'days')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b18dc81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2012-10-01 00:00:00')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_bins[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e995b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine that list of days with the list of stations\n",
    "# We are essentially creating a list of the number of tasks we have to do with the information that is unique to each task; we will do them in parallel\n",
    "task_list = []\n",
    "for sta in station_list:\n",
    "\tfor t in time_bins:\n",
    "\t\ttask_list.append([sta,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0862d591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array(task_list)\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ec6c541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['J57A', Timestamp('2012-10-01 00:00:00')],\n",
       " ['J57A', Timestamp('2012-10-02 00:00:00')],\n",
       " ['J57A', Timestamp('2012-10-03 00:00:00')],\n",
       " ['J57A', Timestamp('2012-10-04 00:00:00')],\n",
       " ['J41A', Timestamp('2012-10-01 00:00:00')],\n",
       " ['J41A', Timestamp('2012-10-02 00:00:00')],\n",
       " ['J41A', Timestamp('2012-10-03 00:00:00')],\n",
       " ['J41A', Timestamp('2012-10-04 00:00:00')],\n",
       " ['M09B', Timestamp('2012-10-01 00:00:00')],\n",
       " ['M09B', Timestamp('2012-10-02 00:00:00')],\n",
       " ['M09B', Timestamp('2012-10-03 00:00:00')],\n",
       " ['M09B', Timestamp('2012-10-04 00:00:00')]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "560aff81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 202.12 ms\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140062026457856 and this is thread id 140049387599616.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# The below actually executes the parallel operation!\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# It's nice to do it with the ProgressBar so you can see how long things are taking.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Each operation should also write a file so that is another way to check on progress.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[0;32m---> 26\u001b[0m \t\u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlazy_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/dask/base.py:603\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    601\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 603\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_execute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mloop_tasks\u001b[0;34m(task, filepath, twin, step, l_blnd, r_blnd)\u001b[0m\n\u001b[1;32m     11\u001b[0m station \u001b[38;5;241m=\u001b[39m task[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Call to the function that will perform the operation and write the results to file\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrun_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtwin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43ml_blnd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr_blnd\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36mrun_detection\u001b[0;34m(station, t1, t2, filepath, twin, step, l_blnd, r_blnd)\u001b[0m\n\u001b[1;32m     16\u001b[0m inventory \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_stations(network\u001b[38;5;241m=\u001b[39mnetwork, station\u001b[38;5;241m=\u001b[39mstations)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Get waveforms and filter\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m sdata \u001b[38;5;241m=\u001b[39m \u001b[43mclient_waveform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_waveforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m7D\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBH?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarttime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m sdata\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbandpass\u001b[39m\u001b[38;5;124m'\u001b[39m,freqmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,freqmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get the necassary information about the station\u001b[39;00m\n",
      "File \u001b[0;32m/data/wsd01/pnwstore/pnwstore/mseed.py:127\u001b[0m, in \u001b[0;36mWaveformClient.get_waveforms\u001b[0;34m(self, headeronly, starttime, endtime, filename, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMulti-day streaming not implemented.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStart/end time should be in the same day.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[0;32m--> 127\u001b[0m rst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbyteoffset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m s \u001b[38;5;241m=\u001b[39m obspy\u001b[38;5;241m.\u001b[39mStream()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m rst:\n",
      "File \u001b[0;32m/data/wsd01/pnwstore/pnwstore/mseed.py:106\u001b[0m, in \u001b[0;36mWaveformClient._query\u001b[0;34m(self, keys, showquery, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(query_str)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor\u001b[38;5;241m.\u001b[39mexecute(query_str)\n",
      "\u001b[0;31mProgrammingError\u001b[0m: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140062026457856 and this is thread id 140049387599616."
     ]
    }
   ],
   "source": [
    "# Now we start setting up a parallel operation using a package called Dask.\n",
    "\n",
    "# Start by writing a new a function that is specifically designed to be run in parallel through dask. All it essentially does is define the inputs to the larger run_detection function and then runs the function itself, but because we \"decorate\" it with @dask.delayed to start, the code will recognize that it should be run in parallel.\n",
    "\n",
    "@dask.delayed\n",
    "def loop_tasks(task,filepath,twin,step,l_blnd,r_blnd):\n",
    "\n",
    "\t# Define the parameters that are specific to each task\n",
    "\tt1 = obspy.UTCDateTime(task[1])\n",
    "\tt2 = obspy.UTCDateTime(t1 + pd.Timedelta(1,'days'))\n",
    "\tstation = task[0]\n",
    "\n",
    "\t# Call to the function that will perform the operation and write the results to file\n",
    "\trun_detection(station,t1,t2,filepath,twin,step,l_blnd,r_blnd)\n",
    "\t\n",
    "\n",
    "# Now we set up the parallel operation\n",
    "# The below builds a framework for the computer to run in parallel. This doesn't actually execute anything.\n",
    "lazy_results = [loop_tasks(task,filepath,twin,step,l_blnd,r_blnd) for task in task_list]\n",
    "    \n",
    "\n",
    "# The below actually executes the parallel operation!\n",
    "# It's nice to do it with the ProgressBar so you can see how long things are taking.\n",
    "# Each operation should also write a file so that is another way to check on progress.\n",
    "with ProgressBar():\n",
    "\tdask.compute(lazy_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf2586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f44b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
