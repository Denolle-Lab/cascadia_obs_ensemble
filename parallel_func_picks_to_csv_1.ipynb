{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04946867",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alembic==1.7.7\r\n",
      "argon2-cffi==21.3.0\r\n",
      "argon2-cffi-bindings==21.2.0\r\n",
      "async-generator==1.10\r\n",
      "attrs==22.2.0\r\n",
      "Babel==2.5.1\r\n",
      "backcall==0.2.0\r\n",
      "bleach==4.1.0\r\n",
      "certipy==0.1.3\r\n",
      "cffi==1.15.1\r\n",
      "chardet==3.0.4\r\n",
      "cryptography==40.0.2\r\n",
      "dataclasses==0.8\r\n",
      "dbus-python==1.2.4\r\n",
      "decorator==4.2.1\r\n",
      "defusedxml==0.7.1\r\n",
      "dnspython==1.15.0\r\n",
      "entrypoints==0.4\r\n",
      "file-magic==0.3.0\r\n",
      "gpg==1.13.1\r\n",
      "greenlet==2.0.2\r\n",
      "gssapi==1.5.1\r\n",
      "html5lib==0.999999999\r\n",
      "idna==2.5\r\n",
      "importlib-metadata==4.8.3\r\n",
      "importlib-resources==5.4.0\r\n",
      "iotop==0.6\r\n",
      "ipaclient==4.9.11\r\n",
      "ipalib==4.9.11\r\n",
      "ipaplatform==4.9.11\r\n",
      "ipapython==4.9.11\r\n",
      "ipykernel==5.5.6\r\n",
      "ipython==7.16.3\r\n",
      "ipython-genutils==0.2.0\r\n",
      "isc==2.0\r\n",
      "jedi==0.17.2\r\n",
      "jhub-remote-user-authenticator==0.1.0\r\n",
      "Jinja2==3.0.3\r\n",
      "jsonschema==3.2.0\r\n",
      "jupyter-client==7.1.2\r\n",
      "jupyter-core==4.9.2\r\n",
      "jupyter-telemetry==0.1.0\r\n",
      "jupyterhub==2.3.1\r\n",
      "jupyterlab-pygments==0.1.2\r\n",
      "jwcrypto==0.5.0\r\n",
      "libcomps==0.1.18\r\n",
      "lxml==4.2.3\r\n",
      "Mako==1.1.6\r\n",
      "MarkupSafe==2.0.1\r\n",
      "mistune==0.8.4\r\n",
      "nbclient==0.5.9\r\n",
      "nbconvert==6.0.7\r\n",
      "nbformat==5.1.3\r\n",
      "nest-asyncio==1.5.8\r\n",
      "netaddr==0.7.19\r\n",
      "netifaces==0.10.6\r\n",
      "nftables==0.1\r\n",
      "notebook==6.4.10\r\n",
      "oauthlib==3.2.2\r\n",
      "packaging==21.3\r\n",
      "pamela==1.1.0\r\n",
      "pandocfilters==1.5.0\r\n",
      "parso==0.7.1\r\n",
      "perf==0.1\r\n",
      "pexpect==4.3.1\r\n",
      "pickleshare==0.7.5\r\n",
      "ply==3.9\r\n",
      "prometheus-client==0.17.1\r\n",
      "prompt-toolkit==3.0.36\r\n",
      "psutil==5.4.3\r\n",
      "ptyprocess==0.5.2\r\n",
      "pyasn1==0.3.7\r\n",
      "pyasn1-modules==0.1.5\r\n",
      "pycairo==1.16.3\r\n",
      "pycparser==2.14\r\n",
      "pydbus==0.6.0\r\n",
      "Pygments==2.14.0\r\n",
      "pygobject==3.28.3\r\n",
      "pyOpenSSL==23.2.0\r\n",
      "pyparsing==3.1.1\r\n",
      "pyrsistent==0.18.0\r\n",
      "PySocks==1.6.8\r\n",
      "python-augeas==0.5.0\r\n",
      "python-dateutil==2.6.1\r\n",
      "python-json-logger==2.0.7\r\n",
      "python-ldap==3.3.1\r\n",
      "python-linux-procfs==0.7.0\r\n",
      "python-yubico==1.3.2\r\n",
      "pytz==2017.2\r\n",
      "pyudev==0.21.0\r\n",
      "pyusb==1.0.0\r\n",
      "PyYAML==3.12\r\n",
      "pyzmq==25.1.2\r\n",
      "qrcode==5.1\r\n",
      "requests==2.20.0\r\n",
      "rpm==4.14.3\r\n",
      "ruamel.yaml==0.18.3\r\n",
      "ruamel.yaml.clib==0.2.8\r\n",
      "selinux==2.9\r\n",
      "semantic-version==2.10.0\r\n",
      "Send2Trash==1.8.2\r\n",
      "sepolicy==1.1\r\n",
      "setools==4.3.0\r\n",
      "setroubleshoot==1.1\r\n",
      "setuptools-rust==1.1.2\r\n",
      "six==1.11.0\r\n",
      "slip==0.6.4\r\n",
      "slip.dbus==0.6.4\r\n",
      "sos==4.5.5\r\n",
      "SQLAlchemy==1.4.50\r\n",
      "SSSDConfig==2.8.2\r\n",
      "sudospawner==0.5.2\r\n",
      "syspurpose==1.28.36\r\n",
      "systemd-python==234\r\n",
      "terminado==0.12.1\r\n",
      "testpath==0.6.0\r\n",
      "tornado==6.1\r\n",
      "traitlets==4.3.3\r\n",
      "typing-extensions==4.1.1\r\n",
      "urllib3==1.24.2\r\n",
      "wcwidth==0.2.12\r\n",
      "webencodings==0.5.1\r\n",
      "zipp==3.6.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d4a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/commands/install.py\", line 365, in run\n",
      "    strip_file_prefix=options.strip_file_prefix,\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/req/req_set.py\", line 789, in install\n",
      "    **kwargs\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/req/req_install.py\", line 854, in install\n",
      "    strip_file_prefix=strip_file_prefix\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/req/req_install.py\", line 1069, in move_wheel_files\n",
      "    strip_file_prefix=strip_file_prefix,\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/wheel.py\", line 316, in clobber\n",
      "    ensure_dir(destdir)\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/utils/__init__.py\", line 83, in ensure_dir\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib64/python3.6/os.py\", line 220, in makedirs\n",
      "    mkdir(name, mode)\n",
      "PermissionError: [Errno 13] Permission denied: '/usr/local/lib64/python3.6/site-packages/numpy'\u001b[0m\n",
      "Collecting obspy\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/8c/eef47074a1884c73bc4f2ba7b2961a79fc54952edadeff4b998de86dcb20/obspy-1.2.2.zip\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"/tmp/pip-build-06uwp55e/obspy/setup.py\", line 33, in <module>\n",
      "        import numpy  # @UnusedImport # NOQA\n",
      "    ModuleNotFoundError: No module named 'numpy'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-build-06uwp55e/obspy/setup.py\", line 37, in <module>\n",
      "        raise ImportError(msg)\n",
      "    ImportError: No module named numpy. Please install numpy first, it is needed before installing ObsPy.\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-06uwp55e/obspy/\u001b[0m\n",
      "Collecting seisbench\n",
      "  Using cached https://files.pythonhosted.org/packages/a5/79/7b8f4866b8b548d8350ef2fc37363f7ffe71e9e383fbf5c74ac9caa10445/seisbench-0.1.7-py3-none-any.whl\n",
      "Collecting obspy>=1.2 (from seisbench)\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/8c/eef47074a1884c73bc4f2ba7b2961a79fc54952edadeff4b998de86dcb20/obspy-1.2.2.zip\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"/tmp/pip-build-sfmflwqf/obspy/setup.py\", line 33, in <module>\n",
      "        import numpy  # @UnusedImport # NOQA\n",
      "    ModuleNotFoundError: No module named 'numpy'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-build-sfmflwqf/obspy/setup.py\", line 37, in <module>\n",
      "        raise ImportError(msg)\n",
      "    ImportError: No module named numpy. Please install numpy first, it is needed before installing ObsPy.\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-sfmflwqf/obspy/\u001b[0m\n",
      "Collecting elep==0.0.2\n",
      "\u001b[31m  Could not find a version that satisfies the requirement elep==0.0.2 (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for elep==0.0.2\u001b[0m\n",
      "Collecting tqdm\n",
      "  Using cached https://files.pythonhosted.org/packages/47/bb/849011636c4da2e44f1253cd927cfb20ada4374d8b3a4e425416e84900cc/tqdm-4.64.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.7\" in /usr/local/lib/python3.6/site-packages (from tqdm)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/site-packages (from importlib-resources; python_version < \"3.7\"->tqdm)\n",
      "Installing collected packages: tqdm\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/commands/install.py\", line 365, in run\n",
      "    strip_file_prefix=options.strip_file_prefix,\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/req/req_set.py\", line 789, in install\n",
      "    **kwargs\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/req/req_install.py\", line 854, in install\n",
      "    strip_file_prefix=strip_file_prefix\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/req/req_install.py\", line 1069, in move_wheel_files\n",
      "    strip_file_prefix=strip_file_prefix,\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/wheel.py\", line 316, in clobber\n",
      "    ensure_dir(destdir)\n",
      "  File \"/usr/lib/python3.6/site-packages/pip/utils/__init__.py\", line 83, in ensure_dir\n",
      "    os.makedirs(path)\n",
      "  File \"/usr/lib64/python3.6/os.py\", line 220, in makedirs\n",
      "    mkdir(name, mode)\n",
      "PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.6/site-packages/tqdm'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install obspy\n",
    "!pip3 install seisbench\n",
    "!pip3 install elep==0.0.2\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ec9cbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'obspy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-29d9e85ed6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#####################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mobspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfdsn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobspy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'obspy'"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "import logging\n",
    "#####################################\n",
    "from obspy.clients.fdsn import Client\n",
    "import numpy as np\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy.clients.fdsn import Client\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from pnwstore.mseed import WaveformClient\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import pandas as pd\n",
    "import gc\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "from ELEP.elep.trigger_func import picks_summary_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e7e246",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-33f0d36aaf1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec5a0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clients\n",
    "client_inventory = Client('IRIS')\n",
    "client_waveform = WaveformClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b976476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in whatever you need to start - likely a list of station codes\n",
    "station_list = ['J57A', 'J41A', 'M09B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff0cf559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hbito/elep-test/surface_events/src/catalogs_elep/7D_20121001.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify some parameters - you can change what you specify here vs. within the large function, this is just an example.\n",
    "# Depending on whether the pertained models take a long time to load every time, you may want to load those outside the function and just feed them to the function rather than loading them every time in parallel.\n",
    "twin = 6000     # length of time window\n",
    "step = 3000     # step length\n",
    "l_blnd, r_blnd = 500, 500\n",
    "\n",
    "###########################\n",
    "# This is not a valid file path to save files. \n",
    "# filepath = 'https://cascadia.ess.washington.edu/jhub/user/hbito/notebooks/elep-test/surface_events/src'\n",
    "filepath = \"/home/hbito/elep-test/surface_events/src/catalogs_elep/\"\n",
    "\n",
    "# If we define:\n",
    "station = \"7D\"\n",
    "t1 = datetime(2012,10,1)\n",
    "tstring = t1.strftime('%Y%m%d')\n",
    "\n",
    "# You use this arguments below as (it should be filepath, not file_path)\n",
    "file_name = filepath+station+'_'+tstring+'.csv'\n",
    "\n",
    "# Then your file name will be:\n",
    "print(file_name)\n",
    "#https://cascadia.ess.washington.edu/jhub/user/hbito/notebooks/elep-test/surface_events/src7D_20121001.csv\n",
    "\n",
    "# 1. There is a missing \"/\" after src in filepath.\n",
    "# 2. This string beginning with https:// is a web link. You can replace filepath as:\n",
    "filepath = \"/home/hbito/elep-test/surface_events/src/\"\n",
    "# 3. it may not be recommended to save data product to a src (source) folder. \n",
    "#    People usually make other directories to save those final files. \n",
    "#    Just a notice: not our top prioirty though.\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5db51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download models\n",
    "pretrain_list = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd7d1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for stacking the segmented time windows after prediction\n",
    "##################################### add a nseg argument here\n",
    "# def stacking(data, npts, l_blnd, r_blnd):\n",
    "def stacking(data, npts, l_blnd, r_blnd, nseg):\n",
    "#####################################\n",
    "    _data = data.copy()\n",
    "    stack = np.full(npts, np.nan, dtype = np.float32)\n",
    "    _data[:, :l_blnd] = np.nan; _data[:, -r_blnd:] = np.nan\n",
    "    stack[:twin] = _data[0, :]\n",
    "    for iseg in range(nseg-1):\n",
    "        idx = step*(iseg+1)\n",
    "        stack[idx:idx + twin] = \\\n",
    "                np.nanmax([stack[idx:idx + twin], _data[iseg+1, :]], axis = 0)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cdf915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your function that you want to run in parallel: I recommend you design this to essentially perform your entire workflow on one station for one day, and write a csv file for that station, much the way you already have it.\n",
    "# This is what will run in parallel!\n",
    "# So, the only inputs are the station name, the start and end times you want to detect for, the path of the folder you want to write the results to, and the parameters you already specified. Here is where you could also feed in the preloaded models if that becomes important.\n",
    "def run_detection(station,t1,t2,filepath,twin,step,l_blnd,r_blnd):\n",
    "\t# Load data\n",
    "\t# Reshape data\n",
    "\t# Predict on base models\n",
    "\t# Stack\n",
    "\t# Create and write csv file. Define file name using the station code and the input filepath\n",
    "    \n",
    "    # Get the inventory for the stations\n",
    "    ###############################\n",
    "    #stations = station           # Seems you never use this \"stations\" again\n",
    "                                  # so you can just use \"station\"\n",
    "    ###############################\n",
    "    network = '7D'\n",
    "    channels = '?H?'\n",
    "    client = client_inventory\n",
    "    inventory = client.get_stations(network=network, station=station)\n",
    "    \n",
    "    # Get waveforms and filter\n",
    "    ###############################\n",
    "   #sdata = client_waveform.get_waveforms(network=\"7D\", station=station, channel=\"BH?\", starttime=t1, \n",
    "   #                                      year=t1.strftime('%Y'), month=t1.strftime('%m'), \n",
    "   #                                      day=t1.strftime('%d'))\n",
    "   # You've already defined stations, network, channels above, so you can use them here.\n",
    "   # You don't need \"starttime = t1\" argument.\n",
    "    sdata = client_waveform.get_waveforms(network=network, station=station, channel=channels, \n",
    "                                          year=t1.strftime('%Y'), month=t1.strftime('%m'), \n",
    "                                          day=t1.strftime('%d'))\n",
    "    ###############################\n",
    "    # If no data returned, skipping\n",
    "    if len(sdata) == 0:\n",
    "        logging.warning(\"No stream returned. Skipping.\")\n",
    "        return\n",
    "    ###############################\n",
    "    \n",
    "    sdata.filter(type='bandpass',freqmin=4,freqmax=15)\n",
    "    \n",
    "    ###############################\n",
    "    sdata.merge(fill_value='interpolate') # fill gaps if there are any.\n",
    "    ###############################\n",
    "\n",
    "    # Get the necassary information about the station\n",
    "    delta = sdata[0].stats.delta\n",
    "    starttime = sdata[0].stats.starttime\n",
    "    fs = sdata[0].stats.sampling_rate\n",
    "    dt = 1/fs\n",
    "    \n",
    "    # Reshaping data\n",
    "    sdata = np.array(sdata)\n",
    "    npts = sdata.shape[1]\n",
    "    ############################### avoiding errors at the end of a stream\n",
    "   #nseg = int(np.ceil((npts - twin) / step)) + 1\n",
    "    nseg = int(np.floor((npts - twin) / step)) + 1\n",
    "    ###############################\n",
    "    windows = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    tap = 0.5 * (1 + np.cos(np.linspace(np.pi, 2 * np.pi, 6)))\n",
    "    \n",
    "    # Define the parameters for semblance\n",
    "    paras_semblance = {'dt':dt, 'semblance_order':2, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "    p_thrd, s_thrd = 0.05, 0.05\n",
    "\n",
    "    windows_std = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    windows_max = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    windows = np.zeros(shape=(nseg, 3, twin), dtype= np.float32)\n",
    "    windows_idx = np.zeros(nseg, dtype=np.int32)\n",
    "\n",
    "    for iseg in range(nseg):\n",
    "        idx = iseg * step\n",
    "        windows[iseg, :] = sdata[:, idx:idx + twin]\n",
    "        windows[iseg, :] -= np.mean(windows[iseg, :], axis=-1, keepdims=True)\n",
    "        # original use std norm\n",
    "        windows_std[iseg, :] = windows[iseg, :] / np.std(windows[iseg, :]) + 1e-10\n",
    "        # others use max norm\n",
    "        windows_max[iseg, :] = windows[iseg, :] / (np.max(np.abs(windows[iseg, :]), axis=-1, keepdims=True))\n",
    "        windows_idx[iseg] = idx\n",
    "\n",
    "    # taper\n",
    "    windows_std[:, :, :6] *= tap; windows_std[:, :, -6:] *= tap[::-1]; \n",
    "    windows_max[:, :, :6] *= tap; windows_max[:, :, -6:] *= tap[::-1];\n",
    "    del windows\n",
    "\n",
    "#     print(f\"Window data shape: {windows_std.shape}\")\n",
    "    \n",
    "    # Predict on base models\n",
    "    \n",
    "    pretrain_list = ['original', 'ethz', 'instance', 'scedc', 'stead']\n",
    "\n",
    "    # dim 0: 0 = P, 1 = S\n",
    "    batch_pred = np.zeros([2, len(pretrain_list), nseg, twin], dtype = np.float32) \n",
    "    for ipre, pretrain in enumerate(pretrain_list):\n",
    "        t0 = time.time()\n",
    "        eqt = sbm.EQTransformer.from_pretrained(pretrain)\n",
    "        eqt.to(device);\n",
    "        eqt._annotate_args['overlap'] = ('Overlap between prediction windows in samples \\\n",
    "                                        (only for window prediction models)', step)\n",
    "        eqt._annotate_args['blinding'] = ('Number of prediction samples to discard on \\\n",
    "                                         each side of each window prediction', (l_blnd, r_blnd))\n",
    "        eqt.eval();\n",
    "        if pretrain == 'original':\n",
    "            # batch prediction through torch model\n",
    "            windows_std_tt = torch.Tensor(windows_std)\n",
    "            _torch_pred = eqt(windows_std_tt.to(device))\n",
    "        else:\n",
    "            windows_max_tt = torch.Tensor(windows_max)\n",
    "            _torch_pred = eqt(windows_max_tt.to(device))\n",
    "        batch_pred[0, ipre, :] = _torch_pred[1].detach().cpu().numpy()\n",
    "        batch_pred[1, ipre, :] = _torch_pred[2].detach().cpu().numpy()\n",
    "\n",
    "    # clean up memory\n",
    "    del _torch_pred, windows_max_tt, windows_std_tt\n",
    "    del windows_std, windows_max\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"All prediction shape: {batch_pred.shape}\")\n",
    "    \n",
    "    ####################### You don't need this\n",
    "#     pretrain_pred = np.zeros([2, len(pretrain_list), npts], dtype = np.float32)\n",
    "#     for ipre, pretrain in enumerate(pretrain_list):\n",
    "#        # 0 for P-wave\n",
    "#         pretrain_pred[0, ipre, :] = stacking(batch_pred[0, ipre, :], npts, l_blnd, r_blnd)\n",
    "# \n",
    "#        # 1 for S-wave\n",
    "#        pretrain_pred[1, ipre, :] = stacking(batch_pred[1, ipre, :], npts, l_blnd, r_blnd)\n",
    "    ####################### You don't need this\n",
    "    \n",
    "    smb_pred = np.zeros([2, nseg, twin], dtype = np.float32)\n",
    "    # calculate the semblance\n",
    "    ## the semblance may takes a while bit to calculate\n",
    "    \n",
    "    ############################# remove tqdm (extra progress bar)\n",
    "#     for iseg in tqdm(range(nseg)):\n",
    "    for iseg in range(nseg):\n",
    "    #############################\n",
    "        # 0 for P-wave\n",
    "        smb_pred[0, iseg, :] = ensemble_semblance(batch_pred[0, :, iseg, :], paras_semblance)\n",
    "\n",
    "        # 1 for P-wave\n",
    "        smb_pred[1, iseg, :] = ensemble_semblance(batch_pred[1, :, iseg, :], paras_semblance)\n",
    "\n",
    "    ## ... and stack\n",
    "    # 0 for P-wave\n",
    "    ####################### add a nseg argument here\n",
    "    #smb_p = stacking(smb_pred[0, :], npts, l_blnd, r_blnd)\n",
    "    smb_p = stacking(smb_pred[0, :], npts, l_blnd, r_blnd, nseg)\n",
    "\n",
    "    # 1 for P-wave\n",
    "    #smb_s = stacking(smb_pred[1, :], npts, l_blnd, r_blnd)\n",
    "    smb_s = stacking(smb_pred[1, :], npts, l_blnd, r_blnd, nseg)\n",
    "    #######################\n",
    "    # clean-up RAM\n",
    "    del smb_pred, batch_pred\n",
    "\n",
    "    p_index = picks_summary_simple(smb_p, p_thrd)\n",
    "    s_index = picks_summary_simple(smb_s, s_thrd)\n",
    "    print(f\"{len(p_index)} P picks\\n{len(s_index)} S picks\")\n",
    "    \n",
    "    # Create lists and a data frame\n",
    "    event_id = []\n",
    "    source_type = []\n",
    "    station_network_code = []\n",
    "    station_channel_code = []\n",
    "    station_code = []\n",
    "    station_location_code = []\n",
    "    station_latitude_deg= []\n",
    "    station_longitude_deg = []\n",
    "    station_elevation_m = []\n",
    "    trace_name = []\n",
    "    trace_sampling_rate_hz = []\n",
    "    trace_start_time = []\n",
    "    trace_S_arrival_sample = []\n",
    "    trace_P_arrival_sample = []\n",
    "    trace_S_onset = []\n",
    "    trace_P_onset = []\n",
    "    trace_snr_db = []\n",
    "    trace_p_arrival = []\n",
    "    trace_s_arrival = []\n",
    "\n",
    "    for i, idx in enumerate(p_index):\n",
    "        event_id.append(' ')\n",
    "        source_type.append(' ')\n",
    "        station_network_code.append('7D')\n",
    "        station_channel_code.append(' ')\n",
    "        station_code.append(station)\n",
    "        station_location_code.append(station.stats.location)   \n",
    "        station_latitude_deg.append(inventory[0][0].latitude)\n",
    "        station_longitude_deg.append(inventory[0][0].longitude)   \n",
    "        station_elevation_m.append(inventory[0][0].elevation)\n",
    "        trace_name.append(' ')\n",
    "        trace_sampling_rate_hz.append(station.stats.sampling_rate)\n",
    "        trace_start_time.append(station.stats.starttime)\n",
    "        trace_S_arrival_sample.append(' ')\n",
    "        trace_P_arrival_sample.append(' ')\n",
    "        trace_S_onset.append(' ')\n",
    "        trace_P_onset.append(' ')\n",
    "        trace_snr_db.append(' ')\n",
    "        trace_s_arrival.append(np.nan)\n",
    "        trace_p_arrival.append(str(starttime  + idx * delta))\n",
    "\n",
    "    for i, idx in enumerate(s_index):\n",
    "        event_id.append(' ')\n",
    "        source_type.append(' ')\n",
    "        station_network_code.append('7D')\n",
    "        station_channel_code.append(' ')\n",
    "        station_code.append(station)\n",
    "        station_location_code.append(station.stats.location)   \n",
    "        station_latitude_deg.append(inventory[0][0].latitude)\n",
    "        station_longitude_deg.append(inventory[0][0].longitude)   \n",
    "        station_elevation_m.append(inventory[0][0].elevation)\n",
    "        trace_name.append(' ')\n",
    "        trace_sampling_rate_hz.append(station.stats.sampling_rate)\n",
    "        trace_start_time.append(station.stats.starttime)\n",
    "        trace_S_arrival_sample.append(' ')\n",
    "        trace_P_arrival_sample.append(' ')\n",
    "        trace_S_onset.append(' ')\n",
    "        trace_P_onset.append(' ')\n",
    "        trace_snr_db.append(' ')\n",
    "        trace_s_arrival.append(str(starttime  + idx * delta))\n",
    "        trace_p_arrival.append(np.nan)\n",
    "\n",
    "    # dictionary of lists\n",
    "    dict = {'event_id':event_id,'source_type':source_type,'station_network_code':station_network_code,\\\n",
    "            'station_channel_code':station_channel_code,'station_code':station_code,'station_location_code':station_location_code,\\\n",
    "            'station_latitude_deg':station_latitude_deg,'station_longitude_deg':station_longitude_deg, \\\n",
    "            'station_elevation_m':station_elevation_m,'trace_name':trace_name,'trace_sampling_rate_hz':trace_sampling_rate_hz,\\\n",
    "            'trace_start_time':trace_start_time,'trace_S_arrival_sample':trace_S_arrival_sample,\\\n",
    "            'trace_P_arrival_sample':trace_P_arrival_sample, 'trace_S_onset':trace_S_onset,'trace_P_onset':trace_P_onset,\\\n",
    "            'trace_snr_db':trace_snr_db, 'trace_s_arrival':trace_s_arrival, 'trace_p_arrival':trace_p_arrival}\n",
    "\n",
    "    df = pd.DataFrame(dict)\n",
    "\n",
    "    # Make the specific day into a string:\n",
    "    tstring = t1.strftime('%Y%m%d')\n",
    "    # Build the full file name:\n",
    "    ##################################################\n",
    "#     file_name = file_path+station+'_'+tstring+'.csv'\n",
    "    file_name = filepath+station+'_'+tstring+'.csv'\n",
    "    ##################################################\n",
    "    # Write to file using that name\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c157967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create your list of days to loop over!\n",
    "t1 = datetime(2012,10,1)\n",
    "t2 = datetime(2012,10,5)\n",
    "time_bins = pd.to_datetime(np.arange(t1,t2,pd.Timedelta(1,'days')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e995b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine that list of days with the list of stations\n",
    "# We are essentially creating a list of the number of tasks we have to do with the information that is unique to each task; we will do them in parallel\n",
    "task_list = []\n",
    "for sta in station_list:\n",
    "\tfor t in time_bins:\n",
    "\t\ttask_list.append([sta,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec6c541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['J57A', Timestamp('2012-10-01 00:00:00')],\n",
       " ['J57A', Timestamp('2012-10-02 00:00:00')],\n",
       " ['J57A', Timestamp('2012-10-03 00:00:00')],\n",
       " ['J57A', Timestamp('2012-10-04 00:00:00')],\n",
       " ['J41A', Timestamp('2012-10-01 00:00:00')],\n",
       " ['J41A', Timestamp('2012-10-02 00:00:00')],\n",
       " ['J41A', Timestamp('2012-10-03 00:00:00')],\n",
       " ['J41A', Timestamp('2012-10-04 00:00:00')],\n",
       " ['M09B', Timestamp('2012-10-01 00:00:00')],\n",
       " ['M09B', Timestamp('2012-10-02 00:00:00')],\n",
       " ['M09B', Timestamp('2012-10-03 00:00:00')],\n",
       " ['M09B', Timestamp('2012-10-04 00:00:00')]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "560aff81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 12m 10sssAll prediction shape: (2, 5, 3599, 6000)\n",
      "[                                        ] | 0% Completed | 20m 40s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1225200/2319557713.py:13: RuntimeWarning: All-NaN axis encountered\n",
      "  np.nanmax([stack[idx:idx + twin], _data[iseg+1, :]], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 20m 40s425 P picks\n",
      "488 S picks\n",
      "[###                                     ] | 8% Completed | 20m 41s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No stream returned. Skipping.\n",
      "WARNING:root:No stream returned. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##########                              ] | 25% Completed | 20m 41s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No stream returned. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#############                           ] | 33% Completed | 30m 12sAll prediction shape: (2, 5, 3599, 6000)\n",
      "[#############                           ] | 33% Completed | 38m 55s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1225200/2319557713.py:13: RuntimeWarning: All-NaN axis encountered\n",
      "  np.nanmax([stack[idx:idx + twin], _data[iseg+1, :]], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#############                           ] | 33% Completed | 38m 55s404 P picks\n",
      "486 S picks\n",
      "[################                        ] | 41% Completed | 46m 46sAll prediction shape: (2, 5, 3599, 6000)\n",
      "[################                        ] | 41% Completed | 53m 35s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1225200/2319557713.py:13: RuntimeWarning: All-NaN axis encountered\n",
      "  np.nanmax([stack[idx:idx + twin], _data[iseg+1, :]], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[################                        ] | 41% Completed | 53m 36s460 P picks\n",
      "526 S picks\n",
      "[####################                    ] | 50% Completed | 53m 36s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No stream returned. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#######################                 ] | 58% Completed | 53m 36s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No stream returned. Skipping.\n",
      "WARNING:root:No stream returned. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##############################          ] | 75% Completed | 56m 19sAll prediction shape: (2, 5, 3599, 6000)\n",
      "[##############################          ] | 75% Completed | 62m 37s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1225200/2319557713.py:13: RuntimeWarning: All-NaN axis encountered\n",
      "  np.nanmax([stack[idx:idx + twin], _data[iseg+1, :]], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##############################          ] | 75% Completed | 62m 37s424 P picks\n",
      "493 S picks\n",
      "[#################################       ] | 83% Completed | 62m 37s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No stream returned. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################################    ] | 91% Completed | 62m 37s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No stream returned. Skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 62m 38s\n"
     ]
    }
   ],
   "source": [
    "# Now we start setting up a parallel operation using a package called Dask.\n",
    "\n",
    "# Start by writing a new a function that is specifically designed to be run in parallel through dask. All it essentially does is define the inputs to the larger run_detection function and then runs the function itself, but because we \"decorate\" it with @dask.delayed to start, the code will recognize that it should be run in parallel.\n",
    "\n",
    "@dask.delayed\n",
    "def loop_tasks(task,filepath,twin,step,l_blnd,r_blnd):\n",
    "\n",
    "\t# Define the parameters that are specific to each task\n",
    "\tt1 = obspy.UTCDateTime(task[1])\n",
    "\tt2 = obspy.UTCDateTime(t1 + pd.Timedelta(1,'days'))\n",
    "\tstation = task[0]\n",
    "\n",
    "\t# Call to the function that will perform the operation and write the results to file\n",
    "\trun_detection(station,t1,t2,filepath,twin,step,l_blnd,r_blnd)\n",
    "\t\n",
    "\n",
    "# Now we set up the parallel operation\n",
    "# The below builds a framework for the computer to run in parallel. This doesn't actually execute anything.\n",
    "lazy_results = [loop_tasks(task,filepath,twin,step,l_blnd,r_blnd) for task in task_list]\n",
    "    \n",
    "\n",
    "# The below actually executes the parallel operation!\n",
    "# It's nice to do it with the ProgressBar so you can see how long things are taking.\n",
    "# Each operation should also write a file so that is another way to check on progress.\n",
    "with ProgressBar():\n",
    "    #################################\n",
    "    # Add scheduler = 'single-threaded'\n",
    "\tdask.compute(lazy_results, scheduler='single-threaded') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd716b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a55cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
