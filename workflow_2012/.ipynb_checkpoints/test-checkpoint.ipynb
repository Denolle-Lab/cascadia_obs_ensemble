{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a68c7128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbito/cascadia_obs_ensemble/workflow_2012/picking_utils.py:160: RuntimeWarning: invalid value encountered in divide\n",
      "  windows_max[iseg, :] = windows[iseg, :] / (np.max(np.abs(windows[iseg, :]), axis=-1, keepdims=True))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m network \u001b[38;5;241m=\u001b[39m task_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     86\u001b[0m station \u001b[38;5;241m=\u001b[39m task_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 87\u001b[0m \u001b[43mrun_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtwin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43ml_blnd\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr_blnd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# @dask.delayed\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# def loop_days(task,filepath,twin,step,l_blnd,r_blnd):\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# #     # Add scheduler = 'single-threaded'\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# # \tdask.compute(lazy_results, scheduler='single-threaded') \u001b[39;00m\n",
      "File \u001b[0;32m~/cascadia_obs_ensemble/workflow_2012/picking_utils.py:179\u001b[0m, in \u001b[0;36mrun_detection\u001b[0;34m(network, station, t1, t2, filepath, twin, step, l_blnd, r_blnd)\u001b[0m\n\u001b[1;32m    177\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    178\u001b[0m eqt \u001b[38;5;241m=\u001b[39m sbm\u001b[38;5;241m.\u001b[39mEQTransformer\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrain)\n\u001b[0;32m--> 179\u001b[0m eqt\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m);\n\u001b[1;32m    180\u001b[0m eqt\u001b[38;5;241m.\u001b[39m_annotate_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverlap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverlap between prediction windows in samples \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124m                                (only for window prediction models)\u001b[39m\u001b[38;5;124m'\u001b[39m, step)\n\u001b[1;32m    182\u001b[0m eqt\u001b[38;5;241m.\u001b[39m_annotate_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblinding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of prediction samples to discard on \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124m                                 each side of each window prediction\u001b[39m\u001b[38;5;124m'\u001b[39m, (l_blnd, r_blnd))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from obspy.clients.fdsn import Client\n",
    "import numpy as np\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "from obspy import Stream\n",
    "\n",
    "from pnwstore.mseed import WaveformClient\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import pandas as pd\n",
    "import gc\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "from picking_utils import run_detection, stacking\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define clients\n",
    "client_inventory = Client('IRIS')\n",
    "client_waveform = WaveformClient()\n",
    "client_ncedc = Client('NCEDC')\n",
    "\n",
    "# Parameters\n",
    "year1 = 2012\n",
    "filepath = \"../data/catalog_picks/\"\n",
    "os.makedirs(filepath,exist_ok=True)\n",
    "\n",
    "twin = 6000     # length of time window\n",
    "step = 3000     # step length\n",
    "l_blnd, r_blnd = 500, 500\n",
    "\n",
    "# Now create your list of days to loop over!\n",
    "t1 = datetime.datetime(year=year1,month=1,day=1)\n",
    "t2 = datetime.datetime(year=year1+1,month=1,day=1)\n",
    "time_bins = pd.to_datetime(np.arange(t1,t2,pd.Timedelta(1,'days')))\n",
    "\n",
    "inventory = client_inventory.get_stations(network=\"C8,7D,7A,CN,NV,UW,UO,NC,BK,TA,OO,PB,X6,Z5,X9\", station=\"*\", minlatitude=40,minlongitude=-127,maxlatitude=50,maxlongitude=-123, starttime=t1.strftime('%Y%m%d'),endtime=t2.strftime('%Y%m%d'))\n",
    "\n",
    "\n",
    "\n",
    "# Make a list of networks and stations\n",
    "networks_stas = []\n",
    "for i in range(len(inventory)):\n",
    "    network = inventory[i].code\n",
    "    \n",
    "    for j in range(len(inventory[i])):\n",
    "        networks_stas.append([network,inventory[i].stations[j].code])\n",
    "\n",
    "networks_stas =np.array(networks_stas)\n",
    "    \n",
    "# download models\n",
    "pretrain_list = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "\n",
    "# Combine that list of days with the list of stations and networks\n",
    "# We are essentially creating a list of the number of tasks we have to do with the information that is unique to each task; we will do them in parallel\n",
    "task_list = []\n",
    "for i in range(len(networks_stas)):\n",
    "\tfor t in time_bins:\n",
    "\t\ttask_list.append([networks_stas[i][0], networks_stas[i][1],t])\n",
    "        \n",
    "# Now we start setting up a parallel operation using a package called Dask.\n",
    "t1 = obspy.UTCDateTime(task_list[0][2])\n",
    "t2 = obspy.UTCDateTime(t1 + pd.Timedelta(1,'days'))\n",
    "network = task_list[0][0]\n",
    "station = task_list[0][1]\n",
    "run_detection(network,station,t1,t2,filepath,twin,step,l_blnd,r_blnd)\n",
    "# @dask.delayed\n",
    "# def loop_days(task,filepath,twin,step,l_blnd,r_blnd):\n",
    "\n",
    "#     # Define the parameters that are specific to each task\n",
    "#     t1 = obspy.UTCDateTime(task[2])\n",
    "#     t2 = obspy.UTCDateTime(t1 + pd.Timedelta(1,'days'))\n",
    "#     network = task[0]\n",
    "#     station = task[1]\n",
    "\n",
    "#     #print network and station\n",
    "#     # Call to the function that will perform the operation and write the results to file\n",
    "#     run_detection(network,station,t1,t2,filepath,twin,step,l_blnd,r_blnd)\n",
    "    \n",
    "#     return\n",
    "\n",
    "\n",
    "# # # Now we set up the parallel operation\n",
    "# # # The below builds a framework for the computer to run in parallel. This doesn't actually execute anything.\n",
    "# lazy_results = [loop_days(task,filepath,twin,step,l_blnd,r_blnd) for task in task_list]\n",
    "    \n",
    "\n",
    "# # # The below actually executes the parallel operation!\n",
    "# # # It's nice to do it with the ProgressBar so you can see how long things are taking.\n",
    "# # # Each operation should also write a file so that is another way to check on progress.\n",
    "# # with ProgressBar():\n",
    "# #     #################################\n",
    "# #     # Add scheduler = 'single-threaded'\n",
    "# # \tdask.compute(lazy_results, scheduler='single-threaded') \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e31ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
