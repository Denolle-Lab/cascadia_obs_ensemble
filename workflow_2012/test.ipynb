{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/hbito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff7d8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter_share/miniconda3/envs/seismo/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d687020d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpwd\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pwd' is not defined"
     ]
    }
   ],
   "source": [
    "print(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c09aff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71645b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from obspy.clients.fdsn import Client\n",
    "import numpy as np\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from obspy.clients.fdsn.client import Client\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "from obspy import Stream\n",
    "\n",
    "from pnwstore.mseed import WaveformClient\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "import pandas as pd\n",
    "import gc\n",
    "import seisbench.models as sbm\n",
    "from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "from picking_utils import *\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define clients\n",
    "client_inventory = Client('IRIS')\n",
    "client_waveform = WaveformClient()\n",
    "client_ncedc = Client('NCEDC')\n",
    "\n",
    "# Parameters\n",
    "year1 = 2012\n",
    "filepath = \"home/hbito/cascadia_obs_ensemble/data/picks/\"\n",
    "os.makedirs(filepath,exist_ok=True)\n",
    "\n",
    "twin = 6000     # length of time window\n",
    "step = 3000     # step length\n",
    "l_blnd, r_blnd = 500, 500\n",
    "\n",
    "# Now create your list of days to loop over!\n",
    "t1 = datetime.datetime(year=year1,month=1,day=1)\n",
    "t2 = datetime.datetime(year=year1+1,month=1,day=1)\n",
    "time_bins = pd.to_datetime(np.arange(t1,t2,pd.Timedelta(1,'days')))\n",
    "\n",
    "inventory = client_inventory.get_stations(network=\"C8,7D,7A,CN,NV,UW,UO,NC,BK,TA,OO,PB,X6,Z5,X9\", station=\"*\", minlatitude=40,minlongitude=-127,maxlatitude=50,maxlongitude=-123, starttime=t1.strftime('%Y%m%d'),endtime=t2.strftime('%Y%m%d'))\n",
    "\n",
    "\n",
    "\n",
    "# Make a list of networks and stations\n",
    "networks_stas = []\n",
    "for i in range(len(inventory)):\n",
    "    network = inventory[i].code\n",
    "    \n",
    "    for j in range(len(inventory[i])):\n",
    "        networks_stas.append([network,inventory[i].stations[j].code])\n",
    "\n",
    "networks_stas =np.array(networks_stas)\n",
    "    \n",
    "# download models\n",
    "pretrain_list = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "\n",
    "# Combine that list of days with the list of stations and networks\n",
    "# We are essentially creating a list of the number of tasks we have to do with the information that is unique to each task; we will do them in parallel\n",
    "task_list = []\n",
    "for i in range(len(networks_stas)):\n",
    "\tfor t in time_bins:\n",
    "\t\ttask_list.append([networks_stas[i][0], networks_stas[i][1],t])\n",
    "        \n",
    "# Now we start setting up a parallel operation using a package called Dask.\n",
    "\n",
    "@dask.delayed\n",
    "def loop_days(task,filepath,twin,step,l_blnd,r_blnd):\n",
    "\n",
    "    # Define the parameters that are specific to each task\n",
    "    t1 = obspy.UTCDateTime(task[2])\n",
    "    t2 = obspy.UTCDateTime(t1 + pd.Timedelta(1,'days'))\n",
    "    network = task[0]\n",
    "    station = task[1]\n",
    "\n",
    "    #print network and station\n",
    "    print([network,station,t1])\n",
    "    # Call to the function that will perform the operation and write the results to file\n",
    "    try: \n",
    "        run_detection(network,station,t1,t2,filepath,twin,step,l_blnd,r_blnd)\n",
    "    except:\n",
    "        return\n",
    "\n",
    "\n",
    "# Now we set up the parallel operation\n",
    "# The below builds a framework for the computer to run in parallel. This doesn't actually execute anything.\n",
    "lazy_results = [loop_days(task,filepath,twin,step,l_blnd,r_blnd) for task in task_list]\n",
    "    \n",
    "\n",
    "# The below actually executes the parallel operation!\n",
    "# It's nice to do it with the ProgressBar so you can see how long things are taking.\n",
    "# Each operation should also write a file so that is another way to check on progress.\n",
    "with ProgressBar():\n",
    "    #################################\n",
    "    # Add scheduler = 'single-threaded'\n",
    "\tdask.compute(lazy_results, scheduler='single-threaded') \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ef92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
