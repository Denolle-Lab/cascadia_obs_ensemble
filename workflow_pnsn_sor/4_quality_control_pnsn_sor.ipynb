{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9659be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install basemap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92e5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import obspy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.clients.fdsn import Client as FDSNClient\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "from tqdm import tqdm\n",
    "from obspy.geodetics import locations2degrees, degrees2kilometers\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '../'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "    \n",
    "from plot_utils import *\n",
    "from qc_utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce408df1",
   "metadata": {},
   "source": [
    "## Morton Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd019d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CI YEAR</th>\n",
       "      <th>TSTRING</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>MINUTE</th>\n",
       "      <th>SECOND</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>...</th>\n",
       "      <th>dist to nearest stn</th>\n",
       "      <th>tt RMS</th>\n",
       "      <th>ERH</th>\n",
       "      <th>ERZ</th>\n",
       "      <th>STRIKE</th>\n",
       "      <th>DIP</th>\n",
       "      <th>RAKE</th>\n",
       "      <th>PLATE DESIGNATION</th>\n",
       "      <th>TEMPLATE EVENT?</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>47.3217</td>\n",
       "      <td>-123.2708</td>\n",
       "      <td>...</td>\n",
       "      <td>27.4</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interface</td>\n",
       "      <td>Catalog</td>\n",
       "      <td>2011-07-26 01:02:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.72</td>\n",
       "      <td>44.2888</td>\n",
       "      <td>-124.3340</td>\n",
       "      <td>...</td>\n",
       "      <td>163.8</td>\n",
       "      <td>0.06</td>\n",
       "      <td>13.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-07-26 01:02:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>44.3017</td>\n",
       "      <td>-124.3180</td>\n",
       "      <td>...</td>\n",
       "      <td>131.1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>35.4</td>\n",
       "      <td>22.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-07-26 01:02:08+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2.17</td>\n",
       "      <td>48.2635</td>\n",
       "      <td>-124.9298</td>\n",
       "      <td>...</td>\n",
       "      <td>44.4</td>\n",
       "      <td>0.77</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-07-26 07:31:02+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>27.63</td>\n",
       "      <td>48.3032</td>\n",
       "      <td>-124.9157</td>\n",
       "      <td>...</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>T</td>\n",
       "      <td>2011-07-26 09:50:27+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CI YEAR       TSTRING    YEAR  MONTH   DAY  HOUR  MINUTE  SECOND      LAT  \\\n",
       "0      1.0  2.011073e+13  2011.0    7.0  26.0   1.0     2.0    7.37  47.3217   \n",
       "1      1.0  2.011073e+13  2011.0    7.0  26.0   1.0     2.0    7.72  44.2888   \n",
       "2      1.0  2.011073e+13  2011.0    7.0  26.0   1.0     2.0    8.56  44.3017   \n",
       "3      1.0  2.011073e+13  2011.0    7.0  26.0   7.0    31.0    2.17  48.2635   \n",
       "4      1.0  2.011073e+13  2011.0    7.0  26.0   9.0    50.0   27.63  48.3032   \n",
       "\n",
       "        LON  ...  dist to nearest stn  tt RMS   ERH   ERZ  STRIKE  DIP  RAKE  \\\n",
       "0 -123.2708  ...                 27.4    0.19   0.8   1.2     NaN  NaN   NaN   \n",
       "1 -124.3340  ...                163.8    0.06  13.1   3.2     NaN  NaN   NaN   \n",
       "2 -124.3180  ...                131.1    0.50  35.4  22.2     NaN  NaN   NaN   \n",
       "3 -124.9298  ...                 44.4    0.77   3.5   6.4     NaN  NaN   NaN   \n",
       "4 -124.9157  ...                 46.1    0.94   4.0   6.9     NaN  NaN   NaN   \n",
       "\n",
       "   PLATE DESIGNATION  TEMPLATE EVENT?                  datetime  \n",
       "0          Interface          Catalog 2011-07-26 01:02:07+00:00  \n",
       "1        Upper Plate              NaN 2011-07-26 01:02:07+00:00  \n",
       "2        Upper Plate              NaN 2011-07-26 01:02:08+00:00  \n",
       "3        Upper Plate              NaN 2011-07-26 07:31:02+00:00  \n",
       "4        Upper Plate                T 2011-07-26 09:50:27+00:00  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Morton's catalog\n",
    "events_morton = pd.read_csv('../data/ds01.csv')\n",
    "# Convert the TSTRING to datetime\n",
    "events_morton['datetime'] = pd.to_datetime(events_morton['TSTRING'], format='%Y%m%d%H%M%S', utc=True)\n",
    "# Get the events in the Morton catalog \n",
    "t1 = pd.Timestamp('2011-1-1 00:00:00.000000+0000', tz='UTC')\n",
    "t2 = pd.Timestamp('2015-12-31 23:59:59.999999+0000', tz='UTC')\n",
    "\n",
    "events_morton= events_morton.loc[(events_morton['datetime'] > t1) & (events_morton['datetime'] < t2) ]\n",
    "\n",
    "events_morton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e58514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CI YEAR</th>\n",
       "      <th>TSTRING</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>MINUTE</th>\n",
       "      <th>SECOND</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>...</th>\n",
       "      <th>dist to nearest stn</th>\n",
       "      <th>tt RMS</th>\n",
       "      <th>ERH</th>\n",
       "      <th>ERZ</th>\n",
       "      <th>STRIKE</th>\n",
       "      <th>DIP</th>\n",
       "      <th>RAKE</th>\n",
       "      <th>PLATE DESIGNATION</th>\n",
       "      <th>TEMPLATE EVENT?</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.37</td>\n",
       "      <td>47.3217</td>\n",
       "      <td>-123.2708</td>\n",
       "      <td>...</td>\n",
       "      <td>27.4</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interface</td>\n",
       "      <td>Catalog</td>\n",
       "      <td>2011-07-26 01:02:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.72</td>\n",
       "      <td>44.2888</td>\n",
       "      <td>-124.3340</td>\n",
       "      <td>...</td>\n",
       "      <td>163.8</td>\n",
       "      <td>0.06</td>\n",
       "      <td>13.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-07-26 01:02:07+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>44.3017</td>\n",
       "      <td>-124.3180</td>\n",
       "      <td>...</td>\n",
       "      <td>131.1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>35.4</td>\n",
       "      <td>22.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-07-26 01:02:08+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2.17</td>\n",
       "      <td>48.2635</td>\n",
       "      <td>-124.9298</td>\n",
       "      <td>...</td>\n",
       "      <td>44.4</td>\n",
       "      <td>0.77</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-07-26 07:31:02+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.011073e+13</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>27.63</td>\n",
       "      <td>48.3032</td>\n",
       "      <td>-124.9157</td>\n",
       "      <td>...</td>\n",
       "      <td>46.1</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>T</td>\n",
       "      <td>2011-07-26 09:50:27+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5277</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.015101e+13</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.83</td>\n",
       "      <td>40.5895</td>\n",
       "      <td>-124.0455</td>\n",
       "      <td>...</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slab</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-07 08:01:50+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.015101e+13</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>40.5380</td>\n",
       "      <td>-123.7217</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-07 08:07:08+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.015101e+13</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.69</td>\n",
       "      <td>40.5822</td>\n",
       "      <td>-124.0432</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slab</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-07 11:31:26+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.015101e+13</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.52</td>\n",
       "      <td>40.2710</td>\n",
       "      <td>-124.3777</td>\n",
       "      <td>...</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upper Plate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-07 18:11:09+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.015101e+13</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>50.37</td>\n",
       "      <td>40.1602</td>\n",
       "      <td>-123.8320</td>\n",
       "      <td>...</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.3</td>\n",
       "      <td>18.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Undef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-07 21:45:50+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5282 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CI YEAR       TSTRING    YEAR  MONTH   DAY  HOUR  MINUTE  SECOND  \\\n",
       "0         1.0  2.011073e+13  2011.0    7.0  26.0   1.0     2.0    7.37   \n",
       "1         1.0  2.011073e+13  2011.0    7.0  26.0   1.0     2.0    7.72   \n",
       "2         1.0  2.011073e+13  2011.0    7.0  26.0   1.0     2.0    8.56   \n",
       "3         1.0  2.011073e+13  2011.0    7.0  26.0   7.0    31.0    2.17   \n",
       "4         1.0  2.011073e+13  2011.0    7.0  26.0   9.0    50.0   27.63   \n",
       "...       ...           ...     ...    ...   ...   ...     ...     ...   \n",
       "5277      4.0  2.015101e+13  2015.0   10.0   7.0   8.0     1.0   50.83   \n",
       "5278      4.0  2.015101e+13  2015.0   10.0   7.0   8.0     7.0    8.40   \n",
       "5279      4.0  2.015101e+13  2015.0   10.0   7.0  11.0    31.0   26.69   \n",
       "5280      4.0  2.015101e+13  2015.0   10.0   7.0  18.0    11.0    9.52   \n",
       "5281      4.0  2.015101e+13  2015.0   10.0   7.0  21.0    45.0   50.37   \n",
       "\n",
       "          LAT       LON  ...  dist to nearest stn  tt RMS   ERH   ERZ  STRIKE  \\\n",
       "0     47.3217 -123.2708  ...                 27.4    0.19   0.8   1.2     NaN   \n",
       "1     44.2888 -124.3340  ...                163.8    0.06  13.1   3.2     NaN   \n",
       "2     44.3017 -124.3180  ...                131.1    0.50  35.4  22.2     NaN   \n",
       "3     48.2635 -124.9298  ...                 44.4    0.77   3.5   6.4     NaN   \n",
       "4     48.3032 -124.9157  ...                 46.1    0.94   4.0   6.9     NaN   \n",
       "...       ...       ...  ...                  ...     ...   ...   ...     ...   \n",
       "5277  40.5895 -124.0455  ...                  5.4    0.14   0.9   0.8     NaN   \n",
       "5278  40.5380 -123.7217  ...                 14.9    0.09   3.3  13.5     NaN   \n",
       "5279  40.5822 -124.0432  ...                  5.6    0.05   1.7   0.8     NaN   \n",
       "5280  40.2710 -124.3777  ...                  7.9    0.19   0.7   0.2     NaN   \n",
       "5281  40.1602 -123.8320  ...                 29.4    0.22   1.3  18.6     NaN   \n",
       "\n",
       "      DIP  RAKE  PLATE DESIGNATION  TEMPLATE EVENT?                  datetime  \n",
       "0     NaN   NaN          Interface          Catalog 2011-07-26 01:02:07+00:00  \n",
       "1     NaN   NaN        Upper Plate              NaN 2011-07-26 01:02:07+00:00  \n",
       "2     NaN   NaN        Upper Plate              NaN 2011-07-26 01:02:08+00:00  \n",
       "3     NaN   NaN        Upper Plate              NaN 2011-07-26 07:31:02+00:00  \n",
       "4     NaN   NaN        Upper Plate                T 2011-07-26 09:50:27+00:00  \n",
       "...   ...   ...                ...              ...                       ...  \n",
       "5277  NaN   NaN               Slab              NaN 2015-10-07 08:01:50+00:00  \n",
       "5278  NaN   NaN        Upper Plate              NaN 2015-10-07 08:07:08+00:00  \n",
       "5279  NaN   NaN               Slab              NaN 2015-10-07 11:31:26+00:00  \n",
       "5280  NaN   NaN        Upper Plate              NaN 2015-10-07 18:11:09+00:00  \n",
       "5281  NaN   NaN              Undef              NaN 2015-10-07 21:45:50+00:00  \n",
       "\n",
       "[5282 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_morton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24957dc",
   "metadata": {},
   "source": [
    "## ANSS Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca384628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>nst</th>\n",
       "      <th>gap</th>\n",
       "      <th>dmin</th>\n",
       "      <th>...</th>\n",
       "      <th>place</th>\n",
       "      <th>type</th>\n",
       "      <th>horizontalError</th>\n",
       "      <th>depthError</th>\n",
       "      <th>magError</th>\n",
       "      <th>magNst</th>\n",
       "      <th>status</th>\n",
       "      <th>locationSource</th>\n",
       "      <th>magSource</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01T00:21:06.570Z</td>\n",
       "      <td>38.809834</td>\n",
       "      <td>-122.793663</td>\n",
       "      <td>1.746</td>\n",
       "      <td>0.88</td>\n",
       "      <td>md</td>\n",
       "      <td>14.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>...</td>\n",
       "      <td>6 km WSW of Cobb, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.010</td>\n",
       "      <td>16.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "      <td>2011-01-01 00:21:06.570000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01T02:04:41.480Z</td>\n",
       "      <td>38.789001</td>\n",
       "      <td>-122.747002</td>\n",
       "      <td>-0.844</td>\n",
       "      <td>0.28</td>\n",
       "      <td>md</td>\n",
       "      <td>5.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>...</td>\n",
       "      <td>4 km SSW of Cobb, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.200</td>\n",
       "      <td>6.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "      <td>2011-01-01 02:04:41.480000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01T02:53:49.640Z</td>\n",
       "      <td>38.817333</td>\n",
       "      <td>-122.821167</td>\n",
       "      <td>1.929</td>\n",
       "      <td>1.03</td>\n",
       "      <td>md</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>...</td>\n",
       "      <td>8 km W of Cobb, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.060</td>\n",
       "      <td>13.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "      <td>2011-01-01 02:53:49.640000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01T03:11:04.720Z</td>\n",
       "      <td>38.841835</td>\n",
       "      <td>-122.829002</td>\n",
       "      <td>1.076</td>\n",
       "      <td>0.43</td>\n",
       "      <td>md</td>\n",
       "      <td>7.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>...</td>\n",
       "      <td>9 km WNW of Cobb, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.150</td>\n",
       "      <td>7.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "      <td>2011-01-01 03:11:04.720000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01T03:32:55.630Z</td>\n",
       "      <td>38.835833</td>\n",
       "      <td>-122.807000</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.16</td>\n",
       "      <td>md</td>\n",
       "      <td>9.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>...</td>\n",
       "      <td>7 km WNW of Cobb, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "      <td>2011-01-01 03:32:55.630000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131527</th>\n",
       "      <td>131527</td>\n",
       "      <td>2015-12-31T22:15:46.650Z</td>\n",
       "      <td>38.837502</td>\n",
       "      <td>-122.825333</td>\n",
       "      <td>1.450</td>\n",
       "      <td>0.18</td>\n",
       "      <td>md</td>\n",
       "      <td>6.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>...</td>\n",
       "      <td>9 km W of Cobb, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.110</td>\n",
       "      <td>2.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "      <td>2015-12-31 22:15:46.650000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131528</th>\n",
       "      <td>131528</td>\n",
       "      <td>2015-12-31T22:18:13.120Z</td>\n",
       "      <td>41.856400</td>\n",
       "      <td>-119.599200</td>\n",
       "      <td>8.700</td>\n",
       "      <td>1.40</td>\n",
       "      <td>ml</td>\n",
       "      <td>6.0</td>\n",
       "      <td>210.1</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>...</td>\n",
       "      <td>45 km E of Fort Bidwell, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.210</td>\n",
       "      <td>3.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>nn</td>\n",
       "      <td>nn</td>\n",
       "      <td>2015-12-31 22:18:13.120000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131529</th>\n",
       "      <td>131529</td>\n",
       "      <td>2015-12-31T23:19:21.650Z</td>\n",
       "      <td>38.823334</td>\n",
       "      <td>-122.765663</td>\n",
       "      <td>1.680</td>\n",
       "      <td>0.54</td>\n",
       "      <td>md</td>\n",
       "      <td>7.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>...</td>\n",
       "      <td>3 km W of Cobb, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "      <td>2015-12-31 23:19:21.650000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131530</th>\n",
       "      <td>131530</td>\n",
       "      <td>2015-12-31T23:22:20.730Z</td>\n",
       "      <td>38.841000</td>\n",
       "      <td>-122.878166</td>\n",
       "      <td>1.730</td>\n",
       "      <td>0.77</td>\n",
       "      <td>md</td>\n",
       "      <td>8.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>...</td>\n",
       "      <td>12 km ENE of Cloverdale, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.180</td>\n",
       "      <td>3.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "      <td>2015-12-31 23:22:20.730000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131531</th>\n",
       "      <td>131531</td>\n",
       "      <td>2015-12-31T23:53:17.350Z</td>\n",
       "      <td>40.457667</td>\n",
       "      <td>-124.763000</td>\n",
       "      <td>24.960</td>\n",
       "      <td>2.32</td>\n",
       "      <td>md</td>\n",
       "      <td>16.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>...</td>\n",
       "      <td>44 km WSW of Ferndale, California</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.99</td>\n",
       "      <td>4.72</td>\n",
       "      <td>0.324</td>\n",
       "      <td>18.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>nc</td>\n",
       "      <td>nc</td>\n",
       "      <td>2015-12-31 23:53:17.350000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131532 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                      time   latitude   longitude   depth  \\\n",
       "0                0  2011-01-01T00:21:06.570Z  38.809834 -122.793663   1.746   \n",
       "1                1  2011-01-01T02:04:41.480Z  38.789001 -122.747002  -0.844   \n",
       "2                2  2011-01-01T02:53:49.640Z  38.817333 -122.821167   1.929   \n",
       "3                3  2011-01-01T03:11:04.720Z  38.841835 -122.829002   1.076   \n",
       "4                4  2011-01-01T03:32:55.630Z  38.835833 -122.807000   0.947   \n",
       "...            ...                       ...        ...         ...     ...   \n",
       "131527      131527  2015-12-31T22:15:46.650Z  38.837502 -122.825333   1.450   \n",
       "131528      131528  2015-12-31T22:18:13.120Z  41.856400 -119.599200   8.700   \n",
       "131529      131529  2015-12-31T23:19:21.650Z  38.823334 -122.765663   1.680   \n",
       "131530      131530  2015-12-31T23:22:20.730Z  38.841000 -122.878166   1.730   \n",
       "131531      131531  2015-12-31T23:53:17.350Z  40.457667 -124.763000  24.960   \n",
       "\n",
       "         mag magType   nst    gap      dmin  ...  \\\n",
       "0       0.88      md  14.0   73.0  0.001802  ...   \n",
       "1       0.28      md   5.0  122.0  0.009910  ...   \n",
       "2       1.03      md  40.0   44.0  0.010810  ...   \n",
       "3       0.43      md   7.0   77.0  0.013510  ...   \n",
       "4       0.16      md   9.0  112.0  0.012610  ...   \n",
       "...      ...     ...   ...    ...       ...  ...   \n",
       "131527  0.18      md   6.0  180.0  0.008108  ...   \n",
       "131528  1.40      ml   6.0  210.1  0.175000  ...   \n",
       "131529  0.54      md   7.0   99.0  0.008108  ...   \n",
       "131530  0.77      md   8.0   95.0  0.007207  ...   \n",
       "131531  2.32      md  16.0  301.0  0.325200  ...   \n",
       "\n",
       "                                      place        type horizontalError  \\\n",
       "0              6 km WSW of Cobb, California  earthquake            0.30   \n",
       "1              4 km SSW of Cobb, California  earthquake            0.42   \n",
       "2                8 km W of Cobb, California  earthquake            0.14   \n",
       "3              9 km WNW of Cobb, California  earthquake            0.43   \n",
       "4              7 km WNW of Cobb, California  earthquake            0.33   \n",
       "...                                     ...         ...             ...   \n",
       "131527           9 km W of Cobb, California  earthquake            0.66   \n",
       "131528  45 km E of Fort Bidwell, California  earthquake             NaN   \n",
       "131529           3 km W of Cobb, California  earthquake            0.50   \n",
       "131530  12 km ENE of Cloverdale, California  earthquake            0.58   \n",
       "131531    44 km WSW of Ferndale, California  earthquake            0.99   \n",
       "\n",
       "       depthError magError magNst     status  locationSource  magSource  \\\n",
       "0            0.56    0.010   16.0  automatic              nc         nc   \n",
       "1            2.39    0.200    6.0  automatic              nc         nc   \n",
       "2            0.19    0.060   13.0   reviewed              nc         nc   \n",
       "3            0.88    0.150    7.0  automatic              nc         nc   \n",
       "4            0.72    0.123    2.0   reviewed              nc         nc   \n",
       "...           ...      ...    ...        ...             ...        ...   \n",
       "131527       1.08    0.110    2.0  automatic              nc         nc   \n",
       "131528       3.40    0.210    3.0   reviewed              nn         nn   \n",
       "131529       1.54    0.030    2.0  automatic              nc         nc   \n",
       "131530       1.02    0.180    3.0  automatic              nc         nc   \n",
       "131531       4.72    0.324   18.0   reviewed              nc         nc   \n",
       "\n",
       "                               datetime  \n",
       "0      2011-01-01 00:21:06.570000+00:00  \n",
       "1      2011-01-01 02:04:41.480000+00:00  \n",
       "2      2011-01-01 02:53:49.640000+00:00  \n",
       "3      2011-01-01 03:11:04.720000+00:00  \n",
       "4      2011-01-01 03:32:55.630000+00:00  \n",
       "...                                 ...  \n",
       "131527 2015-12-31 22:15:46.650000+00:00  \n",
       "131528 2015-12-31 22:18:13.120000+00:00  \n",
       "131529 2015-12-31 23:19:21.650000+00:00  \n",
       "131530 2015-12-31 23:22:20.730000+00:00  \n",
       "131531 2015-12-31 23:53:17.350000+00:00  \n",
       "\n",
       "[131532 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_anss = pd.read_csv('../data/datasets_anss/anss_2011-15.csv')\n",
    "events_anss['datetime'] = pd.to_datetime(events_anss['time'], format='%Y-%m-%dT%H:%M:%S.%fZ', utc=True)\n",
    "events_anss= events_anss.loc[(events_anss['datetime'] > t1) & (events_anss['datetime'] < t2) ]\n",
    "events_anss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2499c46",
   "metadata": {},
   "source": [
    "## Our Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0456713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2224314/1685837610.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mycatalog['datetime'] = pd.to_datetime(mycatalog['time'], utc = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>idx</th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>picks</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>event_idx</th>\n",
       "      <th>pick_idx</th>\n",
       "      <th>residual</th>\n",
       "      <th>station</th>\n",
       "      <th>phase</th>\n",
       "      <th>time_pick</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01 10:44:25.058283+00:00</td>\n",
       "      <td>100.965600</td>\n",
       "      <td>-447.960938</td>\n",
       "      <td>6.640625</td>\n",
       "      <td>6</td>\n",
       "      <td>40.461192</td>\n",
       "      <td>-124.309615</td>\n",
       "      <td>6.640625</td>\n",
       "      <td>0</td>\n",
       "      <td>246022</td>\n",
       "      <td>0.185083</td>\n",
       "      <td>K02D</td>\n",
       "      <td>P</td>\n",
       "      <td>1.293879e+09</td>\n",
       "      <td>2011-01-01 10:44:25.058283+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-02 06:00:27.043481+00:00</td>\n",
       "      <td>-31.795797</td>\n",
       "      <td>-433.656918</td>\n",
       "      <td>12.109375</td>\n",
       "      <td>12</td>\n",
       "      <td>40.595524</td>\n",
       "      <td>-125.875626</td>\n",
       "      <td>12.109375</td>\n",
       "      <td>1</td>\n",
       "      <td>230134</td>\n",
       "      <td>0.866968</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.293948e+09</td>\n",
       "      <td>2011-01-02 06:00:27.043481+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02 22:40:11.017906+00:00</td>\n",
       "      <td>78.652760</td>\n",
       "      <td>-447.960938</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>11</td>\n",
       "      <td>40.463601</td>\n",
       "      <td>-124.572646</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>2</td>\n",
       "      <td>230277</td>\n",
       "      <td>0.451371</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294008e+09</td>\n",
       "      <td>2011-01-02 22:40:11.017906+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03 02:24:55.148617+00:00</td>\n",
       "      <td>90.924822</td>\n",
       "      <td>-254.856673</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>6</td>\n",
       "      <td>42.200770</td>\n",
       "      <td>-124.399086</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>3</td>\n",
       "      <td>220073</td>\n",
       "      <td>-0.211178</td>\n",
       "      <td>KBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294021e+09</td>\n",
       "      <td>2011-01-03 02:24:55.148617+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-03 07:08:42.728265+00:00</td>\n",
       "      <td>237.073922</td>\n",
       "      <td>-194.779791</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>8</td>\n",
       "      <td>42.710313</td>\n",
       "      <td>-122.606237</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>5</td>\n",
       "      <td>282727</td>\n",
       "      <td>-0.079647</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294039e+09</td>\n",
       "      <td>2011-01-03 07:08:42.728265+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13788</th>\n",
       "      <td>13788</td>\n",
       "      <td>2189</td>\n",
       "      <td>2015-12-29 21:21:54.859221+00:00</td>\n",
       "      <td>70.843266</td>\n",
       "      <td>-407.909683</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>12</td>\n",
       "      <td>40.824934</td>\n",
       "      <td>-124.660205</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>2189</td>\n",
       "      <td>172345</td>\n",
       "      <td>0.274588</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451424e+09</td>\n",
       "      <td>2015-12-29 21:21:54.859221+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>13800</td>\n",
       "      <td>2190</td>\n",
       "      <td>2015-12-30 07:41:43.690372+00:00</td>\n",
       "      <td>123.278439</td>\n",
       "      <td>-159.019742</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>7</td>\n",
       "      <td>43.058776</td>\n",
       "      <td>-123.986710</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>2190</td>\n",
       "      <td>383827</td>\n",
       "      <td>-0.221138</td>\n",
       "      <td>J01E</td>\n",
       "      <td>S</td>\n",
       "      <td>1.451461e+09</td>\n",
       "      <td>2015-12-30 07:41:43.690372+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13807</th>\n",
       "      <td>13807</td>\n",
       "      <td>2191</td>\n",
       "      <td>2015-12-30 15:14:18.895135+00:00</td>\n",
       "      <td>85.346612</td>\n",
       "      <td>-406.479281</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>9</td>\n",
       "      <td>40.836433</td>\n",
       "      <td>-124.488107</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>2191</td>\n",
       "      <td>383828</td>\n",
       "      <td>0.159986</td>\n",
       "      <td>J01E</td>\n",
       "      <td>S</td>\n",
       "      <td>1.451489e+09</td>\n",
       "      <td>2015-12-30 15:14:18.895135+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13816</th>\n",
       "      <td>13816</td>\n",
       "      <td>2192</td>\n",
       "      <td>2015-12-30 17:21:56.886771+00:00</td>\n",
       "      <td>76.421476</td>\n",
       "      <td>-173.323761</td>\n",
       "      <td>19.140625</td>\n",
       "      <td>7</td>\n",
       "      <td>42.936197</td>\n",
       "      <td>-124.563756</td>\n",
       "      <td>19.140625</td>\n",
       "      <td>2192</td>\n",
       "      <td>182969</td>\n",
       "      <td>-0.059408</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451496e+09</td>\n",
       "      <td>2015-12-30 17:21:56.886771+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13823</th>\n",
       "      <td>13823</td>\n",
       "      <td>2194</td>\n",
       "      <td>2015-12-31 09:46:36.747704+00:00</td>\n",
       "      <td>103.196884</td>\n",
       "      <td>-322.085565</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>7</td>\n",
       "      <td>41.594114</td>\n",
       "      <td>-124.262270</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>2194</td>\n",
       "      <td>185675</td>\n",
       "      <td>-0.005921</td>\n",
       "      <td>K02D</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451555e+09</td>\n",
       "      <td>2015-12-31 09:46:36.747704+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   idx                              time           x  \\\n",
       "0               0     0  2011-01-01 10:44:25.058283+00:00  100.965600   \n",
       "6               6     1  2011-01-02 06:00:27.043481+00:00  -31.795797   \n",
       "18             18     2  2011-01-02 22:40:11.017906+00:00   78.652760   \n",
       "29             29     3  2011-01-03 02:24:55.148617+00:00   90.924822   \n",
       "35             35     5  2011-01-03 07:08:42.728265+00:00  237.073922   \n",
       "...           ...   ...                               ...         ...   \n",
       "13788       13788  2189  2015-12-29 21:21:54.859221+00:00   70.843266   \n",
       "13800       13800  2190  2015-12-30 07:41:43.690372+00:00  123.278439   \n",
       "13807       13807  2191  2015-12-30 15:14:18.895135+00:00   85.346612   \n",
       "13816       13816  2192  2015-12-30 17:21:56.886771+00:00   76.421476   \n",
       "13823       13823  2194  2015-12-31 09:46:36.747704+00:00  103.196884   \n",
       "\n",
       "                y          z  picks   latitude   longitude      depth  \\\n",
       "0     -447.960938   6.640625      6  40.461192 -124.309615   6.640625   \n",
       "6     -433.656918  12.109375     12  40.595524 -125.875626  12.109375   \n",
       "18    -447.960938   1.953125     11  40.463601 -124.572646   1.953125   \n",
       "29    -254.856673   0.390625      6  42.200770 -124.399086   0.390625   \n",
       "35    -194.779791   0.390625      8  42.710313 -122.606237   0.390625   \n",
       "...           ...        ...    ...        ...         ...        ...   \n",
       "13788 -407.909683   0.390625     12  40.824934 -124.660205   0.390625   \n",
       "13800 -159.019742   5.859375      7  43.058776 -123.986710   5.859375   \n",
       "13807 -406.479281   5.859375      9  40.836433 -124.488107   5.859375   \n",
       "13816 -173.323761  19.140625      7  42.936197 -124.563756  19.140625   \n",
       "13823 -322.085565   1.171875      7  41.594114 -124.262270   1.171875   \n",
       "\n",
       "       event_idx  pick_idx  residual station phase     time_pick  \\\n",
       "0              0    246022  0.185083    K02D     P  1.293879e+09   \n",
       "6              1    230134  0.866968     DBO     P  1.293948e+09   \n",
       "18             2    230277  0.451371     DBO     P  1.294008e+09   \n",
       "29             3    220073 -0.211178     KBO     P  1.294021e+09   \n",
       "35             5    282727 -0.079647     DBO     P  1.294039e+09   \n",
       "...          ...       ...       ...     ...   ...           ...   \n",
       "13788       2189    172345  0.274588     DBO     P  1.451424e+09   \n",
       "13800       2190    383827 -0.221138    J01E     S  1.451461e+09   \n",
       "13807       2191    383828  0.159986    J01E     S  1.451489e+09   \n",
       "13816       2192    182969 -0.059408     DBO     P  1.451496e+09   \n",
       "13823       2194    185675 -0.005921    K02D     P  1.451555e+09   \n",
       "\n",
       "                              datetime  \n",
       "0     2011-01-01 10:44:25.058283+00:00  \n",
       "6     2011-01-02 06:00:27.043481+00:00  \n",
       "18    2011-01-02 22:40:11.017906+00:00  \n",
       "29    2011-01-03 02:24:55.148617+00:00  \n",
       "35    2011-01-03 07:08:42.728265+00:00  \n",
       "...                                ...  \n",
       "13788 2015-12-29 21:21:54.859221+00:00  \n",
       "13800 2015-12-30 07:41:43.690372+00:00  \n",
       "13807 2015-12-30 15:14:18.895135+00:00  \n",
       "13816 2015-12-30 17:21:56.886771+00:00  \n",
       "13823 2015-12-31 09:46:36.747704+00:00  \n",
       "\n",
       "[1780 rows x 17 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all the pick assignments \n",
    "year = 'pnsn_sor'\n",
    "mycatalog_picks = pd.read_csv(f'../data/datasets_{year}/all_pick_assignments_{year}.csv')\n",
    "mycatalog = mycatalog_picks.drop_duplicates(subset=['idx'])\n",
    "# Convert the time series in all_pick_assignments to datetime\n",
    "mycatalog['datetime'] = pd.to_datetime(mycatalog['time'], utc = True)\n",
    "mycatalog=mycatalog.loc[(mycatalog['datetime'] > t1) & (mycatalog['datetime'] < t2) ]\n",
    "mycatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4e9ca",
   "metadata": {},
   "source": [
    "## Run this loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0e7740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched idx of Morton:  147\n",
      "matched idx of Morton:  338\n",
      "matched idx of Morton:  347\n",
      "matched idx of Morton:  351\n",
      "matched idx of Morton:  359\n",
      "matched idx of Morton:  361\n",
      "matched idx of Morton:  364\n",
      "matched idx of Morton:  371\n",
      "matched idx of Morton:  373\n",
      "matched idx of Morton:  378\n",
      "matched idx of Morton:  379\n",
      "matched idx of Morton:  389\n",
      "matched idx of Morton:  390\n",
      "matched idx of Morton:  391\n",
      "matched idx of Morton:  392\n",
      "matched idx of Morton:  393\n",
      "matched idx of Morton:  394\n",
      "matched idx of Morton:  395\n",
      "matched idx of Morton:  396\n",
      "matched idx of Morton:  397\n",
      "matched idx of Morton:  398\n",
      "matched idx of Morton:  399\n",
      "matched idx of Morton:  403\n",
      "matched idx of Morton:  404\n",
      "matched idx of Morton:  405\n",
      "matched idx of Morton:  407\n",
      "matched idx of Morton:  408\n",
      "matched idx of Morton:  410\n",
      "matched idx of Morton:  411\n",
      "matched idx of Morton:  412\n",
      "matched idx of Morton:  413\n",
      "matched idx of Morton:  414\n",
      "matched idx of Morton:  415\n",
      "matched idx of Morton:  416\n",
      "matched idx of Morton:  418\n",
      "matched idx of Morton:  423\n",
      "matched idx of Morton:  425\n",
      "matched idx of Morton:  429\n",
      "matched idx of Morton:  431\n",
      "matched idx of Morton:  432\n",
      "matched idx of Morton:  435\n",
      "matched idx of Morton:  436\n",
      "matched idx of Morton:  437\n",
      "matched idx of Morton:  439\n",
      "matched idx of Morton:  440\n",
      "matched idx of Morton:  445\n",
      "matched idx of Morton:  463\n",
      "matched idx of Morton:  476\n",
      "matched idx of Morton:  490\n",
      "matched idx of Morton:  498\n",
      "matched idx of Morton:  500\n",
      "matched idx of Morton:  501\n",
      "matched idx of Morton:  502\n",
      "matched idx of Morton:  503\n",
      "matched idx of Morton:  504\n",
      "matched idx of Morton:  506\n",
      "matched idx of Morton:  507\n",
      "matched idx of Morton:  509\n",
      "matched idx of Morton:  515\n",
      "matched idx of Morton:  516\n",
      "matched idx of Morton:  517\n",
      "matched idx of Morton:  518\n",
      "matched idx of Morton:  521\n",
      "matched idx of Morton:  525\n",
      "matched idx of Morton:  529\n",
      "matched idx of Morton:  533\n",
      "matched idx of Morton:  543\n",
      "matched idx of Morton:  553\n",
      "matched idx of Morton:  557\n",
      "matched idx of Morton:  558\n",
      "matched idx of Morton:  559\n",
      "matched idx of Morton:  560\n",
      "matched idx of Morton:  577\n",
      "matched idx of Morton:  578\n",
      "matched idx of Morton:  585\n",
      "matched idx of Morton:  590\n",
      "matched idx of Morton:  592\n",
      "matched idx of Morton:  603\n",
      "matched idx of Morton:  604\n",
      "matched idx of Morton:  610\n",
      "matched idx of Morton:  612\n",
      "matched idx of Morton:  633\n",
      "matched idx of Morton:  679\n",
      "matched idx of Morton:  682\n",
      "matched idx of Morton:  688\n",
      "matched idx of Morton:  694\n",
      "matched idx of Morton:  696\n",
      "matched idx of Morton:  701\n",
      "matched idx of Morton:  709\n",
      "matched idx of Morton:  710\n",
      "matched idx of Morton:  722\n",
      "matched idx of Morton:  750\n",
      "matched idx of Morton:  754\n",
      "matched idx of Morton:  759\n",
      "matched idx of Morton:  764\n",
      "matched idx of Morton:  773\n",
      "matched idx of Morton:  780\n",
      "matched idx of Morton:  781\n",
      "matched idx of Morton:  796\n",
      "matched idx of Morton:  802\n",
      "matched idx of Morton:  826\n",
      "matched idx of Morton:  831\n",
      "matched idx of Morton:  855\n",
      "matched idx of Morton:  860\n",
      "matched idx of Morton:  867\n",
      "matched idx of Morton:  919\n",
      "matched idx of Morton:  921\n",
      "matched idx of Morton:  1765\n",
      "matched idx of Morton:  1766\n",
      "matched idx of Morton:  1999\n",
      "matched idx of Morton:  2021\n",
      "matched idx of Morton:  2053\n",
      "matched idx of Morton:  2055\n",
      "matched idx of Morton:  2057\n",
      "matched idx of Morton:  2061\n",
      "matched idx of Morton:  2062\n",
      "matched idx of Morton:  2063\n",
      "matched idx of Morton:  2064\n",
      "matched idx of Morton:  2071\n",
      "matched idx of Morton:  2076\n",
      "matched idx of Morton:  2123\n",
      "matched idx of Morton:  2126\n",
      "matched idx of Morton:  2134\n",
      "matched idx of Morton:  2174\n",
      "matched idx of Morton:  2215\n",
      "matched idx of Morton:  2232\n",
      "matched idx of Morton:  2234\n",
      "matched idx of Morton:  2235\n",
      "matched idx of Morton:  2236\n",
      "matched idx of Morton:  2243\n",
      "matched idx of Morton:  2255\n",
      "matched idx of Morton:  2265\n",
      "matched idx of Morton:  2280\n",
      "matched idx of Morton:  2321\n",
      "matched idx of Morton:  2325\n",
      "matched idx of Morton:  2328\n",
      "matched idx of Morton:  2338\n",
      "matched idx of Morton:  2339\n",
      "matched idx of Morton:  2344\n",
      "matched idx of Morton:  2357\n",
      "matched idx of Morton:  2362\n",
      "matched idx of Morton:  2373\n",
      "matched idx of Morton:  2374\n",
      "matched idx of Morton:  2403\n",
      "matched idx of Morton:  2429\n",
      "matched idx of Morton:  2487\n",
      "matched idx of Morton:  2521\n",
      "matched idx of Morton:  2530\n",
      "matched idx of Morton:  2539\n",
      "matched idx of Morton:  2556\n",
      "matched idx of Morton:  2579\n",
      "matched idx of Morton:  2582\n",
      "matched idx of Morton:  2621\n",
      "matched idx of Morton:  2654\n",
      "matched idx of Morton:  2670\n",
      "matched idx of Morton:  2675\n",
      "matched idx of Morton:  2690\n",
      "matched idx of Morton:  2694\n",
      "matched idx of Morton:  2701\n",
      "matched idx of Morton:  2705\n",
      "matched idx of Morton:  2723\n",
      "matched idx of Morton:  2735\n",
      "matched idx of Morton:  2780\n",
      "matched idx of Morton:  2788\n",
      "matched idx of Morton:  2795\n",
      "matched idx of Morton:  2836\n",
      "matched idx of Morton:  2843\n",
      "matched idx of Morton:  2862\n",
      "matched idx of Morton:  2865\n",
      "matched idx of Morton:  2939\n",
      "matched idx of Morton:  2940\n",
      "matched idx of Morton:  2985\n",
      "matched idx of Morton:  2987\n",
      "matched idx of Morton:  3001\n",
      "matched idx of Morton:  3004\n",
      "matched idx of Morton:  3007\n",
      "matched idx of Morton:  3009\n",
      "matched idx of Morton:  3016\n",
      "matched idx of Morton:  3035\n",
      "matched idx of Morton:  3037\n",
      "matched idx of Morton:  3065\n",
      "matched idx of Morton:  3077\n",
      "matched idx of Morton:  3085\n",
      "matched idx of Morton:  3088\n",
      "matched idx of Morton:  3124\n",
      "matched idx of Morton:  3125\n",
      "matched idx of Morton:  3140\n",
      "matched idx of Morton:  3146\n",
      "matched idx of Morton:  3152\n",
      "matched idx of Morton:  3165\n",
      "matched idx of Morton:  3176\n",
      "matched idx of Morton:  3177\n",
      "matched idx of Morton:  3178\n",
      "matched idx of Morton:  3189\n",
      "matched idx of Morton:  3195\n",
      "matched idx of Morton:  3211\n",
      "matched idx of Morton:  3229\n",
      "matched idx of Morton:  3233\n",
      "matched idx of Morton:  3257\n",
      "matched idx of Morton:  3280\n",
      "matched idx of Morton:  3290\n",
      "matched idx of Morton:  3314\n",
      "matched idx of Morton:  3317\n",
      "matched idx of Morton:  3346\n",
      "matched idx of Morton:  3354\n",
      "matched idx of Morton:  3355\n",
      "matched idx of Morton:  3368\n",
      "matched idx of Morton:  3382\n",
      "matched idx of Morton:  3409\n",
      "matched idx of Morton:  3440\n",
      "matched idx of Morton:  3449\n",
      "matched idx of Morton:  3466\n",
      "matched idx of Morton:  3524\n",
      "matched idx of Morton:  3535\n",
      "matched idx of Morton:  3547\n",
      "matched idx of Morton:  3548\n",
      "matched idx of Morton:  3549\n",
      "matched idx of Morton:  3552\n",
      "matched idx of Morton:  3574\n",
      "matched idx of Morton:  3585\n",
      "matched idx of Morton:  3595\n",
      "matched idx of Morton:  3659\n",
      "matched idx of Morton:  3672\n",
      "matched idx of Morton:  3695\n",
      "matched idx of Morton:  3725\n",
      "matched idx of Morton:  3726\n",
      "matched idx of Morton:  3767\n",
      "matched idx of Morton:  3794\n",
      "matched idx of Morton:  3805\n",
      "matched idx of Morton:  3868\n",
      "matched idx of Morton:  3899\n",
      "matched idx of Morton:  3900\n",
      "matched idx of Morton:  3906\n",
      "matched idx of Morton:  3916\n",
      "matched idx of Morton:  3923\n",
      "matched idx of Morton:  3937\n",
      "matched idx of Morton:  3942\n",
      "matched idx of Morton:  3945\n",
      "matched idx of Morton:  3947\n",
      "matched idx of Morton:  3974\n",
      "matched idx of Morton:  3988\n",
      "matched idx of Morton:  3991\n",
      "matched idx of Morton:  3995\n",
      "matched idx of Morton:  4007\n",
      "matched idx of Morton:  4012\n",
      "matched idx of Morton:  4086\n",
      "matched idx of Morton:  4107\n",
      "matched idx of Morton:  4108\n",
      "matched idx of Morton:  4126\n",
      "matched idx of Morton:  4135\n",
      "matched idx of Morton:  4203\n",
      "matched idx of Morton:  4204\n",
      "matched idx of Morton:  4233\n",
      "matched idx of Morton:  4239\n",
      "matched idx of Morton:  4246\n",
      "matched idx of Morton:  4320\n",
      "matched idx of Morton:  4340\n",
      "matched idx of Morton:  4341\n",
      "matched idx of Morton:  4372\n",
      "matched idx of Morton:  4374\n",
      "matched idx of Morton:  4382\n",
      "matched idx of Morton:  4402\n",
      "matched idx of Morton:  4439\n",
      "matched idx of Morton:  4469\n",
      "matched idx of Morton:  4470\n",
      "matched idx of Morton:  4491\n",
      "matched idx of Morton:  4515\n",
      "matched idx of Morton:  4517\n",
      "matched idx of Morton:  4545\n",
      "matched idx of Morton:  4567\n",
      "matched idx of Morton:  4593\n",
      "matched idx of Morton:  4617\n",
      "matched idx of Morton:  4620\n",
      "matched idx of Morton:  4622\n",
      "matched idx of Morton:  4641\n",
      "matched idx of Morton:  4659\n",
      "matched idx of Morton:  4695\n",
      "matched idx of Morton:  4696\n",
      "matched idx of Morton:  4697\n",
      "matched idx of Morton:  4708\n",
      "matched idx of Morton:  4758\n",
      "matched idx of Morton:  4831\n",
      "matched idx of Morton:  4861\n",
      "matched idx of Morton:  4879\n",
      "matched idx of Morton:  4900\n",
      "matched idx of Morton:  4932\n",
      "matched idx of Morton:  4998\n",
      "matched idx of Morton:  5024\n",
      "matched idx of Morton:  5075\n",
      "matched idx of Morton:  5087\n",
      "matched idx of Morton:  5098\n",
      "matched idx of Morton:  5126\n",
      "matched idx of Morton:  5147\n",
      "matched idx of Morton:  5149\n",
      "matched idx of Morton:  5151\n",
      "matched idx of Morton:  5178\n",
      "matched idx of Morton:  5184\n",
      "matched idx of Morton:  5220\n",
      "matched idx of Morton:  5266\n",
      "matched idx of Morton:  5272\n",
      "1780 1068\n",
      "length of mycatalog:1780\n",
      "length of events_morton:5282\n",
      "length of events_anss:131532\n",
      "matched_indices_morton:300\n",
      "matched_indices_anss:575\n",
      "unmatched_indices_morton:1480\n",
      "unmatched_indices_anss:1205\n",
      "unmatched_indices_morton_and_anss (new events):1068\n"
     ]
    }
   ],
   "source": [
    "matched_events_mycatalog2morton = []\n",
    "matched_times_morton2mycatalog = []\n",
    "matched_events_morton2mycatalog = []\n",
    "unmatched_times_morton2mycatalog = []\n",
    "unmatched_events_morton2mycatalog = []\n",
    "unmatched_events_mycatalog2morton_and_anss = []\n",
    "matched_events_anss2mycatalog=[]\n",
    "matched_events_mycatalog2anss = []\n",
    "unmatched_times_anss2mycatalog = []\n",
    "unmatched_events_anss2mycatalog = []\n",
    "matched_times_anss2mycatalog =[]\n",
    "\n",
    "time_threshold = 5 # in seconds\n",
    "dist_threshold =25\n",
    "\n",
    "lat_morton2mycatalog = []\n",
    "lon_morton2mycatalog = []\n",
    "\n",
    "lat_anss2mycatalog =[]\n",
    "lon_anss2mycatalog = []\n",
    "\n",
    "# A set to keep track of matched indices in mycatalog\n",
    "matched_indices_morton = set()\n",
    "matched_indices_anss = set()\n",
    "count_c = 0\n",
    "count_d = 0\n",
    "# Loop over events in Morton's catalog\n",
    "for i in range(len(events_morton)):\n",
    "    t11 = events_morton.iloc[i]['datetime']\n",
    "    olat = events_morton.iloc[i]['LAT']\n",
    "    olon = events_morton.iloc[i]['LON']\n",
    "    \n",
    "                                       \n",
    "    condition = (mycatalog['datetime'] >= t11 - pd.Timedelta(seconds=time_threshold)) & \\\n",
    "                     (mycatalog['datetime'] <= t11 + pd.Timedelta(seconds=time_threshold)) & \\\n",
    "                     (degrees2kilometers(locations2degrees(olat, olon, mycatalog['latitude'], mycatalog['longitude'])) <= dist_threshold)                        \n",
    "    matched_df = mycatalog.loc[condition]\n",
    "\n",
    "    if len(matched_df) == 1:\n",
    "        print('matched idx of Morton: ',i)\n",
    "        count_c+=1\n",
    "        matched_times_morton2mycatalog.append(t11)\n",
    "        matched_events_morton2mycatalog.append(events_morton.iloc[i])\n",
    "        lat_morton2mycatalog.append(events_morton.iloc[i]['LAT'])\n",
    "        lon_morton2mycatalog.append(events_morton.iloc[i]['LON'])\n",
    "        matched_events_mycatalog2morton.append(matched_df)\n",
    "        matched_indices_morton.update(matched_df['idx'])\n",
    "    elif len(matched_df) > 1:\n",
    "        print('matched idx of Morton: ',i)\n",
    "\n",
    "        count_d+=1\n",
    "        diffs = abs(matched_df['datetime'] - t11)\n",
    "        closest_index = diffs.idxmin()\n",
    "        closest_event = matched_df.loc[[closest_index]]\n",
    "        matched_times_morton2mycatalog.append(t11)\n",
    "        matched_events_morton2mycatalog.append(events_morton.iloc[i])\n",
    "        lat_morton2mycatalog.append(events_morton.iloc[i]['LAT'])\n",
    "        lon_morton2mycatalog.append(events_morton.iloc[i]['LON'])\n",
    "        matched_events_mycatalog2morton.append(closest_event)\n",
    "        matched_indices_morton.update(closest_event['idx'])\n",
    "    else:\n",
    "        unmatched_times_morton2mycatalog.append(t11)\n",
    "        unmatched_events_morton2mycatalog.append(events_morton.iloc[i])\n",
    "\n",
    "# All events in mycatalog not matched with Morton's catalog are unmatched\n",
    "unmatched_indices_morton = set(mycatalog.idx) - matched_indices_morton\n",
    "# unmatched_events_mycatalog2morton = mycatalog.iloc[list(unmatched_indices_morton)]\n",
    "count_a =0\n",
    "count_b =0\n",
    "# Loop over events in ANSS catalog\n",
    "for i in range(len(events_anss)):\n",
    "    t11 = events_anss.iloc[i]['datetime']\n",
    "    olat = events_anss.iloc[i]['latitude']\n",
    "    olon = events_anss.iloc[i]['longitude']\n",
    "    \n",
    "                                       \n",
    "    condition = (mycatalog['datetime'] >= t11 - pd.Timedelta(seconds=time_threshold)) & \\\n",
    "                     (mycatalog['datetime'] <= t11 + pd.Timedelta(seconds=time_threshold)) & \\\n",
    "                     (degrees2kilometers(locations2degrees(olat, olon, mycatalog['latitude'], mycatalog['longitude'])) <= dist_threshold)                        \n",
    "    matched_df = mycatalog.loc[condition]\n",
    "    \n",
    "    if len(matched_df) == 1:\n",
    "        \n",
    "        count_a+=1\n",
    "        matched_times_anss2mycatalog.append(t11)\n",
    "        matched_events_anss2mycatalog.append(events_anss.iloc[i])\n",
    "        lat_anss2mycatalog.append(events_anss.iloc[i]['latitude'])\n",
    "        lon_anss2mycatalog.append(events_anss.iloc[i]['longitude'])\n",
    "        matched_events_mycatalog2anss.append(matched_df)\n",
    "        matched_indices_anss.update(matched_df['idx'])\n",
    "        \n",
    "    elif len(matched_df) > 1:\n",
    "        count_b+=1\n",
    "        diffs = abs(matched_df['datetime'] - t11)\n",
    "        closest_index = diffs.idxmin()\n",
    "        closest_event = matched_df.loc[[closest_index]]\n",
    "        matched_times_anss2mycatalog.append(t11)\n",
    "        matched_events_anss2mycatalog.append(events_anss.iloc[i])\n",
    "        lat_anss2mycatalog.append(events_anss.iloc[i]['latitude'])\n",
    "        lon_anss2mycatalog.append(events_anss.iloc[i]['longitude'])\n",
    "        matched_events_mycatalog2anss.append(closest_event)\n",
    "        matched_indices_anss.update(closest_event['idx'])\n",
    "        \n",
    "    else:\n",
    "        unmatched_times_anss2mycatalog.append(t11)\n",
    "        unmatched_events_anss2mycatalog.append(events_anss.iloc[i])\n",
    "\n",
    "# All events in mycatalog not matched with ANSS catalog are unmatched\n",
    "unmatched_indices_anss = set(mycatalog.idx) - matched_indices_anss\n",
    "\n",
    "unmatched_indices_morton_and_anss = unmatched_indices_morton.intersection(unmatched_indices_anss)\n",
    "print(len(mycatalog),len(unmatched_indices_morton_and_anss))\n",
    "\n",
    "# unmatched_events_mycatalog2morton_and_anss = mycatalog.iloc[list(unmatched_indices_morton_and_anss)]\n",
    "unmatched_events_mycatalog2morton_and_anss = mycatalog.loc[mycatalog['event_idx'].isin(list(unmatched_indices_morton_and_anss))]\n",
    "\n",
    "\n",
    "print(f\"length of mycatalog:{len(mycatalog)}\")\n",
    "print(f\"length of events_morton:{len(events_morton)}\")\n",
    "print(f\"length of events_anss:{len(events_anss)}\")\n",
    "print(f\"matched_indices_morton:{len(matched_indices_morton)}\")\n",
    "print(f\"matched_indices_anss:{len(matched_indices_anss)}\")\n",
    "print(f\"unmatched_indices_morton:{len(unmatched_indices_morton)}\")\n",
    "print(f\"unmatched_indices_anss:{len(unmatched_indices_anss)}\")\n",
    "print(f\"unmatched_indices_morton_and_anss (new events):{len(unmatched_indices_morton_and_anss)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b6dd0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of mycatalog:1780\n",
      "length of events_morton:5282\n",
      "length of events_anss:131532\n",
      "matched_events_mycatalog2morton:300\n",
      "matched_events_morton2mycatalog:300\n",
      "unmatched_events_anss2mycatalog:130957\n",
      "unmatched_events_morton2mycatalog:4982\n",
      "unmatched_events_mycatalog2morton_and_anss (new events):1068\n",
      "matched_events_mycatalog2anss:575\n",
      "matched_events_anss2mycatalog:575\n"
     ]
    }
   ],
   "source": [
    "# new concat code\n",
    "# Concatenate and clean up dataframes\n",
    "if len(matched_events_mycatalog2morton)>0:\n",
    "    matched_events_mycatalog2morton = pd.concat(matched_events_mycatalog2morton).reset_index(drop=True)\n",
    "    matched_times_mycatalog2morton = matched_events_mycatalog2morton['datetime']\n",
    "    lat_mycatalog2morton = matched_events_mycatalog2morton['latitude']\n",
    "    lon_mycatalog2morton = matched_events_mycatalog2morton['longitude']\n",
    "\n",
    "if len(matched_events_morton2mycatalog)>0:   \n",
    "    matched_events_morton2mycatalog = pd.DataFrame(matched_events_morton2mycatalog).reset_index(drop=True)\n",
    "    \n",
    "if len(unmatched_events_anss2mycatalog)>0:\n",
    "    unmatched_events_anss2mycatalog = pd.DataFrame(unmatched_events_anss2mycatalog).reset_index(drop=True)\n",
    "    \n",
    "if len(unmatched_events_morton2mycatalog)>0:\n",
    "    unmatched_events_morton2mycatalog = pd.DataFrame(unmatched_events_morton2mycatalog).reset_index(drop=True)\n",
    "\n",
    "if len(unmatched_events_mycatalog2morton_and_anss) > 0:\n",
    "    unmatched_events_mycatalog2morton_and_anss = unmatched_events_mycatalog2morton_and_anss.reset_index(drop=True)\n",
    "\n",
    "if len(matched_events_mycatalog2anss)>0:\n",
    "    matched_events_mycatalog2anss = pd.concat(matched_events_mycatalog2anss).reset_index(drop=True)\n",
    "    matched_times_mycatalog2anss = matched_events_mycatalog2anss['datetime']\n",
    "    lat_mycatalog2anss = matched_events_mycatalog2anss['latitude']\n",
    "    lon_mycatalog2anss = matched_events_mycatalog2anss['longitude']\n",
    "\n",
    "if len(matched_events_anss2mycatalog)>0:   \n",
    "    matched_events_anss2mycatalog = pd.DataFrame(matched_events_anss2mycatalog).reset_index(drop=True)\n",
    "    matched_times_anss2mycatalog = matched_events_anss2mycatalog['datetime']\n",
    "\n",
    "print(f\"length of mycatalog:{len(mycatalog)}\")\n",
    "print(f\"length of events_morton:{len(events_morton)}\")\n",
    "print(f\"length of events_anss:{len(events_anss)}\")\n",
    "print(f\"matched_events_mycatalog2morton:{len(matched_events_mycatalog2morton)}\")\n",
    "print(f\"matched_events_morton2mycatalog:{len(matched_events_morton2mycatalog)}\")\n",
    "print(f\"unmatched_events_anss2mycatalog:{len(unmatched_events_anss2mycatalog)}\")\n",
    "print(f\"unmatched_events_morton2mycatalog:{len(unmatched_events_morton2mycatalog)}\")\n",
    "print(f\"unmatched_events_mycatalog2morton_and_anss (new events):{len(unmatched_events_mycatalog2morton_and_anss)}\")\n",
    "print(f\"matched_events_mycatalog2anss:{len(matched_events_mycatalog2anss)}\")\n",
    "print(f\"matched_events_anss2mycatalog:{len(matched_events_anss2mycatalog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0da443d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save these three catalogs to csv files\n",
    "matched_events_mycatalog2morton.to_csv(f'../data/datasets_{year}/matched_events_with_morton_mycatalog.csv')\n",
    "matched_events_anss2mycatalog.to_csv(f'../data/datasets_{year}/matched_events_with_mycatalog_anss.csv')\n",
    "matched_events_morton2mycatalog.to_csv(f'../data/datasets_{year}/matched_events_with_mycatalog_morton.csv')\n",
    "unmatched_events_mycatalog2morton_and_anss.to_csv(f'../data/datasets_{year}/new_events.csv')\n",
    "matched_events_mycatalog2anss.to_csv(f'../data/datasets_{year}/matched_events_with_anss_mycatalog.csv')\n",
    "unmatched_events_morton2mycatalog.to_csv(f'../data/datasets_{year}/missing_events_from_mycatalog_morton.csv')\n",
    "unmatched_events_anss2mycatalog.to_csv(f'../data/datasets_{year}/missing_events_from_mycatalog_anss.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b308175",
   "metadata": {},
   "source": [
    "## Plot the Origin Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b722c",
   "metadata": {},
   "source": [
    "### Compare with Morton's Catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists for plotting the 1:1 line\n",
    "x = pd.date_range(start='2011-1-1', end='2015-12-31', periods=len(matched_times_morton2mycatalog))\n",
    "y = pd.date_range(start='2011-1-1', end='2015-12-31', periods=len(matched_times_morton2mycatalog))\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure()\n",
    "plt.scatter(matched_times_mycatalog2morton,matched_times_morton2mycatalog)\n",
    "plt.plot(x,y, 'k--')\n",
    "plt.xlabel('Origin Time of mycatalog')\n",
    "plt.ylabel('Origin Time of Morton')\n",
    "plt.title('Scatter Plot: Origin Times of Matched Events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a2d257",
   "metadata": {},
   "source": [
    "### Compare with the ANSS Catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists for plotting the 1:1 line\n",
    "matched_times_mycatalog2anss = pd.to_datetime(matched_times_mycatalog2anss)\n",
    "matched_times_anss2mycatalog = pd.to_datetime(matched_times_anss2mycatalog)\n",
    "\n",
    "x = pd.date_range(start='2014-1-1', end='2014-12-31', periods=len(matched_times_anss2mycatalog))\n",
    "y = pd.date_range(start='2014-1-1', end='2014-12-31', periods=len(matched_times_anss2mycatalog))\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure()\n",
    "plt.scatter(matched_times_mycatalog2anss,matched_times_anss2mycatalog)\n",
    "plt.plot(x,y, 'k--')\n",
    "plt.xlabel('Origin Time of mycatalog')\n",
    "plt.ylabel('Origin Time of Morton')\n",
    "plt.title('Scatter Plot: Origin Times of Matched Events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43d9d5",
   "metadata": {},
   "source": [
    "## Plot the Latitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed02e61",
   "metadata": {},
   "source": [
    "### Compare with Morton's Catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77491ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot\n",
    "plt.figure()\n",
    "lat_min = min(lat_mycatalog2morton)\n",
    "lat_max = max(lat_mycatalog2morton)\n",
    "\n",
    "# Make lists for plotting the 1:1 line\n",
    "x_lat = np.linspace(lat_min, lat_max, 10)\n",
    "y_lat = np.linspace(lat_min, lat_max, 10)\n",
    "plt.scatter(lat_mycatalog2morton,lat_morton2mycatalog, s=15)\n",
    "plt.plot(x_lat,y_lat, 'k--')\n",
    "plt.xlabel('Latitudes of Our Catalog ($^\\circ$)')\n",
    "plt.ylabel('Latitudes of Morton et al. (2023) ($^\\circ$)')\n",
    "plt.title('Latitudes of Our Catalog vs.Latitudes of Morton et al. (2023)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9ef27",
   "metadata": {},
   "source": [
    "### Compare with the ANSS Catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot\n",
    "plt.figure()\n",
    "lat_min = min(lat_mycatalog2anss)\n",
    "lat_max = max(lat_mycatalog2anss)\n",
    "\n",
    "# Make lists for plotting the 1:1 line\n",
    "x_lat = np.linspace(lat_min, lat_max, 10)\n",
    "y_lat = np.linspace(lat_min, lat_max, 10)\n",
    "plt.scatter(lat_mycatalog2anss,lat_anss2mycatalog, s=15)\n",
    "plt.plot(x_lat,y_lat, 'k--')\n",
    "plt.xlabel('Latitudes of Our Catalog ($^\\circ$)')\n",
    "plt.ylabel('Latitudes of ANSS Catalog ($^\\circ$)')\n",
    "plt.title('Latitudes of Our Catalog vs. ANSS Catalog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc628b1a",
   "metadata": {},
   "source": [
    "## Plot the Longitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb31b59",
   "metadata": {},
   "source": [
    "### Compare with Morton's Catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3873dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot\n",
    "plt.figure()\n",
    "lon_min = min(lon_mycatalog2morton)\n",
    "lon_max = max(lon_mycatalog2morton)\n",
    "\n",
    "# Make lists for plotting the 1:1 line\n",
    "x_lon = np.linspace(lon_min, lon_max, 10)\n",
    "y_lon = np.linspace(lon_min, lon_max, 10)\n",
    "plt.scatter(lon_mycatalog2morton,lon_morton2mycatalog)\n",
    "plt.plot(x_lon,y_lon, 'k--')\n",
    "plt.xlabel('Longitudes of Our Catalog ($^\\circ$)')\n",
    "plt.ylabel('Longitudes of Morton et al. (2023) ($^\\circ$)')\n",
    "plt.title('Longitudes of Our Catalog vs.Latitudes of Morton et al. (2023)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd4045",
   "metadata": {},
   "source": [
    "### Compare with the ANSS Catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31380f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot\n",
    "plt.figure()\n",
    "lon_min = min(lon_mycatalog2anss)\n",
    "lon_max = max(lon_mycatalog2anss)\n",
    "\n",
    "# Make lists for plotting the 1:1 line\n",
    "x_lon = np.linspace(lon_min, lon_max, 10)\n",
    "y_lon = np.linspace(lon_min, lon_max, 10)\n",
    "plt.scatter(lon_mycatalog2anss,lon_anss2mycatalog)\n",
    "plt.plot(x_lon,y_lon, 'k--')\n",
    "plt.xlabel('Longitudes of Our Catalog ($^\\circ$)')\n",
    "plt.ylabel('Longitudes of Morton et al. (2023) ($^\\circ$)')\n",
    "plt.title('Longitudes of Our Catalog vs.Latitudes of Morton et al. (2023)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c25f0",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "Make histograms of mycatalog vs. Morton's catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814dcb8",
   "metadata": {},
   "source": [
    "### Histogram: All of our catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8dd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv('../../data/datasets_all_years/events_pnsn_wa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the same histogram but overlay the histograms\n",
    "bins = np.linspace(5,30,25)\n",
    "\n",
    "plt.hist(unmatched_events_mycatalog2morton_and_anss['picks'], bins=bins,  \n",
    "         alpha=0.5, # the transaparency parameter \n",
    "         label='Newly detected events') \n",
    "plt.hist(matched_events_mycatalog2anss['picks'], bins=bins,\n",
    "         alpha=0.5, \n",
    "         label='Events matching the ANSS Catalog')   \n",
    "plt.hist(matched_events_mycatalog2morton['picks'], bins=bins,\n",
    "         alpha=0.5, \n",
    "         label='Events matching Morton et al., 2023') \n",
    "\n",
    "plt.xlabel('Number of Picks')\n",
    "plt.ylabel('Number of Events')  \n",
    "plt.legend(loc='upper right') \n",
    "plt.title('Picks vs. Number of Events') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039ec69",
   "metadata": {},
   "source": [
    "### Histogram: Matched events alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db128b09",
   "metadata": {},
   "source": [
    "### Compare with Morton et al. (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e453e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the number of picks for the matched events\n",
    "plt.figure()\n",
    "bins = np.linspace(5,30,25)\n",
    "plt.hist(matched_events_mycatalog2morton['picks'],bins=bins)\n",
    "plt.xlabel('Number of Picks')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.title('Histogram of Matched Events with Morton et al. (2023): Picks vs. the Number of Events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84052dd5",
   "metadata": {},
   "source": [
    "### Compare with ANSS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the number of picks for the matched events\n",
    "plt.figure()\n",
    "bins = np.linspace(5,30,25)\n",
    "plt.hist(matched_events_mycatalog2anss['picks'],bins=bins)\n",
    "plt.xlabel('Number of Picks')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.title('Histogram of Matched Events with ANSS: Picks vs. the Number of Events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b3b7f",
   "metadata": {},
   "source": [
    "## 2. Filter events \n",
    "Filter Events based on the following criteria\n",
    "1. For an event, at least two stations have to be less than 50 km from the event and no station can be 100 km apart from each other.\n",
    "2. An event has to have more than or equal to 6 picks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcc97847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>idx</th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>picks</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>event_idx</th>\n",
       "      <th>pick_idx</th>\n",
       "      <th>residual</th>\n",
       "      <th>station</th>\n",
       "      <th>phase</th>\n",
       "      <th>time_pick</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03 02:24:55.148617+00:00</td>\n",
       "      <td>90.924822</td>\n",
       "      <td>-254.856673</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>6</td>\n",
       "      <td>42.200770</td>\n",
       "      <td>-124.399086</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>3</td>\n",
       "      <td>220073</td>\n",
       "      <td>-0.211178</td>\n",
       "      <td>KBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294021e+09</td>\n",
       "      <td>2011-01-03 02:24:55.148617+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>2011-01-03 07:14:53.750228+00:00</td>\n",
       "      <td>56.339920</td>\n",
       "      <td>-447.960938</td>\n",
       "      <td>12.890625</td>\n",
       "      <td>6</td>\n",
       "      <td>40.465413</td>\n",
       "      <td>-124.835705</td>\n",
       "      <td>12.890625</td>\n",
       "      <td>6</td>\n",
       "      <td>397177</td>\n",
       "      <td>0.342941</td>\n",
       "      <td>K02D</td>\n",
       "      <td>S</td>\n",
       "      <td>1.294039e+09</td>\n",
       "      <td>2011-01-03 07:14:53.750228+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>2011-01-04 11:32:14.917683+00:00</td>\n",
       "      <td>55.224278</td>\n",
       "      <td>-427.935310</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>7</td>\n",
       "      <td>40.645813</td>\n",
       "      <td>-124.847109</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>10</td>\n",
       "      <td>229644</td>\n",
       "      <td>0.094996</td>\n",
       "      <td>K02D</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294141e+09</td>\n",
       "      <td>2011-01-04 11:32:14.917683+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-01-06 04:41:30.807808+00:00</td>\n",
       "      <td>79.768402</td>\n",
       "      <td>-133.272507</td>\n",
       "      <td>11.328125</td>\n",
       "      <td>7</td>\n",
       "      <td>43.296315</td>\n",
       "      <td>-124.517004</td>\n",
       "      <td>11.328125</td>\n",
       "      <td>12</td>\n",
       "      <td>270683</td>\n",
       "      <td>-0.017572</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294289e+09</td>\n",
       "      <td>2011-01-06 04:41:30.807808+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>76</td>\n",
       "      <td>14</td>\n",
       "      <td>2011-01-06 12:08:03.937083+00:00</td>\n",
       "      <td>70.843266</td>\n",
       "      <td>-433.656918</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>9</td>\n",
       "      <td>40.593103</td>\n",
       "      <td>-124.663113</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>14</td>\n",
       "      <td>270744</td>\n",
       "      <td>0.270885</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294316e+09</td>\n",
       "      <td>2011-01-06 12:08:03.937083+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>1063</td>\n",
       "      <td>13755</td>\n",
       "      <td>2185</td>\n",
       "      <td>2015-12-26 00:02:24.232579+00:00</td>\n",
       "      <td>15.061167</td>\n",
       "      <td>-449.391340</td>\n",
       "      <td>25.390625</td>\n",
       "      <td>10</td>\n",
       "      <td>40.454304</td>\n",
       "      <td>-125.322445</td>\n",
       "      <td>25.390625</td>\n",
       "      <td>2185</td>\n",
       "      <td>180459</td>\n",
       "      <td>0.405924</td>\n",
       "      <td>J01E</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451088e+09</td>\n",
       "      <td>2015-12-26 00:02:24.232579+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>1064</td>\n",
       "      <td>13765</td>\n",
       "      <td>2186</td>\n",
       "      <td>2015-12-29 00:15:57.947990+00:00</td>\n",
       "      <td>74.190192</td>\n",
       "      <td>-425.074506</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>10</td>\n",
       "      <td>40.670087</td>\n",
       "      <td>-124.622568</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>2186</td>\n",
       "      <td>172333</td>\n",
       "      <td>0.234788</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451348e+09</td>\n",
       "      <td>2015-12-29 00:15:57.947990+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1065</td>\n",
       "      <td>13788</td>\n",
       "      <td>2189</td>\n",
       "      <td>2015-12-29 21:21:54.859221+00:00</td>\n",
       "      <td>70.843266</td>\n",
       "      <td>-407.909683</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>12</td>\n",
       "      <td>40.824934</td>\n",
       "      <td>-124.660205</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>2189</td>\n",
       "      <td>172345</td>\n",
       "      <td>0.274588</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451424e+09</td>\n",
       "      <td>2015-12-29 21:21:54.859221+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1066</td>\n",
       "      <td>13800</td>\n",
       "      <td>2190</td>\n",
       "      <td>2015-12-30 07:41:43.690372+00:00</td>\n",
       "      <td>123.278439</td>\n",
       "      <td>-159.019742</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>7</td>\n",
       "      <td>43.058776</td>\n",
       "      <td>-123.986710</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>2190</td>\n",
       "      <td>383827</td>\n",
       "      <td>-0.221138</td>\n",
       "      <td>J01E</td>\n",
       "      <td>S</td>\n",
       "      <td>1.451461e+09</td>\n",
       "      <td>2015-12-30 07:41:43.690372+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1067</td>\n",
       "      <td>13816</td>\n",
       "      <td>2192</td>\n",
       "      <td>2015-12-30 17:21:56.886771+00:00</td>\n",
       "      <td>76.421476</td>\n",
       "      <td>-173.323761</td>\n",
       "      <td>19.140625</td>\n",
       "      <td>7</td>\n",
       "      <td>42.936197</td>\n",
       "      <td>-124.563756</td>\n",
       "      <td>19.140625</td>\n",
       "      <td>2192</td>\n",
       "      <td>182969</td>\n",
       "      <td>-0.059408</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451496e+09</td>\n",
       "      <td>2015-12-30 17:21:56.886771+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1068 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0   idx                              time  \\\n",
       "0                0          29     3  2011-01-03 02:24:55.148617+00:00   \n",
       "1                1          43     6  2011-01-03 07:14:53.750228+00:00   \n",
       "2                2          62    10  2011-01-04 11:32:14.917683+00:00   \n",
       "3                3          69    12  2011-01-06 04:41:30.807808+00:00   \n",
       "4                4          76    14  2011-01-06 12:08:03.937083+00:00   \n",
       "...            ...         ...   ...                               ...   \n",
       "1063          1063       13755  2185  2015-12-26 00:02:24.232579+00:00   \n",
       "1064          1064       13765  2186  2015-12-29 00:15:57.947990+00:00   \n",
       "1065          1065       13788  2189  2015-12-29 21:21:54.859221+00:00   \n",
       "1066          1066       13800  2190  2015-12-30 07:41:43.690372+00:00   \n",
       "1067          1067       13816  2192  2015-12-30 17:21:56.886771+00:00   \n",
       "\n",
       "               x           y          z  picks   latitude   longitude  \\\n",
       "0      90.924822 -254.856673   0.390625      6  42.200770 -124.399086   \n",
       "1      56.339920 -447.960938  12.890625      6  40.465413 -124.835705   \n",
       "2      55.224278 -427.935310   0.390625      7  40.645813 -124.847109   \n",
       "3      79.768402 -133.272507  11.328125      7  43.296315 -124.517004   \n",
       "4      70.843266 -433.656918   0.390625      9  40.593103 -124.663113   \n",
       "...          ...         ...        ...    ...        ...         ...   \n",
       "1063   15.061167 -449.391340  25.390625     10  40.454304 -125.322445   \n",
       "1064   74.190192 -425.074506   1.171875     10  40.670087 -124.622568   \n",
       "1065   70.843266 -407.909683   0.390625     12  40.824934 -124.660205   \n",
       "1066  123.278439 -159.019742   5.859375      7  43.058776 -123.986710   \n",
       "1067   76.421476 -173.323761  19.140625      7  42.936197 -124.563756   \n",
       "\n",
       "          depth  event_idx  pick_idx  residual station phase     time_pick  \\\n",
       "0      0.390625          3    220073 -0.211178     KBO     P  1.294021e+09   \n",
       "1     12.890625          6    397177  0.342941    K02D     S  1.294039e+09   \n",
       "2      0.390625         10    229644  0.094996    K02D     P  1.294141e+09   \n",
       "3     11.328125         12    270683 -0.017572     DBO     P  1.294289e+09   \n",
       "4      0.390625         14    270744  0.270885     DBO     P  1.294316e+09   \n",
       "...         ...        ...       ...       ...     ...   ...           ...   \n",
       "1063  25.390625       2185    180459  0.405924    J01E     P  1.451088e+09   \n",
       "1064   1.171875       2186    172333  0.234788     DBO     P  1.451348e+09   \n",
       "1065   0.390625       2189    172345  0.274588     DBO     P  1.451424e+09   \n",
       "1066   5.859375       2190    383827 -0.221138    J01E     S  1.451461e+09   \n",
       "1067  19.140625       2192    182969 -0.059408     DBO     P  1.451496e+09   \n",
       "\n",
       "                              datetime  \n",
       "0     2011-01-03 02:24:55.148617+00:00  \n",
       "1     2011-01-03 07:14:53.750228+00:00  \n",
       "2     2011-01-04 11:32:14.917683+00:00  \n",
       "3     2011-01-06 04:41:30.807808+00:00  \n",
       "4     2011-01-06 12:08:03.937083+00:00  \n",
       "...                                ...  \n",
       "1063  2015-12-26 00:02:24.232579+00:00  \n",
       "1064  2015-12-29 00:15:57.947990+00:00  \n",
       "1065  2015-12-29 21:21:54.859221+00:00  \n",
       "1066  2015-12-30 07:41:43.690372+00:00  \n",
       "1067  2015-12-30 17:21:56.886771+00:00  \n",
       "\n",
       "[1068 rows x 18 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the new events file\n",
    "events = pd.read_csv(f'../data/datasets_{year}/new_events.csv')\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b49b018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>idx</th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>picks</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>event_idx</th>\n",
       "      <th>pick_idx</th>\n",
       "      <th>residual</th>\n",
       "      <th>station</th>\n",
       "      <th>phase</th>\n",
       "      <th>time_pick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01 10:44:25.058283+00:00</td>\n",
       "      <td>100.965600</td>\n",
       "      <td>-447.960938</td>\n",
       "      <td>6.640625</td>\n",
       "      <td>6</td>\n",
       "      <td>40.461192</td>\n",
       "      <td>-124.309615</td>\n",
       "      <td>6.640625</td>\n",
       "      <td>0</td>\n",
       "      <td>246022</td>\n",
       "      <td>0.185083</td>\n",
       "      <td>K02D</td>\n",
       "      <td>P</td>\n",
       "      <td>1.293879e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-02 06:00:27.043481+00:00</td>\n",
       "      <td>-31.795797</td>\n",
       "      <td>-433.656918</td>\n",
       "      <td>12.109375</td>\n",
       "      <td>12</td>\n",
       "      <td>40.595524</td>\n",
       "      <td>-125.875626</td>\n",
       "      <td>12.109375</td>\n",
       "      <td>1</td>\n",
       "      <td>230134</td>\n",
       "      <td>0.866968</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.293948e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02 22:40:11.017906+00:00</td>\n",
       "      <td>78.652760</td>\n",
       "      <td>-447.960938</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>11</td>\n",
       "      <td>40.463601</td>\n",
       "      <td>-124.572646</td>\n",
       "      <td>1.953125</td>\n",
       "      <td>2</td>\n",
       "      <td>230277</td>\n",
       "      <td>0.451371</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294008e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03 02:24:55.148617+00:00</td>\n",
       "      <td>90.924822</td>\n",
       "      <td>-254.856673</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>6</td>\n",
       "      <td>42.200770</td>\n",
       "      <td>-124.399086</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>3</td>\n",
       "      <td>220073</td>\n",
       "      <td>-0.211178</td>\n",
       "      <td>KBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294021e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-03 07:08:42.728265+00:00</td>\n",
       "      <td>237.073922</td>\n",
       "      <td>-194.779791</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>8</td>\n",
       "      <td>42.710313</td>\n",
       "      <td>-122.606237</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>5</td>\n",
       "      <td>282727</td>\n",
       "      <td>-0.079647</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.294039e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13788</th>\n",
       "      <td>13788</td>\n",
       "      <td>2189</td>\n",
       "      <td>2015-12-29 21:21:54.859221+00:00</td>\n",
       "      <td>70.843266</td>\n",
       "      <td>-407.909683</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>12</td>\n",
       "      <td>40.824934</td>\n",
       "      <td>-124.660205</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>2189</td>\n",
       "      <td>172345</td>\n",
       "      <td>0.274588</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451424e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>13800</td>\n",
       "      <td>2190</td>\n",
       "      <td>2015-12-30 07:41:43.690372+00:00</td>\n",
       "      <td>123.278439</td>\n",
       "      <td>-159.019742</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>7</td>\n",
       "      <td>43.058776</td>\n",
       "      <td>-123.986710</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>2190</td>\n",
       "      <td>383827</td>\n",
       "      <td>-0.221138</td>\n",
       "      <td>J01E</td>\n",
       "      <td>S</td>\n",
       "      <td>1.451461e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13807</th>\n",
       "      <td>13807</td>\n",
       "      <td>2191</td>\n",
       "      <td>2015-12-30 15:14:18.895135+00:00</td>\n",
       "      <td>85.346612</td>\n",
       "      <td>-406.479281</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>9</td>\n",
       "      <td>40.836433</td>\n",
       "      <td>-124.488107</td>\n",
       "      <td>5.859375</td>\n",
       "      <td>2191</td>\n",
       "      <td>383828</td>\n",
       "      <td>0.159986</td>\n",
       "      <td>J01E</td>\n",
       "      <td>S</td>\n",
       "      <td>1.451489e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13816</th>\n",
       "      <td>13816</td>\n",
       "      <td>2192</td>\n",
       "      <td>2015-12-30 17:21:56.886771+00:00</td>\n",
       "      <td>76.421476</td>\n",
       "      <td>-173.323761</td>\n",
       "      <td>19.140625</td>\n",
       "      <td>7</td>\n",
       "      <td>42.936197</td>\n",
       "      <td>-124.563756</td>\n",
       "      <td>19.140625</td>\n",
       "      <td>2192</td>\n",
       "      <td>182969</td>\n",
       "      <td>-0.059408</td>\n",
       "      <td>DBO</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451496e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13823</th>\n",
       "      <td>13823</td>\n",
       "      <td>2194</td>\n",
       "      <td>2015-12-31 09:46:36.747704+00:00</td>\n",
       "      <td>103.196884</td>\n",
       "      <td>-322.085565</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>7</td>\n",
       "      <td>41.594114</td>\n",
       "      <td>-124.262270</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>2194</td>\n",
       "      <td>185675</td>\n",
       "      <td>-0.005921</td>\n",
       "      <td>K02D</td>\n",
       "      <td>P</td>\n",
       "      <td>1.451555e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   idx                              time           x  \\\n",
       "0               0     0  2011-01-01 10:44:25.058283+00:00  100.965600   \n",
       "6               6     1  2011-01-02 06:00:27.043481+00:00  -31.795797   \n",
       "18             18     2  2011-01-02 22:40:11.017906+00:00   78.652760   \n",
       "29             29     3  2011-01-03 02:24:55.148617+00:00   90.924822   \n",
       "35             35     5  2011-01-03 07:08:42.728265+00:00  237.073922   \n",
       "...           ...   ...                               ...         ...   \n",
       "13788       13788  2189  2015-12-29 21:21:54.859221+00:00   70.843266   \n",
       "13800       13800  2190  2015-12-30 07:41:43.690372+00:00  123.278439   \n",
       "13807       13807  2191  2015-12-30 15:14:18.895135+00:00   85.346612   \n",
       "13816       13816  2192  2015-12-30 17:21:56.886771+00:00   76.421476   \n",
       "13823       13823  2194  2015-12-31 09:46:36.747704+00:00  103.196884   \n",
       "\n",
       "                y          z  picks   latitude   longitude      depth  \\\n",
       "0     -447.960938   6.640625      6  40.461192 -124.309615   6.640625   \n",
       "6     -433.656918  12.109375     12  40.595524 -125.875626  12.109375   \n",
       "18    -447.960938   1.953125     11  40.463601 -124.572646   1.953125   \n",
       "29    -254.856673   0.390625      6  42.200770 -124.399086   0.390625   \n",
       "35    -194.779791   0.390625      8  42.710313 -122.606237   0.390625   \n",
       "...           ...        ...    ...        ...         ...        ...   \n",
       "13788 -407.909683   0.390625     12  40.824934 -124.660205   0.390625   \n",
       "13800 -159.019742   5.859375      7  43.058776 -123.986710   5.859375   \n",
       "13807 -406.479281   5.859375      9  40.836433 -124.488107   5.859375   \n",
       "13816 -173.323761  19.140625      7  42.936197 -124.563756  19.140625   \n",
       "13823 -322.085565   1.171875      7  41.594114 -124.262270   1.171875   \n",
       "\n",
       "       event_idx  pick_idx  residual station phase     time_pick  \n",
       "0              0    246022  0.185083    K02D     P  1.293879e+09  \n",
       "6              1    230134  0.866968     DBO     P  1.293948e+09  \n",
       "18             2    230277  0.451371     DBO     P  1.294008e+09  \n",
       "29             3    220073 -0.211178     KBO     P  1.294021e+09  \n",
       "35             5    282727 -0.079647     DBO     P  1.294039e+09  \n",
       "...          ...       ...       ...     ...   ...           ...  \n",
       "13788       2189    172345  0.274588     DBO     P  1.451424e+09  \n",
       "13800       2190    383827 -0.221138    J01E     S  1.451461e+09  \n",
       "13807       2191    383828  0.159986    J01E     S  1.451489e+09  \n",
       "13816       2192    182969 -0.059408     DBO     P  1.451496e+09  \n",
       "13823       2194    185675 -0.005921    K02D     P  1.451555e+09  \n",
       "\n",
       "[1780 rows x 16 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pick_assignments = pd.read_csv(f'../data/datasets_{year}/all_pick_assignments_{year}.csv')\n",
    "mycatalog = all_pick_assignments.drop_duplicates(subset=['idx'])\n",
    "mycatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c23e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "650f1d56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2224314/2718798803.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mycatalog['datetime'] = pd.to_datetime(mycatalog['time'], utc = True)\n",
      "  0%|          | 2/684 [00:02<12:01,  1.06s/it]\n"
     ]
    },
    {
     "ename": "FDSNException",
     "evalue": "Unknown Error (ConnectionResetError): [Errno 104] Connection reset by peer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFDSNException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m min_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m station \u001b[38;5;129;01min\u001b[39;00m pick_sta:\n\u001b[0;32m---> 34\u001b[0m     sta_inv \u001b[38;5;241m=\u001b[39m \u001b[43mclient2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_stations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m?H?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mstarttime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motime\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sta_inv) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetworks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00motime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.9/site-packages/obspy/clients/fdsn/client.py:748\u001b[0m, in \u001b[0;36mClient.get_stations\u001b[0;34m(self, starttime, endtime, startbefore, startafter, endbefore, endafter, network, station, location, channel, minlatitude, maxlatitude, minlongitude, maxlongitude, latitude, longitude, minradius, maxradius, level, includerestricted, includeavailability, updatedafter, matchtimeseries, filename, format, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m setup_query_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation\u001b[39m\u001b[38;5;124m'\u001b[39m, locs, kwargs)\n\u001b[1;32m    745\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_url_from_parameters(\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstation\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_PARAMETERS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation\u001b[39m\u001b[38;5;124m'\u001b[39m], kwargs)\n\u001b[0;32m--> 748\u001b[0m data_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m data_stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.9/site-packages/obspy/clients/fdsn/client.py:1486\u001b[0m, in \u001b[0;36mClient._download\u001b[0;34m(self, url, return_string, data, use_gzip, content_type)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m content_type\n\u001b[1;32m   1482\u001b[0m code, data \u001b[38;5;241m=\u001b[39m download_url(\n\u001b[1;32m   1483\u001b[0m     url, opener\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_opener, headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1484\u001b[0m     debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug, return_string\u001b[38;5;241m=\u001b[39mreturn_string, data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1485\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, use_gzip\u001b[38;5;241m=\u001b[39muse_gzip)\n\u001b[0;32m-> 1486\u001b[0m \u001b[43mraise_on_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.9/site-packages/obspy/clients/fdsn/client.py:1856\u001b[0m, in \u001b[0;36mraise_on_error\u001b[0;34m(code, data)\u001b[0m\n\u001b[1;32m   1854\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FDSNTimeoutException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed Out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1856\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FDSNException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown Error (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m   1857\u001b[0m             (\u001b[38;5;28mstr\u001b[39m(data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m), \u001b[38;5;28mstr\u001b[39m(data))))\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;66;03m# Catch any non 200 codes.\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[0;31mFDSNException\u001b[0m: Unknown Error (ConnectionResetError): [Errno 104] Connection reset by peer"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Inputs:\n",
    "1. Either of the following files can be the input:\n",
    "    1. matched_events_with_morton_mycatalog.csv from the 4_quality_control file\n",
    "    2. matched_events_with_anss_mycatalog.csv from the 4_quality_control file\n",
    "    3. new_events.csv from the 4_quality_control file\n",
    "2. The all_pick_assignments CSV file from the 3_associate file: e.g., all_pick_assignments = pd.read_csv('../data/datasets_2012/all_pick_assignments_2012.csv')\n",
    "\n",
    "\n",
    "Outputs:\n",
    "1. A new dataframe that only has events that fall into the following categories:\n",
    "    1. For an event, at least two stations have to be less than 50 km from the event and no station can be 100 km apart from each other.\n",
    "2. An event has to have more than or equal to 6 picks\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "client2 = Client(\"IRIS\")\n",
    "\n",
    "mycatalog = all_pick_assignments.drop_duplicates(subset=['idx'])\n",
    "mycatalog['datetime'] = pd.to_datetime(mycatalog['time'], utc = True)\n",
    "\n",
    "for i, idx in tqdm(enumerate(events['idx']),total=len(events['idx'])):\n",
    "    event = mycatalog\n",
    "    picks = all_pick_assignments\n",
    "    picks_idx = picks.loc[picks['idx']==idx]\n",
    "    pick_sta = np.unique(picks_idx['station'])\n",
    "    otime = UTCDateTime(str(event[event['idx'] == idx][\"datetime\"].values[0]))\n",
    "    distances = []\n",
    "    max_dist = 10\n",
    "    min_dist = 0\n",
    "    for station in pick_sta:\n",
    "\n",
    "\n",
    "        sta_inv = client2.get_stations(network='*',\n",
    "                                       station=station, channel=\"?H?\", \n",
    "                                       starttime=otime - 1e8, endtime=otime + 1e8,level=\"response\")\n",
    "        if len(sta_inv) == 0:\n",
    "            print(f\"Failed to fetch for {networks} {station} {otime}\")\n",
    "            continue\n",
    "\n",
    "        _network = sta_inv[0].code\n",
    "        slat = sta_inv[0][0].latitude\n",
    "        slon = sta_inv[0][0].longitude\n",
    "        olat = event.loc[event['idx']==idx, 'latitude'].values[0]\n",
    "        olon = event.loc[event['idx']==idx, 'longitude'].values[0]\n",
    "\n",
    "        dis1 = locations2degrees(olat, olon, slat, slon)\n",
    "        dist = degrees2kilometers(dis1)\n",
    "\n",
    "        distances.append([None, _network, station, dist])\n",
    "\n",
    "    # Sort distances\n",
    "    distances = sorted(distances, key=lambda item: item[-1])\n",
    "\n",
    "    # This is for the first criterion in the markdown above\n",
    "    # Determine if any two of the numbers in the distances list are less than or equal to 50\n",
    "    found = False\n",
    "    for i in range(len(distances)):\n",
    "        for j in range(i + 1, len(distances)):\n",
    "            if distances[i][3] <= 50 and distances[j][3] <= 50:\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "    # Make a list that includes the differences between the consecutive numbers in the distances list.\n",
    "    differences = [distances[i+1][3] - distances[i][3] for i in range(len(distances) - 1)]\n",
    "\n",
    "    if found == False: # If there were not at least two distances between the station and the event less than or equal to 50 km\n",
    "        print(distances)\n",
    "        index = events[events['idx'] == idx].index\n",
    "        events = events.drop(index=index)\n",
    "\n",
    "    elif any(differences > 100 for differences in differences): # If any of the distances between two stations were greater than 100 km\n",
    "        print(distances)\n",
    "        index = events[events['idx'] == idx].index\n",
    "        events = events.drop(index=index)\n",
    "\n",
    "    else: \n",
    "        continue\n",
    "\n",
    "\n",
    "events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3444633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_events_filtered = filter_sta(new_events, mycatalog_picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da81e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events_filtered.to_csv(f'../data/datasets_{year}/new_events_filtered_{year}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f4637d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.to_csv(f'../data/datasets_{year}/new_events_filtered_{year}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "575d60f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>idx</th>\n",
       "      <th>time</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>picks</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>event_idx</th>\n",
       "      <th>pick_idx</th>\n",
       "      <th>residual</th>\n",
       "      <th>station</th>\n",
       "      <th>phase</th>\n",
       "      <th>time_pick</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-08-09 10:26:19.327686+00:00</td>\n",
       "      <td>88.408817</td>\n",
       "      <td>254.877921</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>6</td>\n",
       "      <td>46.787357</td>\n",
       "      <td>-124.342163</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>2</td>\n",
       "      <td>2109680</td>\n",
       "      <td>-0.551091</td>\n",
       "      <td>FN07A</td>\n",
       "      <td>P</td>\n",
       "      <td>1.312886e+09</td>\n",
       "      <td>2011-08-09 10:26:19.327686+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>2011-10-16 22:22:10.456747+00:00</td>\n",
       "      <td>53.722104</td>\n",
       "      <td>262.057644</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>6</td>\n",
       "      <td>46.855631</td>\n",
       "      <td>-124.795545</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>34</td>\n",
       "      <td>1965245</td>\n",
       "      <td>-1.060968</td>\n",
       "      <td>FN07A</td>\n",
       "      <td>P</td>\n",
       "      <td>1.318804e+09</td>\n",
       "      <td>2011-10-16 22:22:10.456747+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>65</td>\n",
       "      <td>2011-10-22 21:20:51.997895+00:00</td>\n",
       "      <td>53.722104</td>\n",
       "      <td>259.664403</td>\n",
       "      <td>8.984375</td>\n",
       "      <td>6</td>\n",
       "      <td>46.834105</td>\n",
       "      <td>-124.795826</td>\n",
       "      <td>8.984375</td>\n",
       "      <td>65</td>\n",
       "      <td>1885860</td>\n",
       "      <td>0.724684</td>\n",
       "      <td>FN07A</td>\n",
       "      <td>P</td>\n",
       "      <td>1.319318e+09</td>\n",
       "      <td>2011-10-22 21:20:51.997895+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>84</td>\n",
       "      <td>2011-10-26 15:45:12.545541+00:00</td>\n",
       "      <td>55.414139</td>\n",
       "      <td>257.271162</td>\n",
       "      <td>12.890625</td>\n",
       "      <td>6</td>\n",
       "      <td>46.812439</td>\n",
       "      <td>-124.773939</td>\n",
       "      <td>12.890625</td>\n",
       "      <td>84</td>\n",
       "      <td>2111346</td>\n",
       "      <td>1.229328</td>\n",
       "      <td>FN07A</td>\n",
       "      <td>P</td>\n",
       "      <td>1.319644e+09</td>\n",
       "      <td>2011-10-26 15:45:12.545541+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>98</td>\n",
       "      <td>2011-10-30 16:30:50.644314+00:00</td>\n",
       "      <td>54.568121</td>\n",
       "      <td>276.417089</td>\n",
       "      <td>7.421875</td>\n",
       "      <td>6</td>\n",
       "      <td>46.984720</td>\n",
       "      <td>-124.782730</td>\n",
       "      <td>7.421875</td>\n",
       "      <td>98</td>\n",
       "      <td>1988348</td>\n",
       "      <td>1.249747</td>\n",
       "      <td>FN07A</td>\n",
       "      <td>P</td>\n",
       "      <td>1.319992e+09</td>\n",
       "      <td>2011-10-30 16:30:50.644314+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16139</th>\n",
       "      <td>16139</td>\n",
       "      <td>100086</td>\n",
       "      <td>62680</td>\n",
       "      <td>2014-06-27 07:59:17.730075+00:00</td>\n",
       "      <td>41.031843</td>\n",
       "      <td>308.725840</td>\n",
       "      <td>13.671875</td>\n",
       "      <td>6</td>\n",
       "      <td>47.276299</td>\n",
       "      <td>-124.957702</td>\n",
       "      <td>13.671875</td>\n",
       "      <td>62680</td>\n",
       "      <td>1835238</td>\n",
       "      <td>1.220912</td>\n",
       "      <td>FN09C</td>\n",
       "      <td>P</td>\n",
       "      <td>1.403856e+09</td>\n",
       "      <td>2014-06-27 07:59:17.730075+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16140</th>\n",
       "      <td>16140</td>\n",
       "      <td>100092</td>\n",
       "      <td>62681</td>\n",
       "      <td>2014-06-27 08:00:36.709904+00:00</td>\n",
       "      <td>51.184052</td>\n",
       "      <td>264.450885</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>6</td>\n",
       "      <td>46.877358</td>\n",
       "      <td>-124.828555</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>62681</td>\n",
       "      <td>3738781</td>\n",
       "      <td>0.753338</td>\n",
       "      <td>FN03C</td>\n",
       "      <td>S</td>\n",
       "      <td>1.403856e+09</td>\n",
       "      <td>2014-06-27 08:00:36.709904+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16142</th>\n",
       "      <td>16142</td>\n",
       "      <td>100104</td>\n",
       "      <td>62692</td>\n",
       "      <td>2014-06-27 09:52:36.180245+00:00</td>\n",
       "      <td>47.799982</td>\n",
       "      <td>256.074542</td>\n",
       "      <td>7.421875</td>\n",
       "      <td>6</td>\n",
       "      <td>46.802265</td>\n",
       "      <td>-124.873822</td>\n",
       "      <td>7.421875</td>\n",
       "      <td>62692</td>\n",
       "      <td>1710667</td>\n",
       "      <td>1.188087</td>\n",
       "      <td>FN03C</td>\n",
       "      <td>P</td>\n",
       "      <td>1.403863e+09</td>\n",
       "      <td>2014-06-27 09:52:36.180245+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16143</th>\n",
       "      <td>16143</td>\n",
       "      <td>100110</td>\n",
       "      <td>62698</td>\n",
       "      <td>2014-06-27 12:02:50.342653+00:00</td>\n",
       "      <td>24.957513</td>\n",
       "      <td>254.877921</td>\n",
       "      <td>10.546875</td>\n",
       "      <td>6</td>\n",
       "      <td>46.792747</td>\n",
       "      <td>-125.173115</td>\n",
       "      <td>10.546875</td>\n",
       "      <td>62698</td>\n",
       "      <td>1699516</td>\n",
       "      <td>0.718257</td>\n",
       "      <td>FN08C</td>\n",
       "      <td>P</td>\n",
       "      <td>1.403871e+09</td>\n",
       "      <td>2014-06-27 12:02:50.342653+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16148</th>\n",
       "      <td>16148</td>\n",
       "      <td>100140</td>\n",
       "      <td>62716</td>\n",
       "      <td>2014-06-27 17:17:17.915898+00:00</td>\n",
       "      <td>95.176956</td>\n",
       "      <td>264.450885</td>\n",
       "      <td>12.890625</td>\n",
       "      <td>6</td>\n",
       "      <td>46.872519</td>\n",
       "      <td>-124.251554</td>\n",
       "      <td>12.890625</td>\n",
       "      <td>62716</td>\n",
       "      <td>716310</td>\n",
       "      <td>1.359923</td>\n",
       "      <td>FN02C</td>\n",
       "      <td>P</td>\n",
       "      <td>1.403889e+09</td>\n",
       "      <td>2014-06-27 17:17:17.915898+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9556 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0    idx                              time  \\\n",
       "0                 0           0      2  2011-08-09 10:26:19.327686+00:00   \n",
       "1                 1          12     34  2011-10-16 22:22:10.456747+00:00   \n",
       "3                 3          32     65  2011-10-22 21:20:51.997895+00:00   \n",
       "4                 4          38     84  2011-10-26 15:45:12.545541+00:00   \n",
       "5                 5          44     98  2011-10-30 16:30:50.644314+00:00   \n",
       "...             ...         ...    ...                               ...   \n",
       "16139         16139      100086  62680  2014-06-27 07:59:17.730075+00:00   \n",
       "16140         16140      100092  62681  2014-06-27 08:00:36.709904+00:00   \n",
       "16142         16142      100104  62692  2014-06-27 09:52:36.180245+00:00   \n",
       "16143         16143      100110  62698  2014-06-27 12:02:50.342653+00:00   \n",
       "16148         16148      100140  62716  2014-06-27 17:17:17.915898+00:00   \n",
       "\n",
       "               x           y          z  picks   latitude   longitude  \\\n",
       "0      88.408817  254.877921   0.390625      6  46.787357 -124.342163   \n",
       "1      53.722104  262.057644   0.390625      6  46.855631 -124.795545   \n",
       "3      53.722104  259.664403   8.984375      6  46.834105 -124.795826   \n",
       "4      55.414139  257.271162  12.890625      6  46.812439 -124.773939   \n",
       "5      54.568121  276.417089   7.421875      6  46.984720 -124.782730   \n",
       "...          ...         ...        ...    ...        ...         ...   \n",
       "16139  41.031843  308.725840  13.671875      6  47.276299 -124.957702   \n",
       "16140  51.184052  264.450885   0.390625      6  46.877358 -124.828555   \n",
       "16142  47.799982  256.074542   7.421875      6  46.802265 -124.873822   \n",
       "16143  24.957513  254.877921  10.546875      6  46.792747 -125.173115   \n",
       "16148  95.176956  264.450885  12.890625      6  46.872519 -124.251554   \n",
       "\n",
       "           depth  event_idx  pick_idx  residual station phase     time_pick  \\\n",
       "0       0.390625          2   2109680 -0.551091   FN07A     P  1.312886e+09   \n",
       "1       0.390625         34   1965245 -1.060968   FN07A     P  1.318804e+09   \n",
       "3       8.984375         65   1885860  0.724684   FN07A     P  1.319318e+09   \n",
       "4      12.890625         84   2111346  1.229328   FN07A     P  1.319644e+09   \n",
       "5       7.421875         98   1988348  1.249747   FN07A     P  1.319992e+09   \n",
       "...          ...        ...       ...       ...     ...   ...           ...   \n",
       "16139  13.671875      62680   1835238  1.220912   FN09C     P  1.403856e+09   \n",
       "16140   0.390625      62681   3738781  0.753338   FN03C     S  1.403856e+09   \n",
       "16142   7.421875      62692   1710667  1.188087   FN03C     P  1.403863e+09   \n",
       "16143  10.546875      62698   1699516  0.718257   FN08C     P  1.403871e+09   \n",
       "16148  12.890625      62716    716310  1.359923   FN02C     P  1.403889e+09   \n",
       "\n",
       "                               datetime  \n",
       "0      2011-08-09 10:26:19.327686+00:00  \n",
       "1      2011-10-16 22:22:10.456747+00:00  \n",
       "3      2011-10-22 21:20:51.997895+00:00  \n",
       "4      2011-10-26 15:45:12.545541+00:00  \n",
       "5      2011-10-30 16:30:50.644314+00:00  \n",
       "...                                 ...  \n",
       "16139  2014-06-27 07:59:17.730075+00:00  \n",
       "16140  2014-06-27 08:00:36.709904+00:00  \n",
       "16142  2014-06-27 09:52:36.180245+00:00  \n",
       "16143  2014-06-27 12:02:50.342653+00:00  \n",
       "16148  2014-06-27 17:17:17.915898+00:00  \n",
       "\n",
       "[9556 rows x 18 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d11af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i, idx in enumerate(range(0,10,1)):\n",
    "    print(i,idx,count)\n",
    "    count+=1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d60411",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aaac94",
   "metadata": {},
   "source": [
    "## Plot using the plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84953e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_picks = pd.read_csv('../data/datasets_2014/all_picks_2014_for_assoc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_picks_networks = all_picks['station_network_code'].drop_duplicates()\n",
    "list_networks = list(all_picks_networks)\n",
    "all_picks_networks = ','.join(all_picks_networks)\n",
    "all_picks_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the memory\n",
    "del all_picks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3946e",
   "metadata": {},
   "source": [
    "## Plot the offshore events north of 44&deg; N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb5ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events = pd.read_csv('../data/datasets_2014/new_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7496a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events_44N_and_above= new_events.loc[(new_events['latitude']>44)&(new_events['longitude']<-124)]\n",
    "new_events_44N_and_above = new_events_44N_and_above[0:50] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for subplots_cluster_scale\n",
    "mycatalog= mycatalog\n",
    "mycatalog_picks=mycatalog_picks\n",
    "networks= all_picks_networks\n",
    "channel= \"?H?\"\n",
    "idx_sta= 50\n",
    "title= \"Events matched\"\n",
    "fig_title= \"new_events_filtered_44N_and_above_plots.pdf\"\n",
    "\n",
    "subplots_cluster_scale(new_events_44N_and_above,mycatalog_picks,networks,channel,idx_sta,title,fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a80d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634dd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78356615",
   "metadata": {},
   "source": [
    "## Plot one of the cluster events at around 46.5&deg; N, 125&deg; W using the plotting functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplots_cluster_scale_p(idx, mycatalog, mycatalog_picks, networks, channel, idx_sta, title, fig_title):\n",
    "    \"\"\"\n",
    "    idx: event_idx\n",
    "    mycatalog: dataframe that contains only the unique picks (i.e., mycatalog_picks.drop_duplicates(subset=['idx']).copy())\n",
    "    mycatalog_picks: all pick assignments csv file (e.g., pd.read_csv('../data/datasets_OR/all_pick_assignments_OR.csv'))\n",
    "    networks: string of networks (e.g., \"NV,OO,7A\")\n",
    "    channel: specify the direction of the channel (i.e., \"?HZ\", \"?HE\" or \"?HN\")\n",
    "    idx_sta: choose the station to which you want to show the waveforms\n",
    "    title: title in a string\n",
    "    fig_title: figure title in as string\n",
    "    \"\"\"\n",
    "        \n",
    "    # Define the clients \n",
    "    client_waveform = WaveformClient()\n",
    "    client2 = Client(\"IRIS\")\n",
    "    client_ncedc = Client('NCEDC')\n",
    "\n",
    "\n",
    "    # Plot the earthquake moveout for one of the unmatched events for all stations \n",
    "    event = mycatalog\n",
    "    picks = mycatalog_picks\n",
    "    picks_idx = picks.loc[picks['idx']==idx]\n",
    "    pick_sta = np.unique(picks_idx['station'])\n",
    "    \n",
    "    otime = UTCDateTime(str(event[event['idx'] == idx][\"datetime\"].values[0]))\n",
    "    distances = []\n",
    "    max_dist = 10\n",
    "    min_dist = 0\n",
    "    \n",
    "    print(event[event['idx'] == idx]['picks'].values[0])\n",
    "    for station in pick_sta:\n",
    "        \n",
    "        \n",
    "        sta_inv = client2.get_stations(network=networks,\n",
    "                                       station=station, channel=\"?H?\", \n",
    "                                       starttime=otime - 1e8, endtime=otime + 1e8,level=\"response\")\n",
    "        if len(sta_inv) == 0:\n",
    "#             print(f\"Failed to fetch for {networks} {station} {otime}\")\n",
    "            continue\n",
    "            \n",
    "        _network = sta_inv[0].code\n",
    "        slat = sta_inv[0][0].latitude\n",
    "        slon = sta_inv[0][0].longitude\n",
    "        olat = event.loc[event['idx']==idx, 'latitude'].values[0]\n",
    "        olon = event.loc[event['idx']==idx, 'longitude'].values[0]\n",
    "        \n",
    "        dis1 = locations2degrees(olat, olon, slat, slon)\n",
    "        dist = degrees2kilometers(dis1)\n",
    "#         if max_dist < dist:\n",
    "#             max_dist = dist\n",
    "            \n",
    "#         if min_dist > dist:\n",
    "#             min_dist = dist\n",
    "            \n",
    "        distances.append([None, _network, station, dist])\n",
    "\n",
    "    # Sort distances\n",
    "    distances = sorted(distances, key=lambda item: item[-1])\n",
    "    distances = distances[:idx_sta+1]\n",
    "    \n",
    "    # Set up to define the xlim and ylim\n",
    "    max_y = 0\n",
    "    min_y = 0\n",
    "    # This count is for the if statements. Only used to ensure that min_y_count \n",
    "    #is changed from 0 to either the first positive value of the distance of one of the stations from the event\n",
    "    min_y_count = 0 \n",
    "    \n",
    "    max_x = 0\n",
    "    min_x = 0\n",
    "    \n",
    "    # This count is for the if statements. Only used to ensure that min_x_count \n",
    "    #is changed from 0 to either the first positive value of P pick time or the first positive value of S pick time\n",
    "    min_x_count= 0\n",
    "    # Create a figure\n",
    "    fig,axs = plt.subplots(1,4,figsize=(18,6))\n",
    "    gs = fig.add_gridspec(3, hspace=0, figure=fig)\n",
    "#     axs = gs.subplots(sharex=True, sharey=True)\n",
    "    starttime = otime -30\n",
    "    endtime = otime + 120\n",
    "    \n",
    "    # Define texts\n",
    "    texts = []\n",
    "    \n",
    "    for i, ii in enumerate(distances):\n",
    "            \n",
    "        if ii[1] in ['NC', 'BK']:\n",
    "            # Query waveforms\n",
    "            st = client_ncedc.get_waveforms(network=ii[1], station=ii[2], location=\"*\", channel=channel,starttime=starttime, endtime=endtime)\n",
    "\n",
    "        elif ii[1] in networks: \n",
    "            st = client_waveform.get_waveforms(network=ii[1], station=ii[2], channel=channel,starttime=starttime, endtime=endtime)\n",
    "  \n",
    "        else: \n",
    "            st =  Stream()\n",
    "            print(f\"WARNING: No data for {ii[1]}.{ii[2]}.{channel} on {otime}.\")    \n",
    "            continue\n",
    "            \n",
    "#         print(f\"len(st):{len(st)}\")\n",
    "#         print(st)\n",
    "    \n",
    "        # Skip empty traces\n",
    "        if len(st) == 0:\n",
    "                continue\n",
    "                \n",
    "        sta_picks = picks_idx[picks_idx['station'] == ii[2]]\n",
    "        p_picks = sta_picks.loc[sta_picks['phase'] == 'P']\n",
    "        s_picks = sta_picks.loc[sta_picks['phase'] == 'S']\n",
    "#         print(len(p_picks),len(s_picks))\n",
    "        \n",
    "        # Define the xlim values\n",
    "        # Define the maximum x value\n",
    "        if len(s_picks) > 0:\n",
    "            if max_x < UTCDateTime(s_picks.iloc[0]['time_pick']) - starttime:\n",
    "                max_x = UTCDateTime(s_picks.iloc[0]['time_pick']+5) - starttime\n",
    "        elif len(p_picks) > 0:\n",
    "            if max_x < UTCDateTime(p_picks.iloc[0]['time_pick']) - starttime: \n",
    "                max_x = UTCDateTime(p_picks.iloc[0]['time_pick']+5) - starttime\n",
    "        else:\n",
    "            print('No picks for this station. Skipping.')\n",
    "            continue \n",
    "            \n",
    "        # Define the minimum x value\n",
    "        if len(p_picks) > 0:\n",
    "            if min_x_count == 0:\n",
    "                if min_x < UTCDateTime(p_picks.iloc[0]['time_pick']) - starttime:\n",
    "                    min_x = UTCDateTime(p_picks.iloc[0]['time_pick']-5) - starttime\n",
    "                    min_x_count += 1           \n",
    "            else:\n",
    "                if min_x >= UTCDateTime(p_picks.iloc[0]['time_pick']) - starttime:\n",
    "                    min_x = UTCDateTime(p_picks.iloc[0]['time_pick']-5) - starttime            \n",
    "        elif len(s_picks) > 0:\n",
    "            if min_x_count == 0:\n",
    "                if min_x < UTCDateTime(s_picks.iloc[0]['time_pick'])- starttime:\n",
    "                    min_x = UTCDateTime(s_picks.iloc[0]['time_pick']-5)- starttime\n",
    "                    min_x_count += 1                \n",
    "            else:\n",
    "                if min_x >= UTCDateTime(s_picks.iloc[0]['time_pick'])- starttime:\n",
    "                    min_x = UTCDateTime(s_picks.iloc[0]['time_pick']-5) - starttime\n",
    "        else:\n",
    "            print('No picks for this station. Skipping.')\n",
    "            continue    \n",
    "            \n",
    "        if len(p_picks) == 0:\n",
    "            continue\n",
    "            \n",
    "#         print('This is after the p_pick continue statement')\n",
    "    \n",
    "        # Define ylim values\n",
    "        if min_y_count == 0:\n",
    "            if min_y < ii[3]:\n",
    "                min_y = ii[3] - 5\n",
    "                min_y_count += 1           \n",
    "        else:\n",
    "            if min_y >= ii[3]:\n",
    "                min_y = ii[3] - 5 \n",
    "                \n",
    "        max_y = ii[3] + 5\n",
    "        \n",
    "    scaling_factor = (1/2)*(max_y-min_y)   \n",
    "        \n",
    "    for i, ii in enumerate(distances):\n",
    "            \n",
    "        if ii[1] in ['NC', 'BK']:\n",
    "            # Query waveforms\n",
    "            st = client_ncedc.get_waveforms(network=ii[1], station=ii[2], location=\"*\", channel=channel,starttime=starttime, endtime=endtime)\n",
    "\n",
    "        elif ii[1] in networks: \n",
    "            st = client_waveform.get_waveforms(network=ii[1], station=ii[2], channel=channel,starttime=starttime, endtime=endtime)\n",
    "  \n",
    "        else: \n",
    "            st =  Stream()\n",
    "            print(f\"WARNING: No data for {ii[1]}.{ii[2]}.{channel} on {otime}.\")    \n",
    "            continue\n",
    "            \n",
    "#         print(f\"len(st):{len(st)}\")\n",
    "#         print(st)\n",
    "    \n",
    "        # Skip empty traces\n",
    "        if len(st) == 0:\n",
    "                continue\n",
    "        _st = Stream()\n",
    "        # Check for HH and BH channels presence\n",
    "        has_HH = bool(st.select(channel=\"HH?\"))\n",
    "        has_BH = bool(st.select(channel=\"BH?\"))\n",
    "\n",
    "        # Apply selection logic based on channel presence\n",
    "        if has_HH and has_BH:\n",
    "            # If both HH and BH channels are present, select only HH\n",
    "            _st += st.select(channel=\"HH?\")\n",
    "        elif has_HH:\n",
    "            # If only HH channels are present\n",
    "            _st += st.select(channel=\"HH?\")\n",
    "        elif has_BH:\n",
    "            # If only BH channels are present\n",
    "            _st += st.select(channel=\"BH?\")\n",
    "\n",
    "        st = _st\n",
    "\n",
    "        print(f'Second st print:{_st}')\n",
    "              \n",
    "        st = Stream(filter(lambda st: st.stats.sampling_rate > 10, st))\n",
    "        st.taper(max_percentage=0.05)\n",
    "        st.filter(type='bandpass', freqmin=2, freqmax=25)\n",
    "        st.merge(fill_value='interpolate') # fill gaps if there are any.\n",
    "\n",
    "#         print(st)\n",
    "        # Select only one trace per channel\n",
    "        unique_channels = set(tr.stats.channel for tr in st)\n",
    "        selected_traces = []\n",
    "        \n",
    "        for ch in unique_channels:\n",
    "            selected_traces.append(next(tr for tr in st if tr.stats.channel == ch))\n",
    "        st = Stream(selected_traces)\n",
    "                   \n",
    "        trim_st = st.copy()\n",
    "        sta_picks = picks_idx[picks_idx['station'] == ii[2]]\n",
    "        p_picks = sta_picks.loc[sta_picks['phase'] == 'P']\n",
    "        s_picks = sta_picks.loc[sta_picks['phase'] == 'S']\n",
    "#         print(len(p_picks),len(s_picks))\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "        if len(p_picks) == 0:\n",
    "            continue \n",
    "#         print('This is after the p_pick continue statement')\n",
    "        print(trim_st)\n",
    "        \n",
    "        for iax in range(len(trim_st)):\n",
    "            print(iax)\n",
    "            sampling_rate = trim_st[iax].stats.sampling_rate\n",
    "            trim_st = trim_st.normalize()\n",
    "            \n",
    "            tp = UTCDateTime(p_picks.iloc[0]['time_pick']) - otime + 30\n",
    "            i1 = int((tp-5)*sampling_rate)\n",
    "            i2 = int((tp+15)*sampling_rate)\n",
    "\n",
    "            offsets1 = ii[3]\n",
    "            try: \n",
    "                wave = trim_st[iax].data\n",
    "                wave = wave / (np.nanmax(wave[i1:i2], axis=-1)*10)\n",
    "            except:\n",
    "                continue \n",
    "            \n",
    "#             print(trim_st[iax].stats.sampling_rate)\n",
    "            axs[iax].plot(trim_st[iax].times(), wave * scaling_factor + offsets1, \n",
    "                          color='black', label=f\"{trim_st[iax].stats.channel}\", alpha=0.7, lw=0.5)\n",
    "#             axs[iax].plot(trim_st[iax].times(), wave * 30 + offsets1, color='black', label=f\"{trim_st[iax].stats.channel}\", alpha=0.7, lw=0.5)\n",
    "\n",
    "#             axs[iax].text(xlim[-1] + 2,   offsets1, \n",
    "#                               [ii[2]], fontsize=8, verticalalignment='bottom')\n",
    "\n",
    "            if len(p_picks) > 0:\n",
    "                axs[iax].vlines(UTCDateTime(p_picks.iloc[0]['time_pick']) - otime + 30, offsets1 - (1/35) * scaling_factor, \n",
    "                                offsets1 + (1/35) * scaling_factor, color='r')\n",
    "            if len(s_picks) > 0:\n",
    "                axs[iax].vlines(UTCDateTime(s_picks.iloc[0]['time_pick']) - otime + 30, offsets1 - (1/35) * scaling_factor, \n",
    "                                offsets1 + (1/35) * scaling_factor, color='b')\n",
    "        texts.append([ii[2],ii[3]])\n",
    "\n",
    "    \n",
    "#     print(max_y,min_y)\n",
    "    chs = ['2','1','Z']\n",
    "    for iax in range(3):\n",
    "        for i, ii in enumerate(texts):\n",
    "            offsets1 = ii[1]\n",
    "            axs[iax].text(max_x + 0.5, offsets1, \n",
    "                                  [ii[0]], fontsize=8, verticalalignment='bottom')\n",
    "        axs[iax].legend(chs[iax],loc='upper right', handlelength=0)\n",
    "        axs[iax].set_ylim([min_y,max_y])\n",
    "        axs[iax].set_xlim([min_x,max_x])\n",
    "        axs[iax].grid(alpha=0.5)\n",
    "    fig.supxlabel('Time [sec]', y=0.07)\n",
    "    fig.supylabel('Distance [km]')\n",
    "    fig.suptitle(f\"{title}: Origin Time={otime}, \\n Latitude={round(event[event['idx']==idx]['latitude'].values[0], 2)}, Longtitude={round(event[event['idx']==idx]['longitude'].values[0], 2)}, Depth={round(event[event['idx']==idx]['depth'].values[0], 2)}\", y=1)\n",
    "    \n",
    "    m = Basemap(projection='merc', llcrnrlat=40, urcrnrlat=50, llcrnrlon=-130, urcrnrlon=-120, resolution='i', ax=axs[3])\n",
    "    m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "    m.drawstates()\n",
    "    m.drawmapboundary()\n",
    "    m.drawparallels(np.arange(40, 51, 1), labels=[1,0,0,0])\n",
    "    m.drawmeridians(np.arange(-130, -119, 1), labels=[0,0,0,1])\n",
    "    x, y = m(event[event['idx']==idx]['longitude'].values[0], event[event['idx']==idx]['latitude'].values[0])\n",
    "    m.plot(x, y, 'ro', markersize=9)\n",
    "    axs[3].set_title('Event Location')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba14668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplots_cluster_scale(idx, mycatalog, mycatalog_picks, networks, channel, idx_sta, title, fig_title):\n",
    "    \"\"\"\n",
    "    idx: event_idx\n",
    "    mycatalog: dataframe that contains only the unique picks (i.e., mycatalog_picks.drop_duplicates(subset=['idx']).copy())\n",
    "    mycatalog_picks: all pick assignments csv file (e.g., pd.read_csv('../data/datasets_OR/all_pick_assignments_OR.csv'))\n",
    "    networks: string of networks (e.g., \"NV,OO,7A\")\n",
    "    channel: specify the direction of the channel (i.e., \"?HZ\", \"?HE\" or \"?HN\")\n",
    "    idx_sta: choose the station to which you want to show the waveforms\n",
    "    title: title in a string\n",
    "    fig_title: figure title in as string\n",
    "    \"\"\"\n",
    "        \n",
    "    # Define the clients \n",
    "    client_waveform = WaveformClient()\n",
    "    client2 = Client(\"IRIS\")\n",
    "    client_ncedc = Client('NCEDC')\n",
    "\n",
    "\n",
    "    # Plot the earthquake moveout for one of the unmatched events for all stations \n",
    "    event = mycatalog\n",
    "    picks = mycatalog_picks\n",
    "    \n",
    "    p = PdfPages(fig_title) \n",
    "    \n",
    "    for idx in event['idx']:\n",
    "        \n",
    "        picks_idx = picks.loc[picks['idx']==idx]\n",
    "        pick_sta = np.unique(picks_idx['station'])\n",
    "\n",
    "        otime = UTCDateTime(str(event[event['idx'] == idx][\"datetime\"].values[0]))\n",
    "        distances = []\n",
    "        max_dist = 10\n",
    "        min_dist = 0\n",
    "\n",
    "        print(event[event['idx'] == idx]['picks'].values[0])\n",
    "        for station in pick_sta:\n",
    "\n",
    "\n",
    "            sta_inv = client2.get_stations(network=networks,\n",
    "                                           station=station, channel=\"?H?\", \n",
    "                                           starttime=otime - 1e8, endtime=otime + 1e8,level=\"response\")\n",
    "            if len(sta_inv) == 0:\n",
    "    #             print(f\"Failed to fetch for {networks} {station} {otime}\")\n",
    "                continue\n",
    "\n",
    "            _network = sta_inv[0].code\n",
    "            slat = sta_inv[0][0].latitude\n",
    "            slon = sta_inv[0][0].longitude\n",
    "            olat = event.loc[event['idx']==idx, 'latitude'].values[0]\n",
    "            olon = event.loc[event['idx']==idx, 'longitude'].values[0]\n",
    "\n",
    "            dis1 = locations2degrees(olat, olon, slat, slon)\n",
    "            dist = degrees2kilometers(dis1)\n",
    "    #         if max_dist < dist:\n",
    "    #             max_dist = dist\n",
    "\n",
    "    #         if min_dist > dist:\n",
    "    #             min_dist = dist\n",
    "\n",
    "            distances.append([None, _network, station, dist])\n",
    "\n",
    "        # Sort distances\n",
    "        distances = sorted(distances, key=lambda item: item[-1])\n",
    "        distances = distances[:idx_sta+1]\n",
    "\n",
    "        # Set up to define the xlim and ylim\n",
    "        max_y = 0\n",
    "        min_y = 0\n",
    "        # This count is for the if statements. Only used to ensure that min_y_count \n",
    "        #is changed from 0 to either the first positive value of the distance of one of the stations from the event\n",
    "        min_y_count = 0 \n",
    "\n",
    "        max_x = 0\n",
    "        min_x = 0\n",
    "\n",
    "        # This count is for the if statements. Only used to ensure that min_x_count \n",
    "        #is changed from 0 to either the first positive value of P pick time or the first positive value of S pick time\n",
    "        min_x_count= 0\n",
    "        # Create a figure\n",
    "        fig,axs = plt.subplots(1,4,figsize=(18,6))\n",
    "        gs = fig.add_gridspec(3, hspace=0, figure=fig)\n",
    "    #     axs = gs.subplots(sharex=True, sharey=True)\n",
    "        starttime = otime -30\n",
    "        endtime = otime + 120\n",
    "\n",
    "        # Define texts\n",
    "        texts = []\n",
    "\n",
    "        for i, ii in enumerate(distances):\n",
    "\n",
    "            if ii[1] in ['NC', 'BK']:\n",
    "                # Query waveforms\n",
    "                st = client_ncedc.get_waveforms(network=ii[1], station=ii[2], location=\"*\", channel=channel,starttime=starttime, endtime=endtime)\n",
    "\n",
    "            elif ii[1] in networks: \n",
    "                st = client_waveform.get_waveforms(network=ii[1], station=ii[2], channel=channel,starttime=starttime, endtime=endtime)\n",
    "\n",
    "            else: \n",
    "                st =  Stream()\n",
    "                print(f\"WARNING: No data for {ii[1]}.{ii[2]}.{channel} on {otime}.\")    \n",
    "                continue\n",
    "\n",
    "    #         print(f\"len(st):{len(st)}\")\n",
    "    #         print(st)\n",
    "\n",
    "            # Skip empty traces\n",
    "            if len(st) == 0:\n",
    "                    continue\n",
    "\n",
    "            sta_picks = picks_idx[picks_idx['station'] == ii[2]]\n",
    "            p_picks = sta_picks.loc[sta_picks['phase'] == 'P']\n",
    "            s_picks = sta_picks.loc[sta_picks['phase'] == 'S']\n",
    "    #         print(len(p_picks),len(s_picks))\n",
    "\n",
    "            # Define the xlim values\n",
    "            # Define the maximum x value\n",
    "            if len(s_picks) > 0:\n",
    "                if max_x < UTCDateTime(s_picks.iloc[0]['time_pick']) - starttime:\n",
    "                    max_x = UTCDateTime(s_picks.iloc[0]['time_pick']+5) - starttime\n",
    "            elif len(p_picks) > 0:\n",
    "                if max_x < UTCDateTime(p_picks.iloc[0]['time_pick']) - starttime: \n",
    "                    max_x = UTCDateTime(p_picks.iloc[0]['time_pick']+5) - starttime\n",
    "            else:\n",
    "                print('No picks for this station. Skipping.')\n",
    "                continue \n",
    "\n",
    "            # Define the minimum x value\n",
    "            if len(p_picks) > 0:\n",
    "                if min_x_count == 0:\n",
    "                    if min_x < UTCDateTime(p_picks.iloc[0]['time_pick']) - starttime:\n",
    "                        min_x = UTCDateTime(p_picks.iloc[0]['time_pick']-5) - starttime\n",
    "                        min_x_count += 1           \n",
    "                else:\n",
    "                    if min_x >= UTCDateTime(p_picks.iloc[0]['time_pick']) - starttime:\n",
    "                        min_x = UTCDateTime(p_picks.iloc[0]['time_pick']-5) - starttime            \n",
    "            elif len(s_picks) > 0:\n",
    "                if min_x_count == 0:\n",
    "                    if min_x < UTCDateTime(s_picks.iloc[0]['time_pick'])- starttime:\n",
    "                        min_x = UTCDateTime(s_picks.iloc[0]['time_pick']-5)- starttime\n",
    "                        min_x_count += 1                \n",
    "                else:\n",
    "                    if min_x >= UTCDateTime(s_picks.iloc[0]['time_pick'])- starttime:\n",
    "                        min_x = UTCDateTime(s_picks.iloc[0]['time_pick']-5) - starttime\n",
    "            else:\n",
    "                print('No picks for this station. Skipping.')\n",
    "                continue    \n",
    "\n",
    "    #         if len(p_picks) == 0:\n",
    "    #             continue\n",
    "\n",
    "    #         print('This is after the p_pick continue statement')\n",
    "\n",
    "            # Define ylim values\n",
    "            if min_y_count == 0:\n",
    "                if min_y < ii[3]:\n",
    "                    min_y = ii[3] - 5\n",
    "                    min_y_count += 1           \n",
    "            else:\n",
    "                if min_y >= ii[3]:\n",
    "                    min_y = ii[3] - 5 \n",
    "\n",
    "            max_y = ii[3] + 5\n",
    "\n",
    "        scaling_factor = (1/2)*(max_y-min_y)   \n",
    "\n",
    "        for i, ii in enumerate(distances):\n",
    "\n",
    "            if ii[1] in ['NC', 'BK']:\n",
    "                # Query waveforms\n",
    "                st = client_ncedc.get_waveforms(network=ii[1], station=ii[2], location=\"*\", channel=channel,starttime=starttime, endtime=endtime)\n",
    "\n",
    "            elif ii[1] in networks: \n",
    "                st = client_waveform.get_waveforms(network=ii[1], station=ii[2], channel=channel,starttime=starttime, endtime=endtime)\n",
    "\n",
    "            else: \n",
    "                st =  Stream()\n",
    "                print(f\"WARNING: No data for {ii[1]}.{ii[2]}.{channel} on {otime}.\")    \n",
    "                continue\n",
    "\n",
    "    #         print(f\"len(st):{len(st)}\")\n",
    "    #         print(st)\n",
    "\n",
    "            # Skip empty traces\n",
    "            if len(st) == 0:\n",
    "                    continue\n",
    "            _st = Stream()\n",
    "            # Check for HH and BH channels presence\n",
    "            has_HH = bool(st.select(channel=\"HH?\"))\n",
    "            has_BH = bool(st.select(channel=\"BH?\"))\n",
    "\n",
    "            # Apply selection logic based on channel presence\n",
    "            if has_HH and has_BH:\n",
    "                # If both HH and BH channels are present, select only HH\n",
    "                _st += st.select(channel=\"HH?\")\n",
    "            elif has_HH:\n",
    "                # If only HH channels are present\n",
    "                _st += st.select(channel=\"HH?\")\n",
    "            elif has_BH:\n",
    "                # If only BH channels are present\n",
    "                _st += st.select(channel=\"BH?\")\n",
    "\n",
    "            st = _st\n",
    "\n",
    "            print(f'Second st print:{_st}')\n",
    "\n",
    "            st = Stream(filter(lambda st: st.stats.sampling_rate > 10, st))\n",
    "            st.taper(max_percentage=0.05)\n",
    "            st.filter(type='bandpass', freqmin=2, freqmax=25)\n",
    "            st.merge(fill_value='interpolate') # fill gaps if there are any.\n",
    "\n",
    "    #         print(st)\n",
    "            # Select only one trace per channel\n",
    "            unique_channels = set(tr.stats.channel for tr in st)\n",
    "            selected_traces = []\n",
    "\n",
    "            for ch in unique_channels:\n",
    "                selected_traces.append(next(tr for tr in st if tr.stats.channel == ch))\n",
    "            st = Stream(selected_traces)\n",
    "\n",
    "            trim_st = st.copy()\n",
    "            sta_picks = picks_idx[picks_idx['station'] == ii[2]]\n",
    "            p_picks = sta_picks.loc[sta_picks['phase'] == 'P']\n",
    "            s_picks = sta_picks.loc[sta_picks['phase'] == 'S']\n",
    "    #         print(len(p_picks),len(s_picks))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #         if len(p_picks) == 0:\n",
    "    #             continue \n",
    "    #         print('This is after the p_pick continue statement')\n",
    "            print(trim_st)\n",
    "\n",
    "            for iax in range(len(trim_st)):\n",
    "                print(iax)\n",
    "                sampling_rate = trim_st[iax].stats.sampling_rate\n",
    "                trim_st = trim_st.normalize()\n",
    "\n",
    "                if len(p_picks)>0:\n",
    "                    tp = UTCDateTime(p_picks.iloc[0]['time_pick']) - otime + 30\n",
    "                    i1 = int((tp-5)*sampling_rate)\n",
    "                    i2 = int((tp+15)*sampling_rate)\n",
    "                elif len(s_picks)>0:\n",
    "                    ts = UTCDateTime(s_picks.iloc[0]['time_pick']) - otime + 30\n",
    "                    i1 = int((ts-10)*sampling_rate)\n",
    "                    i2 = int((ts+10)*sampling_rate)\n",
    "                else:\n",
    "                    print(f\"WARNING: No pick time for {ii[1]}.{ii[2]}.{channel} on {otime}.\")\n",
    "\n",
    "                offsets1 = ii[3]\n",
    "                try: \n",
    "                    wave = trim_st[iax].data\n",
    "                    wave = wave / (np.nanmax(wave[i1:i2], axis=-1)*10)\n",
    "                except:\n",
    "                    continue \n",
    "\n",
    "    #             print(trim_st[iax].stats.sampling_rate)\n",
    "                axs[iax].plot(trim_st[iax].times(), wave * scaling_factor + offsets1, \n",
    "                              color='black', label=f\"{trim_st[iax].stats.channel}\", alpha=0.7, lw=0.5)\n",
    "    #             axs[iax].plot(trim_st[iax].times(), wave * 30 + offsets1, color='black', label=f\"{trim_st[iax].stats.channel}\", alpha=0.7, lw=0.5)\n",
    "\n",
    "    #             axs[iax].text(xlim[-1] + 2,   offsets1, \n",
    "    #                               [ii[2]], fontsize=8, verticalalignment='bottom')\n",
    "\n",
    "                if len(p_picks) > 0:\n",
    "                    axs[iax].vlines(UTCDateTime(p_picks.iloc[0]['time_pick']) - otime + 30, offsets1 - (1/35) * scaling_factor, \n",
    "                                    offsets1 + (1/35) * scaling_factor, color='r')\n",
    "                if len(s_picks) > 0:\n",
    "                    axs[iax].vlines(UTCDateTime(s_picks.iloc[0]['time_pick']) - otime + 30, offsets1 - (1/35) * scaling_factor, \n",
    "                                    offsets1 + (1/35) * scaling_factor, color='b')\n",
    "            texts.append([ii[2],ii[3]])\n",
    "\n",
    "\n",
    "    #     print(max_y,min_y)\n",
    "        chs = ['2','1','Z']\n",
    "        for iax in range(3):\n",
    "            for i, ii in enumerate(texts):\n",
    "                offsets1 = ii[1]\n",
    "                axs[iax].text(max_x + 0.5, offsets1, \n",
    "                                      [ii[0]], fontsize=8, verticalalignment='bottom')\n",
    "            axs[iax].legend(chs[iax],loc='upper right', handlelength=0)\n",
    "            axs[iax].set_ylim([min_y,max_y])\n",
    "            axs[iax].set_xlim([min_x,max_x])\n",
    "            axs[iax].grid(alpha=0.5)\n",
    "        fig.supxlabel('Time [sec]', y=0.07)\n",
    "        fig.supylabel('Distance [km]')\n",
    "        fig.suptitle(f\"{title}: Origin Time={otime}, \\n Latitude={round(event[event['idx']==idx]['latitude'].values[0], 2)}, Longtitude={round(event[event['idx']==idx]['longitude'].values[0], 2)}, Depth={round(event[event['idx']==idx]['depth'].values[0], 2)}\", y=1)\n",
    "\n",
    "        m = Basemap(projection='merc', llcrnrlat=40, urcrnrlat=50, llcrnrlon=-130, urcrnrlon=-120, resolution='i', ax=axs[3])\n",
    "        m.drawcoastlines()\n",
    "        m.drawcountries()\n",
    "        m.drawstates()\n",
    "        m.drawmapboundary()\n",
    "        m.drawparallels(np.arange(40, 51, 1), labels=[1,0,0,0])\n",
    "        m.drawmeridians(np.arange(-130, -119, 1), labels=[0,0,0,1])\n",
    "        x, y = m(event[event['idx']==idx]['longitude'].values[0], event[event['idx']==idx]['latitude'].values[0])\n",
    "        m.plot(x, y, 'ro', markersize=9)\n",
    "        axs[3].set_title('Event Location')\n",
    "        \n",
    "        fig.savefig(p, format='pdf')  \n",
    "\n",
    "    p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef03321",
   "metadata": {},
   "source": [
    "### Specify the latitudes and longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f79e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events = pd.read_csv('../data/datasets_2014/new_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d048ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = new_events[new_events['station']=='J25B']['idx'].values\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events = new_events[new_events[\"idx\"].isin(idx)]\n",
    "new_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac241e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events_specific_lat_lon = new_events.loc[(new_events['latitude']>46)&(new_events['latitude']<47.5)&(new_events['longitude']<-124)&(new_events['longitude']>-126)]\n",
    "new_events_specific_lat_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c610d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_events = new_events_specific_lat_lon.loc[(new_events_specific_lat_lon['depth']>1)&(new_events_specific_lat_lon['depth']<40)]\n",
    "new_events = new_events_specific_lat_lon.loc[new_events_specific_lat_lon['picks']>=6][0:30]\n",
    "new_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eda7e0",
   "metadata": {},
   "source": [
    "### Histogram: Depths of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92381c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the number of picks for the matched events\n",
    "plt.figure()\n",
    "bins = np.linspace(0,50,25)\n",
    "plt.hist(new_events['depth'],bins=bins)\n",
    "plt.xlabel('Depth (km)')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.title('Histogram of the depths of the events in the cluster at ~46.5$^\\circ$ N')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436cab64",
   "metadata": {},
   "source": [
    "### Plot HH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35b6a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "mycatalog= mycatalog\n",
    "mycatalog_picks=mycatalog_picks\n",
    "networks= all_picks_networks\n",
    "channel= \"HH?\"\n",
    "idx_sta= 50\n",
    "title= \"Events matched\"\n",
    "fig_title= \"events_matched.png\"\n",
    "ylim= [0,300]\n",
    "xlim= [30,45]\n",
    "\n",
    "for idx in new_events['idx']:\n",
    "    subplots_cluster_scale_p(idx,mycatalog,mycatalog_picks,networks,channel,idx_sta,title,fig_title,ylim,xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72ab1e",
   "metadata": {},
   "source": [
    "### Plot BH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369c148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "mycatalog= mycatalog\n",
    "mycatalog_picks=mycatalog_picks\n",
    "networks= all_picks_networks\n",
    "channel= \"BH?\"\n",
    "idx_sta= 50\n",
    "title= \"Events matched\"\n",
    "fig_title= \"events_matched.png\"\n",
    "ylim= [0,300]\n",
    "xlim= [30,45]\n",
    "\n",
    "for idx in new_events['idx']:\n",
    "    subplots_cluster_scale_p(idx,mycatalog,mycatalog_picks,networks,channel,idx_sta,title,fig_title,ylim,xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6211d",
   "metadata": {},
   "source": [
    "### Plot both HH? and BH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events['datetime'] = pd.to_datetime(new_events['datetime'], utc = True)\n",
    "new_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f15bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters for subplots_cluster_scale_p\n",
    "mycatalog= mycatalog\n",
    "mycatalog_picks=mycatalog_picks\n",
    "networks= all_picks_networks\n",
    "channel= \"?H?\"\n",
    "idx_sta= 50\n",
    "title= \"Events matched\"\n",
    "fig_title= \"events_matched.png\"\n",
    "\n",
    "for idx in new_events['idx']:\n",
    "    subplots_cluster_scale_p(idx,new_events,mycatalog_picks,networks,channel,idx_sta,title,fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae32807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters for subplots_cluster_scale\n",
    "mycatalog= mycatalog\n",
    "mycatalog_picks=mycatalog_picks\n",
    "networks= all_picks_networks\n",
    "channel= \"?H?\"\n",
    "idx_sta= 50\n",
    "title= \"Events matched\"\n",
    "fig_title= \"new_events_plots.pdf\"\n",
    "\n",
    "subplots_cluster_scale(new_events,mycatalog_picks,networks,channel,idx_sta,title,fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e815a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the number of picks for the matched events\n",
    "new_events['time'] = pd.to_datetime(new_events['time'])\n",
    "year1 = 2012\n",
    "time1 = datetime(year=year1, month=1, day=1)\n",
    "time2 = datetime(year=year1 + 1, month=1, day=1)\n",
    "time_bins = pd.date_range(start=time1, end=time2, freq='5D')\n",
    "plt.figure()\n",
    "plt.hist(new_events['time'], bins=time_bins)\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.title('Histogram for the frequency of the events in the cluster at ~46.5$^\\circ$ N')\n",
    "plt.xticks(rotation=45)  # Optional: Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02952d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt \n",
    "from matplotlib.backends.backend_pdf import PdfPages \n",
    "\n",
    "# customizing runtime configuration stored \n",
    "# in matplotlib.rcParams \n",
    "plt.rcParams[\"figure.figsize\"] = [7.00, 3.50] \n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "fig1 = plt.figure() \n",
    "plt.plot([17, 45, 7, 8, 7], color='orange') \n",
    "\n",
    "fig2 = plt.figure() \n",
    "plt.plot([13, 25, 1, 6, 3], color='blue') \n",
    "\n",
    "Fig3 = plt.figure() \n",
    "plt.plot([22, 11, 2, 1, 23], color='green') \n",
    "\n",
    "\n",
    "def save_image(filename): \n",
    "\t\n",
    "\t# PdfPages is a wrapper around pdf \n",
    "\t# file so there is no clash and create \n",
    "\t# files with no error. \n",
    "\tp = PdfPages(filename) \n",
    "\t\n",
    "\t# get_fignums Return list of existing \n",
    "\t# figure numbers \n",
    "\tfig_nums = plt.get_fignums() \n",
    "\tfigs = [plt.figure(n) for n in fig_nums] \n",
    "\t\n",
    "\t# iterating over the numbers in list \n",
    "\tfor fig in figs: \n",
    "\t\t\n",
    "\t\t# and saving the files \n",
    "\t\tfig.savefig(p, format='pdf') \n",
    "\t\n",
    "\t# close the object \n",
    "\tp.close() \n",
    "\n",
    "# name your Pdf file \n",
    "filename = \"multi_plot_image.pdf\"\n",
    "\n",
    "# call the function \n",
    "save_image(filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84d3bd",
   "metadata": {},
   "source": [
    "## Plot the cluster events at around 43.25&deg; N, 124.25&deg; W using the plotting functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events_specific_lat_lon = unmatched_events_mycatalog2morton_and_anss.loc[(unmatched_events_mycatalog2morton_and_anss['latitude']>43.0)&(unmatched_events_mycatalog2morton_and_anss['latitude']<43.3)&(unmatched_events_mycatalog2morton_and_anss['longitude']<-124.25)&(unmatched_events_mycatalog2morton_and_anss['longitude']>-124.75)]\n",
    "new_events_specific_lat_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events = new_events_specific_lat_lon.loc[new_events_specific_lat_lon['picks']>5]\n",
    "new_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa50fe",
   "metadata": {},
   "source": [
    "### Plot HH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "mycatalog= mycatalog\n",
    "mycatalog_picks=mycatalog_picks\n",
    "networks= all_picks_networks\n",
    "channel= \"HH?\"\n",
    "idx_sta= 50\n",
    "title= \"Events matched\"\n",
    "fig_title= \"events_matched.png\"\n",
    "ylim= [0,300]\n",
    "xlim= [20,150]\n",
    "\n",
    "for idx in new_events['idx']:\n",
    "    subplots_cluster_scale_p(idx,mycatalog,mycatalog_picks,networks,channel,idx_sta,title,fig_title,ylim,xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0acaf0",
   "metadata": {},
   "source": [
    "### Plot BH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "mycatalog= mycatalog\n",
    "mycatalog_picks=mycatalog_picks\n",
    "networks= all_picks_networks\n",
    "channel= \"BH?\"\n",
    "idx_sta= 50\n",
    "title= \"Events matched\"\n",
    "fig_title= \"events_matched.png\"\n",
    "ylim= [0,300]\n",
    "xlim= [20,150]\n",
    "\n",
    "for idx in new_events['idx']:\n",
    "    subplots_cluster_scale_p(idx,mycatalog,mycatalog_picks,networks,channel,idx_sta,title,fig_title,ylim,xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863afcd8",
   "metadata": {},
   "source": [
    "## Calculate SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8944e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_picks = pd.read_csv('../data/datasets_2014/all_picks_2014.csv')\n",
    "all_pick_assignments = pd.read_csv('../data/datasets_2014/all_pick_assignments_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_snr(all_picks,all_pick_assignments):\n",
    "    \"\"\" \n",
    "    This function calculates the SNRs for each station in each event.\n",
    "    \n",
    "    Inputs:\n",
    "    1. The all_pick.csv file from the 2_format_pick2associate file.\n",
    "    2. The all_pick_assignments.csv file from the 3_association file.\n",
    "    \n",
    "    Oututs:\n",
    "    The new all_pick_assignments_snr file with the additional SNR column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the list of networks\n",
    "    all_picks_networks = all_picks['station_network_code'].drop_duplicates()\n",
    "    list_networks = list(all_picks_networks)\n",
    "    networks = ','.join(all_picks_networks)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define the clients \n",
    "    client_waveform = WaveformClient()\n",
    "    client2 = Client(\"IRIS\")\n",
    "    client_ncedc = Client('NCEDC')\n",
    "    \n",
    "    all_pick_assignments['datetime'] = pd.to_datetime(all_pick_assignments['time'], utc = True)\n",
    "\n",
    "    \n",
    "    # Define parameters\n",
    "    percentile=98\n",
    "    \n",
    "    # Create empty lists\n",
    "    snr_list = []\n",
    "        \n",
    "    # Make sure if a station for an event has more than 1 P or S pick\n",
    "    for idx in all_pick_assignments['idx'].drop_duplicates():\n",
    "        # Define parameters\n",
    "        otime = UTCDateTime(str(all_pick_assignments[all_pick_assignments['idx'] == idx][\"datetime\"].values[0]))\n",
    "\n",
    "\n",
    "        # Create empty lists\n",
    "        networks_stas = []\n",
    "\n",
    "        # Plot the earthquake moveout for one of the unmatched events for all stations \n",
    "    #     event = mycatalog\n",
    "        pick_idx = all_pick_assignments.loc[all_pick_assignments['idx']==idx]\n",
    "        pick_sta = np.unique(pick_idx['station'])\n",
    "        \n",
    "    #     distances = []\n",
    "    #     max_dist = 10\n",
    "    #     min_dist = 0\n",
    "        for station in pick_sta:\n",
    "\n",
    "            sta_inv = client2.get_stations(network=networks,\n",
    "                                           station=station, channel=\"?H?\", \n",
    "                                           starttime=otime - 1e8, endtime=otime + 1e8,level=\"response\")\n",
    "            if len(sta_inv) == 0:\n",
    "                print(f'No inventory for station {station}. Skipping.')\n",
    "                continue\n",
    "\n",
    "            network = sta_inv[0].code\n",
    "    #         slat = sta_inv[0][0].latitude\n",
    "    #         slon = sta_inv[0][0].longitude\n",
    "    #         olat = event.loc[event['idx']==idx, 'latitude'].values[0]\n",
    "    #         olon = event.loc[event['idx']==idx, 'longitude'].values[0]\n",
    "\n",
    "    #         dis1 = locations2degrees(olat, olon, slat, slon)\n",
    "    #         dist = degrees2kilometers(dis1)\n",
    "    # #         if max_dist < dist:\n",
    "    # #             max_dist = dist\n",
    "\n",
    "    # #         if min_dist > dist:\n",
    "    # #             min_dist = dist\n",
    "\n",
    "            networks_stas.append([network,station])\n",
    "        \n",
    "   \n",
    "        events = all_pick_assignments[all_pick_assignments['idx']==idx]\n",
    "        for i in networks_stas:\n",
    "            events1 = events[events['station']==i[1]]\n",
    "            p_picks = events1[events1['phase']=='P']\n",
    "            s_picks = events1[events1['phase']=='S']\n",
    "            \n",
    "\n",
    "            if len(p_picks)>0 and len(s_picks)>0:\n",
    "                print(p_picks['datetime'].values,s_picks['datetime'].values)\n",
    "                p_pick_time = UTCDateTime(str(p_picks['datetime'].values[0]))\n",
    "                s_pick_time = UTCDateTime(str(s_picks['datetime'].values[0]))\n",
    "                \n",
    "                starttime_st = UTCDateTime(p_pick_time)-timedelta(seconds=120)\n",
    "                endtime_st = UTCDateTime(p_pick_time)+timedelta(seconds=120)\n",
    "                \n",
    "                starttime_noise = UTCDateTime(p_pick_time)-timedelta(seconds=8)\n",
    "                endtime_noise = UTCDateTime(p_pick_time)\n",
    "                \n",
    "                starttime_signal = UTCDateTime(s_pick_time)-timedelta(seconds=1)\n",
    "                endtime_signal = UTCDateTime(s_pick_time)+timedelta(seconds=2)\n",
    "                \n",
    "                print(starttime_noise,endtime_noise)\n",
    "            \n",
    "                if i[0] in ['NC', 'BK']:\n",
    "                # Query waveforms\n",
    "                    st = client_ncedc.get_waveforms(network=i[0], station=i[1],\n",
    "                                                    location=\"*\", channel=\"?HZ\",starttime=starttime_st,\n",
    "                                                    endtime=endtime_st)\n",
    "\n",
    "                elif i[0] in networks: \n",
    "                    st = client_waveform.get_waveforms(network=i[0], station=i[1],\n",
    "                                                       channel='?HZ',starttime=starttime_st, endtime=endtime_st)\n",
    "\n",
    "                else: \n",
    "                    st =  Stream()\n",
    "                    print(f\"WARNING: No data for {ii[1]}.{ii[2]}.{channel} on {otime}.\")    \n",
    "                    continue\n",
    "\n",
    "        #         print(f\"len(st):{len(st)}\")\n",
    "        #         print(st)\n",
    "\n",
    "                # Skip empty traces\n",
    "                if len(st) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                print(f'First st print:{st}')\n",
    "                # Create a new stream\n",
    "                _st = Stream()\n",
    "                # Check for HH and BH channels presence\n",
    "                has_HH = bool(st.select(channel=\"HH?\"))\n",
    "                has_BH = bool(st.select(channel=\"BH?\"))\n",
    "\n",
    "                # Apply selection logic based on channel presence\n",
    "                if has_HH and has_BH:\n",
    "                    # If both HH and BH channels are present, select only HH\n",
    "                    _st += st.select(channel=\"HH?\")\n",
    "                elif has_HH:\n",
    "                    # If only HH channels are present\n",
    "                    _st += st.select(channel=\"HH?\")\n",
    "                elif has_BH:\n",
    "                    # If only BH channels are present\n",
    "                    _st += st.select(channel=\"BH?\")\n",
    "\n",
    "                _st.merge(fill_value='interpolate') # fill gaps if there are any.\n",
    "\n",
    "                print(f'Second st print:{_st}')\n",
    "            \n",
    "                noise = _st.copy().trim(starttime=starttime_noise,endtime=endtime_noise)    \n",
    "                signal = _st.copy().trim(starttime=starttime_signal,endtime=endtime_signal) \n",
    "                \n",
    "                print(f\"P and S pick times:{p_pick_time} and {s_pick_time}\")\n",
    "                print(f\"Stream start and end times:{starttime_st} and {endtime_st}\")\n",
    "                print(f\"Noise start and end times:{starttime_noise} and {endtime_noise}\")\n",
    "                print(f\"Signal start and end times:{starttime_signal} and {endtime_signal}\")\n",
    "                \n",
    "                print(f'Noise:{noise}')\n",
    "                print(f'Signal:{signal}')\n",
    "\n",
    "                noise_abs = np.percentile(abs(noise[0].data),percentile)\n",
    "                signal_abs = np.percentile(abs(signal[0].data),percentile)\n",
    "\n",
    "                snr = 20 * np.log10((signal_abs/noise_abs))\n",
    "\n",
    "                snr_list.append(snr)\n",
    "                \n",
    "            if len(p_picks)>0 and len(s_picks)==0:\n",
    "                \n",
    "                print(p_picks['datetime'].values,s_picks['datetime'].values)\n",
    "                p_pick_time = UTCDateTime(str(p_picks['datetime'].values[0]))\n",
    "#                 s_pick_time = UTCDateTime(str(s_picks['datetime'].values[0]))\n",
    "\n",
    "                starttime_st = UTCDateTime(p_pick_time)-timedelta(seconds=120)\n",
    "                endtime_st = UTCDateTime(p_pick_time)+timedelta(seconds=120)\n",
    "                \n",
    "                starttime_noise = UTCDateTime(p_pick_time)-timedelta(seconds=8)\n",
    "                endtime_noise = UTCDateTime(p_pick_time)\n",
    "                \n",
    "                starttime_signal = UTCDateTime(p_pick_time)+timedelta(seconds=1)\n",
    "                endtime_signal = UTCDateTime(p_pick_time)+timedelta(seconds=4)\n",
    "                \n",
    "            \n",
    "                if i[0] in ['NC', 'BK']:\n",
    "                # Query waveforms\n",
    "                    st = client_ncedc.get_waveforms(network=i[0], station=i[1],\n",
    "                                                    location=\"*\", channel=\"?HZ\",starttime=starttime_st, \n",
    "                                                    endtime=endtime_st)\n",
    "\n",
    "                elif i[0] in networks: \n",
    "                    st = client_waveform.get_waveforms(network=i[0], station=i[1],\n",
    "                                                       channel='?HZ',starttime=starttime_st, endtime=endtime_st)\n",
    "\n",
    "                else: \n",
    "                    st =  Stream()\n",
    "                    print(f\"WARNING: No data for {ii[1]}.{ii[2]}.{channel} on {otime}.\")    \n",
    "                    continue\n",
    "\n",
    "        #         print(f\"len(st):{len(st)}\")\n",
    "        #         print(st)\n",
    "\n",
    "                # Skip empty traces\n",
    "                if len(st) == 0:\n",
    "                    continue\n",
    "                print(f'First st print:{st}')\n",
    "                # Create a new stream\n",
    "                _st = Stream()\n",
    "                # Check for HH and BH channels presence\n",
    "                has_HH = bool(st.select(channel=\"HH?\"))\n",
    "                has_BH = bool(st.select(channel=\"BH?\"))\n",
    "\n",
    "                # Apply selection logic based on channel presence\n",
    "                if has_HH and has_BH:\n",
    "                    # If both HH and BH channels are present, select only HH\n",
    "                    _st += st.select(channel=\"HH?\")\n",
    "                elif has_HH:\n",
    "                    # If only HH channels are present\n",
    "                    _st += st.select(channel=\"HH?\")\n",
    "                elif has_BH:\n",
    "                    # If only BH channels are present\n",
    "                    _st += st.select(channel=\"BH?\")\n",
    "\n",
    "                _st.merge(fill_value='interpolate') # fill gaps if there are any.\n",
    "\n",
    "                print(f'Second st print:{_st}')\n",
    "                \n",
    "                \n",
    "                    \n",
    "\n",
    "                noise = _st.copy().trim(starttime=starttime_noise,endtime=endtime_noise)    \n",
    "                signal = _st.copy().trim(starttime=starttime_signal,endtime=endtime_signal) \n",
    "                \n",
    "                print(f\"P pick time:{p_pick_time}\")\n",
    "                print(f\"Stream start and end times:{starttime_st} and {endtime_st}\")\n",
    "                print(f\"Noise start and end times:{starttime_noise} and {endtime_noise}\")\n",
    "                print(f\"Signal start and end times:{starttime_signal} and {endtime_signal}\")\n",
    "\n",
    "                print(f'Noise:{noise}')\n",
    "                print(f'Signal:{signal}')\n",
    "\n",
    "                noise_abs = np.percentile(abs(noise[0].data),percentile)\n",
    "                signal_abs = np.percentile(abs(signal[0].data),percentile)\n",
    "\n",
    "                snr = 20 * np.log10((signal_abs/noise_abs))\n",
    "\n",
    "                snr_list.append(snr)\n",
    "                    \n",
    "            if len(p_picks)==0 and len(s_picks)>0:\n",
    "                print(p_picks['datetime'].values,s_picks['datetime'].values)\n",
    "#                 p_pick_time = UTCDateTime(str(p_picks['datetime'].values))\n",
    "                s_pick_time = UTCDateTime(str(s_picks['datetime'].values[0]))\n",
    "    \n",
    "                starttime_st = UTCDateTime(s_pick_time)-timedelta(seconds=120)\n",
    "                endtime_st = UTCDateTime(s_pick_time)+timedelta(seconds=120)\n",
    "                \n",
    "                starttime_noise = UTCDateTime(s_pick_time)-timedelta(seconds=8)\n",
    "                endtime_noise = UTCDateTime(s_pick_time)\n",
    "                \n",
    "                starttime_signal = UTCDateTime(s_pick_time)\n",
    "                endtime_signal = UTCDateTime(s_pick_time)+timedelta(seconds=3)\n",
    "                \n",
    "                print(starttime_noise,endtime_noise)\n",
    "            \n",
    "                if i[0] in ['NC', 'BK']:\n",
    "                # Query waveforms\n",
    "                    st = client_ncedc.get_waveforms(network=i[0], station=i[1],\n",
    "                                                    location=\"*\", channel=\"?HZ\",starttime=starttime_st, \n",
    "                                                    endtime=endtime_st)\n",
    "\n",
    "                elif i[0] in networks: \n",
    "                    st = client_waveform.get_waveforms(network=i[0], station=i[1],\n",
    "                                                       channel='?HZ',starttime=starttime_st, endtime=endtime_st)\n",
    "\n",
    "                else: \n",
    "                    st =  Stream()\n",
    "                    print(f\"WARNING: No data for {ii[1]}.{ii[2]}.{channel} on {otime}.\")    \n",
    "                    continue\n",
    "\n",
    "        #         print(f\"len(st):{len(st)}\")\n",
    "        #         print(st)\n",
    "\n",
    "                # Skip empty traces\n",
    "                if len(st) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                print(f'First st print:{st}')\n",
    "                # Create a new stream\n",
    "                _st = Stream()\n",
    "                # Check for HH and BH channels presence\n",
    "                has_HH = bool(st.select(channel=\"HH?\"))\n",
    "                has_BH = bool(st.select(channel=\"BH?\"))\n",
    "\n",
    "                # Apply selection logic based on channel presence\n",
    "                if has_HH and has_BH:\n",
    "                    # If both HH and BH channels are present, select only HH\n",
    "                    _st += st.select(channel=\"HH?\")\n",
    "                elif has_HH:\n",
    "                    # If only HH channels are present\n",
    "                    _st += st.select(channel=\"HH?\")\n",
    "                elif has_BH:\n",
    "                    # If only BH channels are present\n",
    "                    _st += st.select(channel=\"BH?\")\n",
    "\n",
    "                _st.merge(fill_value='interpolate') # fill gaps if there are any.\n",
    "\n",
    "                print(f'Second st print:{_st}')\n",
    "\n",
    "            \n",
    "                noise = _st.copy().trim(starttime=starttime_noise,endtime=endtime_noise)    \n",
    "                signal = _st.copy().trim(starttime=starttime_signal,endtime=endtime_signal) \n",
    "                \n",
    "                print(f\"S pick time:{s_pick_time}\")\n",
    "                print(f\"Stream start and end times:{starttime_st} and {endtime_st}\")\n",
    "                print(f\"Noise start and end times:{starttime_noise} and {endtime_noise}\")\n",
    "                print(f\"Signal start and end times:{starttime_signal} and {endtime_signal}\")\n",
    "                \n",
    "                print(f'Noise:{noise}')\n",
    "                print(f'Signal:{signal}')\n",
    "\n",
    "                noise_abs = np.percentile(abs(noise[0].data),percentile)\n",
    "                signal_abs = np.percentile(abs(signal[0].data),percentile)\n",
    "\n",
    "                snr = 20 * np.log10((signal_abs/noise_abs))\n",
    "\n",
    "                snr_list.append(snr)\n",
    "            \n",
    "    all_assignments_picks['snr']=snr_list\n",
    "     \n",
    "    return snr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52736088",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_list = calc_snr(all_picks,all_pick_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_waveform = WaveformClient()\n",
    "\n",
    "p_pick_time = '2014-01-02T07:40:02.642533Z'\n",
    "\n",
    "starttime_st = UTCDateTime(p_pick_time)-timedelta(seconds=120)\n",
    "endtime_st = UTCDateTime(p_pick_time)+timedelta(seconds=120)\n",
    "\n",
    "# starttime_st = UTCDateTime('2011-01-03T00:00:0.000000Z')\n",
    "# endtime_st = UTCDateTime('2011-01-03T12:59:59.000000Z')\n",
    "\n",
    "print(f'Requested start and end times:{starttime_st} and {endtime_st}')\n",
    "\n",
    "\n",
    "st = client_waveform.get_waveforms(network='CN', station='BTB',\n",
    "                                       channel='HHZ',starttime=starttime_st, endtime=endtime_st)\n",
    "print(f'Actual start and end times: {st[0].stats.starttime} and {st[0].stats.endtime}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a901a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
